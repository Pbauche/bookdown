---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Caret Package
```{r}
library(caret)
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')

path = "C:/Website Rmarkdown/bookdown/save/"


```


## Pre-Processing
Serveral function can be use to preprocess the data. Caret package assume that variables are numeric. Factor have been converted to dummy. 

  - **Create Dummy variables**
    - dummyVars() : create dummy from one or more factors. In caret, one-hot-encodings can be created using dummyVars(). Just pass in all the features to dummyVars() as the training data and all the factor columns will automatically be converted to one-hot-encodings.


  - **Near Zero Variance PrÃ©dictors**
    - Si variables as un seul facteur ou trÃ¨s peu de varianc, elles peuvent biasÃ© une les modÃ¨les prÃ©dictifs. Si predictor trop unballanced, when we split data in subsample for crossvalidation or other subsample, predictor may become zero variance.
    - some metric :
      - frÃ©quency ratio : frequency of the most prevalent value over the second most frequent.  (proche de 1 si bien equilibrÃ©)
      - percent of unique values : number of unique values divided by the total number of samples. Approaches zero as the granularity of the data increases 

  - **Identifying correlated predictors**
    - findCorrelation() function uses the following algorithm to flag predictors for removal. 

  - **Linear dependencies**
    - findLinearCombos() function uses the QR decomposition of a matrix to enumerate sets of linear combinations 
    
    ```{r dummy}
## Dummy variable    
library(earth)
data(etitanic)
head(model.matrix(survived ~ ., data = etitanic))

# Use dummyVars to create dummy
dummies <- dummyVars(survived ~ ., data = etitanic)
head(predict(dummies, newdata = etitanic))

## Near zero variance
data(mdrr)
data.frame(table(mdrrDescr$nR11))

nzv <- nearZeroVar(mdrrDescr, saveMetrics= TRUE)
nzv[nzv$nzv,][1:10,]

nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)

## correlated predictors
descrCor <-  cor(filteredDescr)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .999)
# there are 65 descriptors that are almost perfectly correlated

summary(descrCor[upper.tri(descrCor)])

# remove var with corr above 0.75
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
filteredDescr <- filteredDescr[,-highlyCorDescr]
descrCor2 <- cor(filteredDescr)
summary(descrCor2[upper.tri(descrCor2)])

## Linear dependencies
# comboInfo <- findLinearCombos(ltfrDesign)
# comboInfo
# ltfrDesign[, -comboInfo$remove]


```

  - **preProcess**
    - Operation on predictor like centering, scaling, ...
    - can be use in train() function 
    - Imputation
        -  KNN : For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g. using the mean)
        - Bagged tree : For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. 
    - TRansforming predictor : need to be centered and scaled
      - PCA
      - boxcox : data need to be greater than zero
      - exponential transformation, Yeo-Johnson ..
    
  

```{r}
library(AppliedPredictiveModeling)
data(schedulingData)

# The data are a mix of categorical and numeric predictors. 
#  Yeo-Johnson transformation on the continuous predictors then center and scale them. 

pp_hpc <- preProcess(schedulingData[, -8], 
                     method = c("center", "scale", "YeoJohnson"))
pp_hpc

# Use predict to get transformed data
transformed <- predict(pp_hpc, newdata = schedulingData[, -8])
head(transformed)

# the predictor for the number of pending jobs, has a very sparse and unbalanced distribution:
mean(schedulingData$NumPending == 0)

# We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations : 

pp_no_nzv <- preProcess(schedulingData[, -8], 
                        method = c("center", "scale", "YeoJohnson", "nzv"))
pp_no_nzv

predict(pp_no_nzv, newdata = schedulingData[1:6, -8])
```
    
    

  - **Class distance calculations**  
  

caret contain fonction to generate new predictors variables based on distance to class centroids (see linear discriminant analysis).  For each level of a factor variable, the class centroid and covariance matrix is calculated. For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non-linear models when the true decision boundary is actually linear.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
centroids <- classDist(trainBC, trainMDRR)
distances <- predict(centroids, testBC)
distances <- as.data.frame(distances)
head(distances)

xyplot(dist.Active ~ dist.Inactive,
       data = distances, 
       groups = testMDRR, 
       auto.key = list(columns = 2))
```

    
    
    
## Data Splitting 

  - **Simple splitting based on the outcome**
    - createDataPartition  : create balanced split of the data
      - list = FALSE avoids returning the data as a list
      - times, that can create multiple splits at once
      - the data indices are returned in a list of integer vectors
    - createResample can be used to make simple bootstrap samples
    createFolds can be used to generate balanced crossvalidation groupings from a set of data.
  - **splitting based on the predictors**
    - maxDissim can be used to create subsamples using a maximum dissimilarity approach.
  - **Data splitting for time series**
    - createTimeSlices 
      - initialWindow: the initial number of consecutive values in each training set sample
      - horizon: The number of consecutive values in test set sample
      - fixedWindow: A logical: if FALSE, the training set always start at the first sample and the training set size will vary over data splits.
  - **splitting with important groups**
    -   see documentation package

```{r data split}
library(mlbench)
load(file = "C:/Website Rmarkdown/bookdown/save/Sonar.RData")
 
# inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
# training <- Sonar[ inTraining,]
training = sonar_train
# testing  <- Sonar[-inTraining,]
testing = sonar_test
```


## Model Training and tuning

  - ** train() function**
    - evaluate, using resampling, the effect of model tuning parameters on performance
    - Choose the optimalâ model across these parameters
    - Estimate model performance from a training set

  - **Train() algorithm**
      - for each parameter set
          - for each resampling iteration
            - fit
            - predict on train set out of sample
          - end
          - calculate average perfomance
      - end
      - determine optimal paramater set
      - fit final model
 
   - **information to done**
    - model type
    - parameter value
    - resampling solution
      - k fold crossvalidation :  Te original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. And repeated. The k results can then be averaged to produce a single estimation.
      
      - Leave one out 
      - Bootstrap : random sampling with replacement
 
 
### Exemple : Basic tuning for  boosted tree model GBM   
```{r, message=FALSE, warning=FALSE}
 

fitControl <- trainControl(    method = "repeatedcv"  
                             , number = 10,          ## 10-fold CV  : number of fold or number of resampling iteration
                               repeats = 10)         ## repeated ten times
 
# gbmFit1 <- train(Class ~ ., data = training, 
#                  method = "gbm", 
#                  trControl = fitControl,
#                  ## This last option is actually one for gbm() that passes through
#                  verbose = FALSE)

# save(gbmFit1, file= paste(path, "gbmfit1.RData",sep="" ))
load(file= paste(path, "gbmfit1.RData",sep="" ))

gbmFit1 
 
```
 
  - For a gradient boosting machine (GBM) model, there are three main tuning parameters:
     - number of iterations, i.e.trees, (called n.trees in the gbm function)
     - complexity of the tree, called interaction.depth
     - learning rate: how quickly the algorithm adapts, called shrinkage 
     - the minimum number of training set samples in a node to commence splitting (n.minobsinnode)
     
 
Train() can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p. 


### Customizing the Tuning Process

  -  **Alternate Tuning Grid*s** : tuneGrid option in train
    - Par dÃ©faut train chose model with largest perfomance value.
    - Il existe d'autr methode de recherche pour le uning des paramÃ¨tre comme random search ( option search = "random" in the call to trainControl)

  - **Plotting the resampling profile**
    - Plot function to examine the relationship between the estimates of the performance and the tuning parameters. 
    
  - **trainControl** : generates parameters that further control how models are created
    - Method = boot, cv, repeatedcv , ...
      - oob = out-of-bag estimates (for DT or RF)
    - number and repeats (only if repeatedcv): number controls with the numbe r of folds in K-fold cross-validation or number of resampling iterations for bootstrapping
    - allowParallel: a logical that governs whether train should use parallel processing (if availible).
    - summaryFunction that specifies a function for computing performance for user defined performance metrics

  - **Alternate Performance Metrics**
    - defaut ;  RMSE, MSA, R2 for regression and accurancy , kappa for classification.
    - twoClassSummary() function, will compute the sensitivity, specificity and area under the ROC curve:

  - **Choosing the Final model**
    - train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. User-defined functions can be used.
      -  tolerance function could be used to find a less complex model. 
      
  - **Extracting Predictions and Class Probabilities**
    - objects produced by the train function contain the optimized model in the finalModel sub-object
    - predict.train, the type options are standardized to be "class" and "prob"
    
  - **Fitting Models Without Parameter Tuning**
      - In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method = "none" option in trainControl
 
```{r}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
 
# gbmFit2 <- train(Class ~ ., data = training, 
#                  method = "gbm", 
#                  trControl = fitControl, 
#                  verbose = FALSE, 
#                  ## Now specify the exact models 
#                  ## to evaluate:
#                  tuneGrid = gbmGrid)
# 
# save(gbmFit2,file= paste(path, "gbmFit2.RData",sep="" ))

load(file=paste(path, "gbmFit2.RData",sep="" ))
gbmFit2

trellis.par.set(caretTheme())
ggplot(gbmFit2, metric = "Kappa")


fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,           
                           classProbs = TRUE, ## Estimate class probabilities
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)

# gbmFit3 <- train(Class ~ ., data = training, 
#                  method = "gbm", 
#                  trControl = fitControl, 
#                  verbose = FALSE, 
#                  tuneGrid = gbmGrid,
#                  ## Specify which metric to optimize
#                  metric = "ROC")
# save(gbmFit3,file= paste(path, "gbmFit3.RData",sep="" ))

load(file=paste(path, "gbmFit3.RData",sep="" ))

gbmFit3

whichTwoPct <- tolerance(gbmFit3$results, metric = "ROC", 
                         tol = 2, maximize = TRUE)  
gbmFit3$results[whichTwoPct,1:6]
# This indicates that we can get a less complex model with an area under the ROC curve of 0.901 (compared to the pick the best value of 0.914).

# predict(gbmFit3, newdata = head(testing))

# predict(gbmFit3, newdata = head(testing), type = "prob")


# Fitting Models Without Parameter Tuning

fitControl <- trainControl(method = "none", classProbs = TRUE)

set.seed(825)
# gbmFit4 <- train(Class ~ ., data = training, 
#                  method = "gbm", 
#                  trControl = fitControl, 
#                  verbose = FALSE, 
#                  ## Only a single model can be passed to the
#                  ## function when no resampling is used:
#                  tuneGrid = data.frame(interaction.depth = 4,
#                                        n.trees = 100,
#                                        shrinkage = .1,
#                                        n.minobsinnode = 20),
#                  metric = "ROC")

# save(gbmFit4,file= paste(path, "gbmFit4.RData",sep="" ))

load(file=paste(path, "gbmFit4.RData",sep="" ))

gbmFit4



```
 
 
  -  **Random hyperparameter search**

To use random search, another option is available in trainControl called search. Possible values of this argument are "grid" and "random". The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the tuneLength option to train.

```{r}

fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "random",
                           verboseIter =  FALSE)

# rda_fit <- train(Class ~ ., data = training, 
#                   method = "rda",
#                   metric = "ROC",
#                   tuneLength = 30,
#                   trControl = fitControl)
# 
# save(rda_fit,file= paste(path, "rda_fit.RData",sep="" ))

load(file=paste(path, "rda_fit.RData",sep="" ))

rda_fit

# view of the random search
ggplot(rda_fit) + theme(legend.position = "top")
```

 
 
### Exploring and Comparing Resampling Distributions   
  - **Within Model**
    - explore relationships between tuning parameters and the resampling results for a specific model
      - xyplot and stripplot can be used to plot resampling statistics against (numeric) tuning parameters.
      - histogram and densityplot can also be used to look at distributions of the tuning parameters

  - **Between models**
    -  resample() can be use to collect the resampling result and make statistical statements about their performance differences of different model. 
    - several lattice plot methods that can be used to visualize the resampling distributions
    
```{r}
# svmFit <- train(Class ~ ., data = training, 
#                  method = "svmRadial", 
#                  trControl = fitControl, 
#                  preProc = c("center", "scale"),
#                  tuneLength = 8,
#                  metric = "ROC")
# 
# save(svmFit,file= paste(path, "svmFit.RData",sep="" ))

load(file=paste(path, "svmFit.RData",sep="" ))

resamps <- resamples(list(GBM = rda_fit,
                          SVM = svmFit))
summary(resamps)

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))

dotplot(resamps, metric = "ROC")
splom(resamps)
```


## Best available Models
- **Logit**
- **ababoost**
- **random forest**
- **xgboost**
- **SVM**


```{r}
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)

trainData <- orange[trainRowNumbers,]
testData <- orange[-trainRowNumbers,]

x = trainData[, 2:18]
y = trainData$Purchase

# library(skimr)
# skimmed <- skim_to_wide(trainData)
# skimmed[, c(1:5, 9:11, 13, 15:16)]

preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model

trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)

# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
trainData_mat <- predict(dummies_model, newdata = trainData)
trainData <- data.frame(trainData_mat)

preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Purchase <- y

featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

featurePlot(x = trainData[, 1:18], 
            y = trainData$Purchase, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

# feature selection

subsets <- c(1:5, 10, 15, 18)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

# lmProfile <- rfe(x=trainData[, 1:4], y=trainData$Purchase, 
#                sizes = subsets,
#                 rfeControl = ctrl)

# lmProfile

# modeling
## get some info
modelLookup('earth')

model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)
plot(model_mars, main="Model Accuracies with MARS")
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")

# same imputation for test set
testData2 <- predict(preProcess_missingdata_model, testData)  
testData3 <- predict(dummies_model, testData2)
testData4 <- predict(preProcess_range_model, testData3)
predicted <- predict(model_mars, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted, mode='everything', positive='MM')


# tuning model

## by tunelength

fitControl <- trainControl(
    method = 'cv',                   # k-fold cross validation
    number = 5,                      # number of folds
    savePredictions = 'final',       # saves predictions for optimal tuning parameter
    classProbs = T,                  # should class probabilities be returned
    summaryFunction=twoClassSummary  # results summary function
) 

# tuneLength corresponds to the number of unique values for the tuning parameters caret will consider while forming the hyper parameter combinations.


# Step 1: Tune hyper parameters by setting tuneLength
model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl)
model_mars2

# Step 2: Predict on testData and Compute the confusion matrix
predicted2 <- predict(model_mars2, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted2, mode='everything', positive='MM')

# by tunegrid

marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10), 
                        degree = c(1, 2, 3))

# Step 2: Tune hyper parameters by setting tuneGrid
 model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)
 model_mars3

# Step 3: Predict on testData and Compute the confusion matrix
predicted3 <- predict(model_mars3, testData4)
confusionMatrix(reference = testData$Purchase, data = predicted3, mode='everything', positive='MM')


# Compare model

# model_adaboost = train(Purchase ~ ., data=trainData[1:100,], method='adaboost', tuneLength=2, trControl = fitControl)

# model_rf = train(Purchase ~ ., data=trainData[1:100,], method='rf', tuneLength=5, trControl = fitControl)

# model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)

# model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)

# models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf)) #, XGBDART=model_xgbDART, MARS=model_mars3, SVM=model_svmRadial))

# save(models_compare,file= paste(path, "models_compare.RData",sep="" ))

load(file=paste(path, "models_compare.RData",sep="" ))

summary(models_compare)

scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)

#  ensemble predictions from multiple models using caretEnsemble

library(caretEnsemble)

trainControl <- trainControl(method="repeatedcv", 
                             number=3, 
                             repeats=2,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

algorithmList <- c('rf', 'adaboost') #, 'earth', 'xgbDART', 'svmRadial')

# models <- caretList(Purchase ~ ., data=trainData[1:50,], trControl=trainControl, methodList=algorithmList) 
# save(models,file= paste(path, "models.RData",sep="" ))

load(file=paste(path, "models.RData",sep="" ))

results <- resamples(models)
summary(results)

# Combine the predictions of multiple models to form a final prediction

# Create the trainControl
set.seed(101)
stackControl <- trainControl(method="repeatedcv", 
                             number=3, 
                             repeats=3,
                             savePredictions=TRUE, 
                             classProbs=TRUE)

# Ensemble the predictions of `models` to form a new combined prediction based on glm
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)

# /!\ The ensembles tend to perform better if the predictions are less correlated with each other.

# Predict on testData
stack_predicteds <- predict(stack.glm, newdata=testData4)

```

 

## Parallel Processing 

```{r}
# library(doParallel)
# cl <- detectCores() 
# cl <- makePSOCKcluster(3)
# registerDoParallel(cl)
# 
# ## All subsequent models are then run in parallel
# model <- train(Class ~ ., data = training, method = "rf")
# 
# ## When you are done:
# stopCluster(cl)
# registerDoSEQ()
```



## Subsampling for class imbalances

Examples of sampling methods : 

  - **down-sampling**: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class (downSample option)
  - **up-sampling**: randomly sample (with replacement) the minority class to be the same size as the majority class (upSample option)
  - **hybrid methods**: techniques such as SMOTE and ROSE down-sample the majority class and synthesize new data points in the minority class. There are two packages (DMwR and ROSE) that implement these procedures
  
In practice, one could take the training set and, before model fitting, sample the data. During model may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance. 

The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below.


Deux aporoche 
  - Use sampling before model crossvalidation 
  - use sampling in the model crossvalidation 
    - Repeating the subsampling procedures for every resample produces results that are more consistent with the test set.
  

## Variables importance 
  - **Model Specific Metrics**
    - Linear Models: the absolute value of the t-statistic
    - Random Forest
    - ..
  - **Model Independent Metrics**
    -  the importance of each predictor is evaluated individually using a "filter" approach.
    

The function automatically scales the importance scores to be between 0 and 100. Using scale = FALSE avoids this normalization step.  

Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve.
```{r}
# library(gbm)
# gbmImp <- varImp(gbmFit3, scale = FALSE)
# gbmImp
# 
# roc_imp <- filterVarImp(x = training[, -ncol(training)], y = training$Class)
# head(roc_imp)
# 
# roc_imp2 <- varImp(svmFit, scale = FALSE)
# roc_imp2
# 
# plot(gbmImp, top = 20)
```
  


## measurung performance

  - **Measure for Regression**
    - postResample() function : estimate RMSE, MAE
    
  - **Measure for predicted classes**
    - confusionMatrix() function : compute a cross-tabulation of the observed and predicted classes.  IF Generating the predicted classes based on **50% cutoff** for the probabilities. 
    - this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument.
    - If there are three or more classes, confusionMatrix will show the confusion matrix and a set of "one-versus-all" results
    
add png : http://topepo.github.io/caret/measuring-performance.html
    
    
  - **Measure for class probabilities**
    - twoClassSummary() function computes the area under the ROC curve and the specificity and sensitivity under the **50% cutoff**
      - this function uses the first class level to define the "event" of interest. To change this, use the lev option to the function. 
      - there must be columns in the data for each of the class probabilities (named the same as the outcome's class levels)
    
  - **For multi-class problems**
    - mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities
    - multiClassSummary() : computes a number of relevant metrics:
   
   
   
   
  - **Lift Curves**
    - The function requires a set of sample probability predictions and the true class labels
  
  
```{r}
# Regression
data(BostonHousing)

bh_index <- createDataPartition(BostonHousing$medv, p = .75, list = FALSE)
bh_tr <- BostonHousing[ bh_index, ]
bh_te <- BostonHousing[-bh_index, ]

lm_fit <- train(medv ~ . + rm:lstat,
                data = bh_tr, 
                method = "lm")
bh_pred <- predict(lm_fit, bh_te)

lm_fit

postResample(pred = bh_pred, obs = bh_te$medv)

# classification
## create dataset
true_class <- factor(sample(paste0("Class", 1:2), 
                            size = 1000,
                            prob = c(.2, .8), replace = TRUE))
true_class <- sort(true_class)
class1_probs <- rbeta(sum(true_class == "Class1"), 4, 1)
class2_probs <- rbeta(sum(true_class == "Class2"), 1, 2.5)
test_set <- data.frame(obs = true_class,
                       Class1 = c(class1_probs, class2_probs))
test_set$Class2 <- 1 - test_set$Class1
test_set$pred <- factor(ifelse(test_set$Class1 >= .5, "Class1", "Class2"))

ggplot(test_set, aes(x = Class1)) + 
  geom_histogram(binwidth = .05) + 
  facet_wrap(~obs) + 
  xlab("Probability of Class #1")

confusionMatrix(data = test_set$pred, reference = test_set$obs)

twoClassSummary(test_set, lev = levels(test_set$obs))
prSummary(test_set, lev = levels(test_set$obs))

mnLogLoss(test_set, lev = levels(test_set$obs))

# lift curves

lift_training <- twoClassSim(1000)
lift_testing  <- twoClassSim(1000)

ctrl <- trainControl(method = "cv", classProbs = TRUE,
                     summaryFunction = twoClassSummary)

fda_lift <- train(Class ~ ., data = lift_training,
                  method = "fda", metric = "ROC",
                  tuneLength = 20,
                  trControl = ctrl)

lda_lift <- train(Class ~ ., data = lift_training,
                  method = "lda", metric = "ROC",
                  trControl = ctrl)

 # c5_lift <- train(Class ~ ., data = lift_training,
 #                 method = "C5.0", metric = "ROC",
 #                 tuneLength = 10,
 #                 trControl = ctrl,
 #                 control = C5.0Control(earlyStopping = FALSE))

## Generate the test set results
lift_results <- data.frame(Class = lift_testing$Class)
lift_results$FDA <- predict(fda_lift, lift_testing, type = "prob")[,"Class1"]
lift_results$LDA <- predict(lda_lift, lift_testing, type = "prob")[,"Class1"]
#lift_results$C5.0 <- predict(c5_lift, lift_testing, type = "prob")[,"Class1"]
head(lift_results)

trellis.par.set(caretTheme())
lift_obj <- lift(Class ~ FDA + LDA , data = lift_results)
ggplot(lift_obj, values = 60)
```





## feature selection 
### Overview
  - **Wrapper**
    -  evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. 
   - wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. 
   - caret has wrapper methods based on recursive feature elimination, genetic algorithms, and simulated annealing.
  
  
  - **Filter**
    - evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. 
    - Only predictors with important relationships would then be included in a classification model


### Univariate approach

### recursive feature elimination
  - **rfe function**
    - x : matrix of predictor variables
    - y : a vector of outcomes
    - sizes : specifie the subset sizes that should be tested
    - rfeControl : a list of options that can be used
  
There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs), random forests (rfFuncs), naive Bayes (nbFuncs), bagged trees (treebagFuncs) and functions that can be used with caret's train function (caretFuncs).


RFE works in 3 broad steps:

  -  Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.
  - Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subgroups of most important predictors determined from step 1. Ranking of the predictors is recalculated in each iteration.
  - Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.


```{r eval=FALSE, include=FALSE}
#simu

n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),
                     paste("bogus", 1:5, sep = ""))
bogus <- matrix(rnorm(n * p), nrow = n)
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y

# Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on

normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(1:5, 10, 15, 20, 25)

# The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1.

ctrl <- rfeControl(functions = lmFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x, y,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile

# The output shows that the best subset size was estimated to be 4 predictors

predictors(lmProfile)

lmProfile$fit

head(lmProfile$resample)

trellis.par.set(caretTheme())
plot(lmProfile, type = c("g", "o"))

```


### genetic algorimth

### simulated annealing

















