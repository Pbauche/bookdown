## Parallel Processing 

```{r}
# library(doParallel)
# cl <- detectCores() 
# cl <- makePSOCKcluster(3)
# registerDoParallel(cl)
# 
# ## All subsequent models are then run in parallel
# model <- train(Class ~ ., data = training, method = "rf")
# 
# ## When you are done:
# stopCluster(cl)
# registerDoSEQ()
```



## Subsampling for class imbalances

Examples of sampling methods : 

  - **down-sampling**: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class (downSample option)
  - **up-sampling**: randomly sample (with replacement) the minority class to be the same size as the majority class (upSample option)
  - **hybrid methods**: techniques such as SMOTE and ROSE down-sample the majority class and synthesize new data points in the minority class. There are two packages (DMwR and ROSE) that implement these procedures
  
In practice, one could take the training set and, before model fitting, sample the data. During model may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance. 

The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below.


Deux aporoche 
  - Use sampling before model crossvalidation 
  - use sampling in the model crossvalidation 
    - Repeating the subsampling procedures for every resample produces results that are more consistent with the test set.
  

## Variables importance 
  - **Model Specific Metrics**
    - Linear Models: the absolute value of the t-statistic
    - Random Forest
    - ..
  - **Model Independent Metrics**
    -  the importance of each predictor is evaluated individually using a "filter" approach.
    

The function automatically scales the importance scores to be between 0 and 100. Using scale = FALSE avoids this normalization step.  

Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve.
```{r}
# library(gbm)
# gbmImp <- varImp(gbmFit3, scale = FALSE)
# gbmImp
# 
# roc_imp <- filterVarImp(x = training[, -ncol(training)], y = training$Class)
# head(roc_imp)
# 
# roc_imp2 <- varImp(svmFit, scale = FALSE)
# roc_imp2
# 
# plot(gbmImp, top = 20)
```
  


## measurung performance

  - **Measure for Regression**
    - postResample() function : estimate RMSE, MAE
    
  - **Measure for predicted classes**
    - confusionMatrix() function : compute a cross-tabulation of the observed and predicted classes.  IF Generating the predicted classes based on **50% cutoff** for the probabilities. 
    - this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument.
    - If there are three or more classes, confusionMatrix will show the confusion matrix and a set of "one-versus-all" results
    
add png : http://topepo.github.io/caret/measuring-performance.html
    
    
  - **Measure for class probabilities**
    - twoClassSummary() function computes the area under the ROC curve and the specificity and sensitivity under the **50% cutoff**
      - this function uses the first class level to define the "event" of interest. To change this, use the lev option to the function. 
      - there must be columns in the data for each of the class probabilities (named the same as the outcome's class levels)
    
  - **For multi-class problems**
    - mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities
    - multiClassSummary() : computes a number of relevant metrics:
   
   
   
   
  - **Lift Curves**
    - The function requires a set of sample probability predictions and the true class labels
  
  
```{r}
# Regression
data(BostonHousing)

bh_index <- createDataPartition(BostonHousing$medv, p = .75, list = FALSE)
bh_tr <- BostonHousing[ bh_index, ]
bh_te <- BostonHousing[-bh_index, ]

lm_fit <- train(medv ~ . + rm:lstat,
                data = bh_tr, 
                method = "lm")
bh_pred <- predict(lm_fit, bh_te)

lm_fit

postResample(pred = bh_pred, obs = bh_te$medv)

# classification
## create dataset
true_class <- factor(sample(paste0("Class", 1:2), 
                            size = 1000,
                            prob = c(.2, .8), replace = TRUE))
true_class <- sort(true_class)
class1_probs <- rbeta(sum(true_class == "Class1"), 4, 1)
class2_probs <- rbeta(sum(true_class == "Class2"), 1, 2.5)
test_set <- data.frame(obs = true_class,
                       Class1 = c(class1_probs, class2_probs))
test_set$Class2 <- 1 - test_set$Class1
test_set$pred <- factor(ifelse(test_set$Class1 >= .5, "Class1", "Class2"))

ggplot(test_set, aes(x = Class1)) + 
  geom_histogram(binwidth = .05) + 
  facet_wrap(~obs) + 
  xlab("Probability of Class #1")

confusionMatrix(data = test_set$pred, reference = test_set$obs)

twoClassSummary(test_set, lev = levels(test_set$obs))
prSummary(test_set, lev = levels(test_set$obs))

mnLogLoss(test_set, lev = levels(test_set$obs))

# lift curves

lift_training <- twoClassSim(1000)
lift_testing  <- twoClassSim(1000)

ctrl <- trainControl(method = "cv", classProbs = TRUE,
                     summaryFunction = twoClassSummary)

fda_lift <- train(Class ~ ., data = lift_training,
                  method = "fda", metric = "ROC",
                  tuneLength = 20,
                  trControl = ctrl)

lda_lift <- train(Class ~ ., data = lift_training,
                  method = "lda", metric = "ROC",
                  trControl = ctrl)

 # c5_lift <- train(Class ~ ., data = lift_training,
 #                 method = "C5.0", metric = "ROC",
 #                 tuneLength = 10,
 #                 trControl = ctrl,
 #                 control = C5.0Control(earlyStopping = FALSE))

## Generate the test set results
lift_results <- data.frame(Class = lift_testing$Class)
lift_results$FDA <- predict(fda_lift, lift_testing, type = "prob")[,"Class1"]
lift_results$LDA <- predict(lda_lift, lift_testing, type = "prob")[,"Class1"]
#lift_results$C5.0 <- predict(c5_lift, lift_testing, type = "prob")[,"Class1"]
head(lift_results)

trellis.par.set(caretTheme())
lift_obj <- lift(Class ~ FDA + LDA , data = lift_results)
ggplot(lift_obj, values = 60)
```





## feature selection 
### Overview
  - **Wrapper**
    -  evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. 
   - wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. 
   - caret has wrapper methods based on recursive feature elimination, genetic algorithms, and simulated annealing.
  
  
  - **Filter**
    - evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. 
    - Only predictors with important relationships would then be included in a classification model


### Univariate approach

### recursive feature elimination
  - **rfe function**
    - x : matrix of predictor variables
    - y : a vector of outcomes
    - sizes : specifie the subset sizes that should be tested
    - rfeControl : a list of options that can be used
  
There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs), random forests (rfFuncs), naive Bayes (nbFuncs), bagged trees (treebagFuncs) and functions that can be used with caret's train function (caretFuncs).


RFE works in 3 broad steps:

  -  Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset.
  - Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subgroups of most important predictors determined from step 1. Ranking of the predictors is recalculated in each iteration.
  - Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors.


```{r eval=FALSE, include=FALSE}
#simu

n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),
                     paste("bogus", 1:5, sep = ""))
bogus <- matrix(rnorm(n * p), nrow = n)
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y

# Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on

normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(1:5, 10, 15, 20, 25)

# The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1.

ctrl <- rfeControl(functions = lmFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x, y,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile

# The output shows that the best subset size was estimated to be 4 predictors

predictors(lmProfile)

lmProfile$fit

head(lmProfile$resample)

trellis.par.set(caretTheme())
plot(lmProfile, type = c("g", "o"))

```


### genetic algorimth

### simulated annealing

















