[
["index.html", "Handout_V2 Chapter 1 Information 1.1 Usefull ressource 1.2 todo 1.3 interesting stuff 1.4 Hint", " Handout_V2 Pierre Bauche 2018-10-25 Chapter 1 Information 1.1 Usefull ressource Feature engenering : http://www.feat.engineering/review-predictive-modeling-process.html Semi supervized learning : https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/ - Machine Learning wth unlabbeled data : psuedo labeling Book Artificial Intelligence: A Modern Approach&quot; by Stuart Russell and Peter Norvig. 1.2 todo create Blogdown tydiverse package Tensorflow for deep learning : open source software library developed at google for complex computation 1.3 interesting stuff polynomial regression using kernel smoothing Logistic regression using 5 fold stratified cross validation blockchain data science reinforcement learning adversarial training -Deep Learning microsoft service Tensorflow : read exemple RKWard : free and open source Graphical User Interface for the R software ML with H2O, lime, Keras spark R image recognizing with R sentiment analysis = microsoft azure data privacy data quality control Web developpement Shiny : (microsoft azure serveur pour upload Rmarckodw : blogdown (Hugo) Insurance fraud underwriting models : predict somebody’s insurance risk marketing predition customer segmentation 1.4 Hint "],
["feature-engeneering.html", "Chapter 2 Feature engeneering 2.1 Input feature 2.2 Missing Value 2.3 Outlier Detection 2.4 Sampling and resampling 2.5 variables selections 2.6 Example 2.7 Method Summary 2.8 tips", " Chapter 2 Feature engeneering 2.1 Input feature A feature is a numeric representation of raw data. Feature engineering is the process of formulating the most appropriate features given the data, the model, and the task. If features are’t good enought then model canot be good. Features selection is important. If they are to many fearture, model use noise or irrelevant information or redundant. IF they are not enought feature, model don’t have the information . Create new input : Combine feature reduire dimention reduire colinéarité predictor qui ont du sens use the input interaction kmeans clustering as feature : attetion pas inclure la target risque overfitting a data point can also be represented by a dense vector of its inverse distance to each cluster center. This retains more information than simple binary cluster assignment n-day average (in time series) : peut reduire la variabilite etle noise ratio Tips Use knowledge to construct a better set of features (business) Visualizing the correlation and check de relation between input and output when output is numeric between different input Normalize the feature if metrics differt or unknow 2.1.1 Numeric Data Predictors that are on a continuous scale are subject to somes issues that can be mitigated through the choose of model. Models that are smooth functions of input features or model hat use euclidian distance (regression, clustering, …) are sensitive to the scale. Models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale. There are a variety of modifications that can be made to an individual predictor that might improve its utility in a model. scaling : not change the shape of the distribution \\(/frac{x-min(x)}{max(x)-min(x)}\\) Feature scaling is useful in situations where a set of input features differs wildly in scale. standardization on N(0,1) : \\(/frac{x-mean(x)}{sqrt(var())}\\) essential when the distance or dot products between predictors are used (such as K-nearest neighbors or support vector machines) essential when the variables are required to be a a common scale in order to apply a penalty (e.g. the lasso or ridge regression) normalisation : divide by the euclienne l² norme (=sums the squares of the values of the features across data points). SO the feature column has norm = 1 Discretization : fixed width quantile binning Variables scaled and standardized are comparable Some models need gaussian input : scale + transform Power transforms : variance-stabilizing transformations** Power transforms change the distribution of the variable to more symetric distribution log sqrt inverse boxcox : generalisation : Only work for positive variable johnson transform logit transformations : This transformation changes the scale from zero and one to values between negative and positive infinity 2.1.2 count data Raw counts that span several orders of magnitude are problematic for many models.In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. count transform binarise 0/1 if value quantizing the count or group the counts fixed-width binning, each bin contains a specific numeric range (ex age) If count have multiple magnitudes, group by powers of 10 ( 0–9, 10–99, 100–999, 1000–9999, etc) Quantile binning : adaptively positioning the bins based on the distribution of the data log transform 2.1.3 categorical data Use Dummy or keep factors with somes levels is same for most modeling. It suggest using the predictors without converting to dummy variables and, if the model appears promising, to also try refitting using dummy variables. unordered categorical data dummy coding : in Feature engineering, il recommande de flag chaque variable categorielle en varible binaire effect coding : -1 0 1 : -1 si different de categorie de reference. Effect coding is very similar to dummy coding, but results in linear regression models that are even simpler to interpret. Dealing with Large Categorical Variables do nothing dummy : create many variable with zero value for rare categories and add zero-variance predictor( computentional intencive ) delete rare value recode and regroup categorical data Compress the features. There are two choices: Feature hashing, popular with linear models. A hash function is a deterministic function that maps a potentially unbounded integer to a finite integer range [1, m]. Feature hashing compresses the original feature vector into an m-dimensional vector. It Converte large cat var into small hash feature (but hashing feature are uninterpretable) Bin counting, popular with linear models as well as trees. Rather than using the value of the categorical variable as the feature, use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict Ordered data how measure de force to pass between each categorie ? linear quadratic 2.1.4 Date Time : Lubridate package Use as.POSIXct() and UTC (universal coordinated time)in time zone. create new variables : weekend (0/1), bankholiday (0/1), … 2.2 Missing Value Do nothing remove impute by mean : doesn’t impact analysis by singular value decomposition : approximate true value by regression :approximate true value Check lien 5 methode impute missing value 2.3 Outlier Detection 2.4 Sampling and resampling Modern statistical methods assume that the underlying data comes from a random distribution. The performance measurements of models derived from data are also subject to random noise. the sample can be generalized for the population with statistical confidence. Is an approximatation. Weak law of large numbers : \\(\\bar{X_n} =&gt; \\mu\\) Central limit theorem : distribution standardis? tend vers une normale asymptotiquement model sampling : population data is already collected and you want to reduce time and the computational cost of analysis, along with improve the inference of your models survey sampling : create a sample design and then survey the population only to collect sample to save data collection costs. Type of sampling methods : Boostrap sampling : sampling with replacement Jackknife = leave one out sampling + calculate average of the estimation Vfold crossvalidation : Resampling methods that can generate V different versions of the training set (same size) that can be used to evaluate model on test set. Each of the V assessment sets contains 1/V of the training set and each of these exclude different data points. Suppose V = 10, then there are 10 different versions of 90% of the data and also 10 versions of the remaining 10% for each corresponding resample. in the end, there are V estimates of performance for the model and each was calculated on a different assessment set. The cross-validation estimate of performance is computed by averaging the V individual metrics. Monte Carlo : Produces splits that are likely to contain overlap. For each resample, a random sample is taken with π proportion of the training set going into the analysis set and the remaining samples allocated to the assessment set bootstrap : A bootstrap resample of the data is defined to be a simple random sample that is the same size as the training set where the data are sampled with replacement 2.5 variables selections How do we cleanly separate the signal from the noise? First Filter Na filter : column with to many NA Variance filter : Column with not enought variance to explain dataset corrélation filter : e will remove predictors that are highly correlated (r2 &gt; 0.9) with other predictors. see corrplot Variance treshold : Variable with high variability also have higher information in them. We remove all variables havant variance less than a treshold. 2.5.1 Filter methods : Select variables sans modélisation. Methode univariée. Order feature by importance. Methode robust contre overfitting mais peut selectionner variables redondantes. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step Chi square test Correlation coefficients information gain metrics fisher score variance treshold 2.5.2 Wrapper Methods: Test differentes combinaisons de feature selon crit?re de performance. Predictive model is used to evaluate the set of feature by accurancy metric. Méthode efficace pour la mod?lisation. Peut causé de l’overfitting. forward/backward selection recursive feature elimation algorithm … see supervised analysis 2.5.3 Embedded Methods : Next step to wrapper methods. Introduce a penalty factor to the evaluation criteria of the model to bias the model toward lower complexity. Balance between complexity and accurancy. Less computationally expensive than Wrapper. Less prone to overfitting. These methods perform feature selection as part of the model training process Lasso Ridge regression … Decision tree Gradiant descent methods 2.5.4 Dimension reduction : See unsuppervized section PCA see unsupervised analysis : Due to the orthogonality constraint in the objective function, PCA transformation produces a nice side effect: the transformed features are no longer correlated. svd k-means as a featurization procedure, a data point can be represented by its cluster membership 2.6 Example 2.6.1 Credit risk modeling Feature ranking Fit logistic model Calculate Gini coefficient rearrange variables ? combine, weighted sums, etc Need to understand variable individually ? use Filtering method data dirty ? detect outlier Data selection? use first ranking, forward selection and last Embedded method. Compare with crit?rion (misclassi, MSE, AIC, etc) improve performance? bootstrap : subsample your data et redo analysis ### Data Prep ### ################# library(MLmetrics) data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) #Create the default variable data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) print(table(data$default)*100/nrow(data)) ## ## 0 1 ## 90.635 9.365 # Without prior kwowledge : if more than 30 variable is continuous continuous &lt;-character() categorical &lt;-character() i = names(data)[1] p&lt;-1 q&lt;-1 for (i in names(data)){ unique_levels =length(unique(data[,i])) if(i %in% c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)){ next; }else if (unique_levels &lt;=30 |is.character(data[,i])){ categorical[p] &lt;-i p=p+1 data[[i]] &lt;-factor(data[[i]]) }else{ continuous[q] &lt;-i q=q+1 }} cat(&quot;\\nTotal number of continuous variables in feature set &quot;,length(continuous) -1) ## ## Total number of continuous variables in feature set 714 cat(&quot;\\nTotal number of categorical variable in feature set &quot;,length(categorical) -2) ## ## Total number of categorical variable in feature set 52 # Gini coef performance_metric_gini &lt;-data.frame(feature =character(), Gini_value =numeric()) # for (feature in names(data)){ # if(feature %in%c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)) { # next # } else { # tryCatch( # {glm_model &lt;-glm(default ~get(feature),data=data,family=binomial(link=&quot;logit&quot;)); # predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;); # Gini_value &lt;-Gini(predicted_values,data$default); # performance_metric_gini &lt;-rbind(performance_metric_gini,cbind(feature,Gini_value));},error=function(e){}) # } # } # # saveRDS(performance_metric_gini, &quot;performance_metric_gini.rds&quot;) performance_metric_gini &lt;- readRDS(&quot;./save/performance_metric_gini.rds&quot;) performance_metric_gini$Gini_value &lt;-as.numeric(as.character(performance_metric_gini$Gini_value)) Ranked_Features &lt;-performance_metric_gini[order(-performance_metric_gini$Gini_value),] head(Ranked_Features) ## feature Gini_value ## 389 f404 0.2579189 ## 710 f766 0.2578312 ## 585 f630 0.2415352 ## 584 f629 0.2354368 ## 321 f333 0.2352707 ## 56 f64 0.2348747 # Note : When you are running loops over large datasets, it is possible that the loop might stop due to some errors. to escape that, consider using the trycatch() function in r ################################################### ### Try logistic regression with top 5 features ### ################################################### glm_model &lt;-glm(default ~f766 +f404 +f629 +f630 +f281 +f322,data=data,family=binomial(link=&quot;logit&quot;)) predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;) Gini_value &lt;-Gini(predicted_values,data$default) summary(glm_model) ## ## Call: ## glm(formula = default ~ f766 + f404 + f629 + f630 + f281 + f322, ## family = binomial(link = &quot;logit&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6928 -0.4946 -0.4102 -0.3329 3.0013 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.502550 4.939282 -0.304 0.7610 ## f766 -0.010228 4.916519 -0.002 0.9983 ## f404 -1.395602 4.908606 -0.284 0.7762 ## f629 -0.306456 0.172632 -1.775 0.0759 . ## f630 -0.165047 0.128300 -1.286 0.1983 ## f281 0.007759 0.019386 0.400 0.6890 ## f322 0.264196 0.128472 2.056 0.0397 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12415 on 19938 degrees of freedom ## Residual deviance: 12040 on 19932 degrees of freedom ## (61 observations deleted due to missingness) ## AIC: 12054 ## ## Number of Fisher Scoring iterations: 5 Gini_value ## [1] 0.2697868 # Every features aren&#39;t always significant. Indication that features themselves are highly correlated. Gini coef has not improved. So investigate multicorrelation. # Variable ranking method is univariate and lead to the selection of a redundant variables. top_6_feature &lt;-data.frame(data$f766,data$f404,data$f629,data$f630,data$f281,data$f322) cor(top_6_feature, use=&quot;complete&quot;) ## data.f766 data.f404 data.f629 data.f630 data.f281 ## data.f766 1.0000000 0.9996754 0.6777553 0.6378040 0.8205665 ## data.f404 0.9996754 1.0000000 0.6774434 0.6374457 0.8204153 ## data.f629 0.6777553 0.6774434 1.0000000 0.9155376 0.6628148 ## data.f630 0.6378040 0.6374457 0.9155376 1.0000000 0.6202698 ## data.f281 0.8205665 0.8204153 0.6628148 0.6202698 1.0000000 ## data.f322 -0.7706228 -0.7707861 -0.5450001 -0.5048133 -0.7371242 ## data.f322 ## data.f766 -0.7706228 ## data.f404 -0.7707861 ## data.f629 -0.5450001 ## data.f630 -0.5048133 ## data.f281 -0.7371242 ## data.f322 1.0000000 2.6.2 variance treshold approach # Attention, les variables ne sont pas standardisées, on ne peut pas les comparer directement. On utilise le coeficient de variation :$c= \\fraq{\\sigma}{\\mu}$ # Calculate CV coefficient_of_variance &lt;-data.frame(feature =character(), cov =numeric()) for (feature in names(data)){ if(feature %in%c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)){next }else if(feature %in% continuous){ tryCatch({ cov &lt;-abs(sd(data[[feature]], na.rm =TRUE)/mean(data[[feature]],na.rm =TRUE)); if(cov !=Inf){ coefficient_of_variance &lt;-rbind(coefficient_of_variance,cbind(feature, cov)); } else {next} },error=function(e){}) }else{next} } coefficient_of_variance$cov &lt;-as.numeric(as.character(coefficient_of_variance$cov)) Ranked_Features_cov &lt;-coefficient_of_variance[order(-coefficient_of_variance$cov),] head(Ranked_Features_cov) ## feature cov ## 294 f338 128.05980 ## 377 f422 111.93083 ## 664 f724 69.64913 ## 349 f393 55.39446 ## 712 f775 47.64456 ## 350 f394 46.68719 ## Logistic model glm_model &lt;-glm(default ~f338 +f422 +f724 +f636 +f775 +f723,data=data, family=binomial(link=&quot;logit&quot;)); predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;) Gini_value &lt;-Gini(predicted_values,data$default) cat(&quot;The Gini Coefficient for the fitted model is &quot;,Gini_value); ## The Gini Coefficient for the fitted model is 0.1465253 Contrairement au Ranking avec Gini, les variables ne sont pas dominés par leur structure de correlation. Mais les variables ne sont pas toutes significatives individuellement et le coef GINI pas particuliérement amélioré. Avec variance treshlod on espére selectionné des variables indépendantes 2.7 Method Summary Variable quanti Variable quali Graph Time series, barplot, boxplot, histographe, QQplot, scaterplot barplot, boxplot Test t-test sur la moyenne, chi2 sur la variance, test normalité, corrélation, test F variance, test de levene test proportion, test ajustement, test indépendance Modélisation Régression linéaire régression logistique, analyse discriminante, abre décision Parametric : assume thaht sample data is drawn from a known probabilité distribution based on fixed set of parameters. For instance, linear regression assumes normal distribution, whereas logistic assumes binomial distribution, etc. This assumption allows the methods to be applied to small datasets as well. involve a two-step model-based approach : Chose model (ex : linear) and estimate (ex: ols) reduce the probleme of model estimation to a probleme of parameter estimation but if the chosen model is too far from the true f, then the estimate will be poor Non parametric : not assume any probabilty distribution or prior. Contruct empirical distributions from data. (= Kernel regression, NPMR) Models can also be evaluated in terms of variance and bias. A model has high variance if small changes to the underlying data used to estimate the parameters cause a sizable change in those parameters (or in the structure of the model) Model bias reflects the ability of a model to conform to the underlying theoretical structure of the data. A low bias model is one that can be highly flexible and has the capacity to fit a variety of different shapes and patterns. A high bias model would be unable to estimate values close to their true theoretical counterparts. Linear methods often have high bias since, without modification, cannot describe nonlinear patterns in the predictor variables. Tree-based models, support vector machines, neural networks, and others can be very adaptable to the data and have low bias. 2.8 tips Tidyverse package Given below are some of the rare feature engineering tricks implemented in the winning solutions of several data science competitions. - Transform data to Image - Meta-leaks - Representation learning features Mean encodings - Transforming target variable "],
["data-visualization.html", "Chapter 3 Data visualization 3.1 Descriptive 3.2 Caret Package 3.3 Spacial map", " Chapter 3 Data visualization 3.1 Descriptive #change theme =&gt; + theme() iris %&gt;% qplot(Petal.Width, Petal.Length , color = Species, data = .) cars %&gt;% ggplot(aes(x = speed, y = ..count..)) + geom_histogram(bins = 10) + geom_density() cars %&gt;% ggplot(aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = &quot;lm&quot;) # If non linear smooth : method = &#39;loess&#39; Multiple line longley %&gt;% ggplot(aes(x = Year)) + geom_point(aes(y = Unemployed)) + geom_point(aes(y = Armed.Forces), color = &quot;blue&quot;) + geom_line(aes(y = Unemployed)) + geom_line(aes(y = Armed.Forces), color = &quot;blue&quot;) Scaling cars %&gt;% ggplot(aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_x_reverse(&quot;Speed&quot;) + scale_y_continuous(&quot;Stopping Distance&quot;) iris %&gt;% ggplot(aes(x = Species, y = Petal.Length)) + geom_boxplot() + geom_jitter(width = 0.1, height = 0.1) + scale_x_discrete(labels = c(&quot;setosa&quot; = &quot;Setosa&quot;, &quot;versicolor&quot; = &quot;Versicolor&quot;, &quot;virginica&quot; = &quot;Virginica&quot;)) Correlation plot Pearson correlation : relation lin?aire \\[\\rho(X,Y)=\\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y} \\] library(corrplot) correlation_world &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 4/Correlation/Correlation Data.csv&quot;) corrplot(cor(correlation_world[,2:6],method =&quot;pearson&quot;),diag =FALSE, title =&quot;Correlation Plot&quot;, method =&quot;ellipse&quot;, tl.cex =0.7, tl.col =&quot;black&quot;, cl.ratio =0.2 ) 3.2 Caret Package data(&quot;iris&quot;) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # FeaturePlot For classification target library(AppliedPredictiveModeling) transparentTheme(trans = .4) library(caret) ## Warning: package &#39;caret&#39; was built under R version 3.3.3 ## Loading required package: lattice ## Scatter plot featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;ellipse&quot;, auto.key = list(columns = 3)) ## Add a key at the top ## Density plot transparentTheme(trans = .9) featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;density&quot;, ## Pass in options to xyplot() to make it prettier scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(4, 1), auto.key = list(columns = 3)) ## BoxPlot featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;box&quot;, ## Pass in options to bwplot() scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(4,1 ), auto.key = list(columns = 2)) # FeaturePlot For regression target library(mlbench) ## Warning: package &#39;mlbench&#39; was built under R version 3.3.3 data(BostonHousing) regVar &lt;- c(&quot;age&quot;, &quot;lstat&quot;, &quot;tax&quot;) theme1 &lt;- trellis.par.get() theme1$plot.symbol$col = rgb(.2, .2, .2, .4) theme1$plot.symbol$pch = 16 theme1$plot.line$col = rgb(1, 0, 0, .7) theme1$plot.line$lwd &lt;- 2 trellis.par.set(theme1) featurePlot(x = BostonHousing[, regVar], y = BostonHousing$medv, plot = &quot;scatter&quot;, type = c(&quot;p&quot;, &quot;smooth&quot;), span = .5, layout = c(3, 1)) 3.3 Spacial map Static map qmap from ggplot Esay way : Machine learning with R, chap4 Interactive map : leaflet More information on https://rstudio.github.io/leaflet for more complex cartographie, check geoJSON https://rstudio.github.io/leaflet/json.html gps geolocalisation : https://github.com/AugustT/shiny_geolocation library(leaflet) lat = seq(50, 51 ,by= 0.005) lon = seq(4,5, by=0.005) coords &lt;- as.data.frame(cbind(Longitude = sample(lon,50), Latitude = sample(lat,50))) coords$V3 = as.factor(rep(&quot;Amandine&quot;,50)) coords$V4 = rep(seq(1,5,by=1),10) # simple use # Possibilité d&#39;utilise d&#39;autre map que google open street map m &lt;- leaflet() %&gt;% setView(lng = 4.8, lat = 50.5, zoom = 10) m %&gt;% addProviderTiles(providers$Stamen.Toner) m %&gt;% addProviderTiles(providers$Esri.NatGeoWorldMap) # cartographe library(maps) mapStates = map(&quot;state&quot;, fill = TRUE, plot = FALSE) leaflet(data = mapStates) %&gt;% addTiles() %&gt;% addPolygons(fillColor = topo.colors(10, alpha = NULL), stroke = FALSE) # modifier les markeret pop up leaflet(coords) %&gt;% addTiles() %&gt;% addMarkers(clusterOptions = markerClusterOptions(), popup = coords$V4) leaflet(coords) %&gt;% addTiles() %&gt;% addCircles(lng = ~Longitude, lat = ~Latitude, weight = 1, radius = ~V4^2 * 30, popup = ~V3 ) # rectangle zone leaflet() %&gt;% addTiles() %&gt;% addRectangles( lng1=-118.456554, lat1=34.078039, lng2=-118.436383, lat2=34.062717, fillColor = &quot;transparent&quot; ) "],
["regression.html", "Chapter 4 Regression 4.1 introduction 4.2 Linear regression 4.3 ANOVA 4.4 Polynomiale regression 4.5 Logistique 4.6 Generalized Linear Models 4.7 Model Selection 4.8 Regularization Algorithms 4.9 Locally estimated Scaterplot Smoothing (LOESS)", " Chapter 4 Regression load(&quot;./save/BreastCancer.Rdata&quot;) #import Data_Purchase_Prediction &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/PurchasePredictionDataset.csv&quot;,header=TRUE) library(IDPmisc) library(Metrics) library(MASS) library(&quot;lattice&quot;) library(car) 4.1 introduction First supervized learning. Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y. Dependante variable is available and regression use others predictives variables to estimate regression coefficient. 4.2 Linear regression Model : \\(Y=\\alpha + X \\beta + \\epsilon\\) Linéaire: on suppose distribition normal \\(\\alpha\\) :intercepte : la reponse moyenne si les variables explicatives sont zéro Remarque Categorical data : set to as factor Check Missing value : delete, impute, new catégorie Hypothèses : \\(rang(X) = p\\) =&gt; Rang est connu, exclus la multicolinéarité X est une matrice déterminée \\(\\epsilon\\) sont des erreurs indépendantes \\(E(\\epsilon) = 0\\) =&gt; erreur de moyenne nulle (normalité des résidus) \\(var(\\epsilon) = \\sigma_2 In\\) =&gt; variance Homoskédastique non autocorrélé Estimation et propriétés des estimateurs : Estimation par moindres carrés ordinaires : Minimise les squares error. Estimateur le plus efficace dans la classe des estimateurs non biaisé :BLUE \\(E[Y] = X \\beta\\) \\(Var(Y) = \\sigma In\\) \\(E[\\hat(\\beta)] = \\beta\\) \\(var(\\hat(\\beta)) = \\sigma (X&#39;X)^(-1)\\) Si \\(\\epsilon ~ N(0, \\sigma In)\\), alors \\(\\hat(\\beta) ~ N(\\beta, \\sigma^2 (X&#39;X)^(-1))\\) \\[SSTO = SSR + SSE\\] \\[\\sum{(Y_i - \\bar{Y})^2} = \\sum{(\\hat{Y}_i - \\bar{Y})^2} + \\sum{(Y_i - \\hat{Y})^2}\\] Diagnostiques : F-test : \\(H_0 : \\beta_i = 0 \\forall i\\) stat de test : \\(\\frac{(SSTO - SSE)/(p-1)}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F(p-1,n-1)\\) coefficient de détermination multiple \\(R^2\\) : mesure de qualité d’ajustement \\(\\frac{SSR/SSTO}\\) Multicolinéarité : forte corrélation entre variables explicatives Conséquence : Interprétation des coéfficients impossibles Diagnostiques : variance des coefficients très larges, coefficients varient beaucoup a l’ajouts/retrait de variables, coefficients ont signes non intruitifs Calcule des VIF (variance inflation factor) : si mpoyenne des VIF &gt; 1 ou un VIF &gt;10) \\(tolérance = 1-R²\\) et \\(VIF = \\frac{1}{tolérance}\\) Solution : Supprimer des variables, regression de Ridge (permet l’inversion de la matrice X’X qui est impossible en cas de multicolinéarité parfaite) Linéarité : Graph des résidus Vs régresseurs Si forme connue : transformer les regressieurs (log, sqrt) ou ajouté un terme (quadratique, log, d’interaction, …) Homoskédasticité : graph résidus vs valeurs prédites, test de Breush et Pagan, BreushPAgan, Berlett test, arch test Variance des erreurs indépendante des variable explicative Estimation reste correcte sous homoskédasticité : utilisé une variance corrigé : Régression de white Erreur Non indépendante : test d’autocorrélation Dubin watson test, plot acf If résidual show definite relationship with prior résidual (like autocorrelation), the noise isn’t random and we still have some information that we can extract and put in the model Problème de modèle : passer en log lin, oubli de régresseur (qui est autocorrélé), inclure des lag de la variable dépendante Normalité des erreurs : QQplot, test de Jarque Berra, KS test estimation correcte mais interprétation des tests et des IC sont faussées car basé sur la normalité théorie des grand nombre, si assez observations, estimateur OLS est assymptiquement normal et les test et IC tendent assymptotiquement Influential Point Analysis: Les valeurs abérantes peuvent crée des biais dans les estimateurs. Si trop extreme, on peut les deletes, check, impute, … DFFITS DFBETAS Distance de Cooks : \\[ D_i = \\frac{e²_i}{s²p} [\\frac{h_i}{(1-h_i)²}]\\] where \\(s²= (n-p)^{-1}e^Te\\) est la moyenne des erreurs quadratiques de la regression. Et \\(h_i =x^T(x^Tx)^{-1}\\). Avec cutoff \\(D_i &gt; 4/(n-k-1)\\) ou k est le nombre de paramètre Distance de Cook mesure l’effet of deleting a given observation. Si supprimer des observations cause grosse influence, alors ce point est suppiser etre outlier. Evaluation : RMSE = sqrt(mean($residuals)^2) ou $residuals = actual-predicted Interprétation : Pour une augmentation de une unité de speed, dist augmente de 3.9324. Intercepte donne la dist si speed vaut zero reglin = lm(dist~ speed, data=cars) summary(reglin) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 y = cars$dist x = cars$speed res &lt;-stack(data.frame(Observed = y, Predicted=fitted(reglin))) res &lt;-cbind(res, x =rep(x, 2)) #Plot using lattice xyplot(function) library(&quot;lattice&quot;) xyplot(values ~x, data = res, group = ind, auto.key =TRUE) sqrt(mean(residuals(reglin)^2)) ## [1] 15.06886 rmse(cars$dist,predict(reglin)) ## [1] 15.06886 # Normalité des résidus sresid = studres(reglin) sresid=NaRV.omit(sresid) hist(sresid, freq=FALSE, main=&quot;Distribution of Studentized Residuals&quot;,breaks=25) xfit&lt;-seq(min(sresid),max(sresid),length=40) yfit&lt;-dnorm(xfit) lines(xfit, yfit) ## ADD QQplot ## test normalité (attention juste indicateur) ks.test(reglin$residuals,pnorm,alternative=&quot;two.sided&quot;) ## ## One-sample Kolmogorov-Smirnov test ## ## data: reglin$residuals ## D = 0.49833, p-value = 3.283e-11 ## alternative hypothesis: two-sided shapiro.test(reglin$residuals) ## ## Shapiro-Wilk normality test ## ## data: reglin$residuals ## W = 0.94509, p-value = 0.02152 # Multicolinnéarité : VIF # vif(reglin) # residual autocorrelation : H0 = pas d&#39;autocorrélation durbinWatsonTest(reglin) ## lag Autocorrelation D-W Statistic p-value ## 1 0.1604322 1.676225 0.194 ## Alternative hypothesis: rho != 0 plot(acf(reglin$residuals)) # Homoskédasticité : breush pagan test # h0 : variance hétéscedastic ncvTest(reglin) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 4.650233 Df = 1 p = 0.03104933 #plot resi vs fit : detect non liearité, heterocedasticity, outlier # if random = ok plot(reglin$residuals,reglin$fitted.values) # cook&#39;s distance cutoff &lt;-4/((nrow(cars)-length(reglin$coefficients)-1)) plot(reglin, which=4, cook.levels=cutoff) # taille du cercle proportionnel a la distance de cook influencePlot(reglin, id.method=&quot;identify&quot;,main=&quot;Influence Plot&quot;, sub=&quot;Circle size is proportional to Cook&#39;s Distance&quot;, id.location=NULL) outlierTest(reglin) ## ## No Studentized residuals with Bonferonni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferonni p ## 49 3.184993 0.0025707 0.12853 # now investigate vs mean of data variable 4.3 ANOVA 4.4 Polynomiale regression Si la relation entre variables explicatives et variable dépendante n’est pas linéaire. Possibilité d’augmenter la relation dans des haut degré polynomials mais will cause overfitting. \\[ y_i = \\alpha_0 + \\alpha_i x_i + \\alpha_2 x²_i+ ... + \\epsilon_i\\] Exemple : Dependant variable = price of a commodity Explicative variable = quantiée vendue The general principle is if the price is too cheap, people will not buy the commodity thinking it’s not of good quality, but if the price is too high, people will not buy due to cost consideration. Let’s try to quantify this relationship using linear and quadratic regression y &lt;-as.numeric(c(&quot;3.3&quot;,&quot;2.8&quot;,&quot;2.9&quot;,&quot;2.3&quot;,&quot;2.6&quot;,&quot;2.1&quot;,&quot;2.5&quot;,&quot;2.9&quot;,&quot;2.4&quot;,&quot;3.0&quot;,&quot;3.1&quot;,&quot;2.8&quot;,&quot;3.3&quot;,&quot;3.5&quot;,&quot;3&quot;)) x&lt;-as.numeric(c(&quot;50&quot;,&quot;55&quot;,&quot;49&quot;,&quot;68&quot;,&quot;73&quot;,&quot;71&quot;,&quot;80&quot;,&quot;84&quot;,&quot;79&quot;,&quot;92&quot;,&quot;91&quot;,&quot;90&quot;,&quot;110&quot;,&quot;103&quot;,&quot;99&quot;)); linear_reg &lt;-lm(y~x) summary(linear_reg) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.66844 -0.25994 0.03346 0.20895 0.69004 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.232652 0.445995 5.006 0.00024 *** ## x 0.007546 0.005463 1.381 0.19046 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3836 on 13 degrees of freedom ## Multiple R-squared: 0.128, Adjusted R-squared: 0.06091 ## F-statistic: 1.908 on 1 and 13 DF, p-value: 0.1905 plot(y) lines(linear_reg$fitted.values) quad_reg &lt;-lm(y~x +I(x^2) ) summary(quad_reg) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43380 -0.13005 0.00493 0.20701 0.33776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8737010 1.1648621 5.901 7.24e-05 *** ## x -0.1189525 0.0309061 -3.849 0.00232 ** ## I(x^2) 0.0008145 0.0001976 4.122 0.00142 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2569 on 12 degrees of freedom ## Multiple R-squared: 0.6391, Adjusted R-squared: 0.5789 ## F-statistic: 10.62 on 2 and 12 DF, p-value: 0.002211 plot(y) lines(quad_reg$fitted.values) # improvement in R square, quadratic term significant 4.5 Logistique 4.5.1 General Variable dépendante binaire : binomially distribued binomial distribution probability mass function : \\(f(k;n,p) = P(X=k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k}\\) Trois classe de modèle logistiques: binomial logistic regression : var dépendante soit 0 soit 1 multinomial logistic regression : 3 ouplus niveu pour la variable dépendante (on utilise ditribution multinomiale) ordered logistic regression Transformation logit : fonction de lien pour la regression : \\(logit = \\frac{e^t}{e^t+1}=\\frac{1}{1+e^{-t}}\\) LA cote : représente la relation entre presence/absence d’un event odd = P(A)/(1-P(A)) un odd de 2 pour un event A mean l’event est deux fois plus probable qu’il se réalise que rien ne se réalise. Odd Ratio : rapport des cotes = Odd(A) / Odd(B) SI OR = 2 : Chanque que B se réalise sont deux fois suppérieur a celle de A 4.5.2 Binomial Logistic MODEL Model : \\[ logit(p_i) = \\ln(\\frac{p_i}{1-p_i}) = \\beta_0 + \\beta X \\] Hypothèses : Estimation par MLE ou itérative avec optimisation du logLoss Diagnostiques : Si but est classification : check les predictions et classement Si but est analyse des coefficients : vérification des hypothèsese stat Wald test : same a t-test in reg lin. Test sur les levels des variables sont individuellements significatifs. Suit une distri chi-square. pseudo R-square : Mesure la proportion de variance expliqué par le modele. Mesure la différence entre la déviance un model null et fitted. Calcul par le likelihood ratio : \\[R²_i = \\frac{D_{null} - D_{fitted}}{D_{null}}\\] ou D est la déviance : $ D = - 2ln $ Bivariate plot : observed and predictied vs variable explicative. Plot donne info sur comme le model sur comporte selon les différent niveau Matrice de classification : - Spécificity = combien de negatif le model prédit correctement - sensitivity = combien de positif le model prédit correctement library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.3 library(mlbench) ## Warning: package &#39;mlbench&#39; was built under R version 3.3.3 BreastCancer$Cl.thickness = as.numeric(as.character(BreastCancer$Cl.thickness)) BreastCancer$IsMalignant = ifelse( BreastCancer$Class== &quot;benign&quot;, 0, 1) ggplot(data =BreastCancer, aes(x = Cl.thickness, y = IsMalignant)) + geom_jitter(height = 0.05, width = 0.3, alpha=0.4) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) reglog = glm(IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;, data = BreastCancer) summary(reglog) ## ## Call: ## glm(formula = IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;, ## data = BreastCancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## Cl.thickness 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 table(BreastCancer$Class, ifelse(predict(reglog, BreastCancer) &lt; 0.5, 0, 1)) ## ## 0 1 ## benign 453 5 ## malignant 94 147 4.5.3 Multinomial Logistic Regression Variable dépendante a plus de une catégorie et suit une distribution multinomiale. On fait une regression logistic pour chaque classe et combine dans un seul equation sous contrainte que la somme des probabilités vallent 1. Estimation par iterative optimization of the LogLoss function. But : clairement de la classification. Deux méthode possible : Pick de highest probability : classe dans la classe qui a le plus haute probabilité par rapport au autres classe. Méthode soufre de la “Class imbalance probleme” (si les classes sont non equilibré, tendance à toujours assigner dans la plus grande classe) Ratio of probabilities : prendre la ratio des probabilité prédite et la prior distribution and choisir la classe basé sur le plus haut ratio. Cette méthode normalise les probabilité par le ratio du prior pour réduire le biais liéà la distribution du pior Data_Purchase&lt;-na.omit(Data_Purchase_Prediction) rownames(Data_Purchase)&lt;-NULL #Random Sample for easy computation Data_Purchase_Model&lt;-Data_Purchase[sample(nrow(Data_Purchase),10000),] # prior distribution table(Data_Purchase_Model$ProductChoice) ## ## 1 2 3 4 ## 2217 3849 2925 1009 # multinomial model library(nnet) mnl_model &lt;-multinom (ProductChoice ~MembershipPoints +IncomeClass + CustomerPropensity +LastPurchaseDuration +CustomerAge +MartialStatus, data = Data_Purchase) ## # weights: 44 (30 variable) ## initial value 672765.880864 ## iter 10 value 615285.850873 ## iter 20 value 607471.781374 ## iter 30 value 607231.472034 ## final value 604217.503433 ## converged mnl_model ## Call: ## multinom(formula = ProductChoice ~ MembershipPoints + IncomeClass + ## CustomerPropensity + LastPurchaseDuration + CustomerAge + ## MartialStatus, data = Data_Purchase) ## ## Coefficients: ## (Intercept) MembershipPoints IncomeClass CustomerPropensityLow ## 2 0.77137077 -0.02940732 0.00127305 -0.3960318 ## 3 0.01775506 0.03340207 0.03540194 -0.8573716 ## 4 -1.15109893 -0.12366367 0.09016678 -0.6427954 ## CustomerPropensityMedium CustomerPropensityUnknown ## 2 -0.2745419 -0.5715016 ## 3 -0.4038433 -1.1824810 ## 4 -0.4035627 -0.9769569 ## CustomerPropensityVeryHigh LastPurchaseDuration CustomerAge ## 2 0.2553831 0.04117902 0.001638976 ## 3 0.5645137 0.05539173 0.005042405 ## 4 0.5897717 0.07047770 0.009664668 ## MartialStatus ## 2 -0.033879645 ## 3 -0.007461956 ## 4 0.122011042 ## ## Residual Deviance: 1208435 ## AIC: 1208495 # Modele converge en 30itérations. #Predict the probabilities predicted_test &lt;-as.data.frame(predict(mnl_model, newdata = Data_Purchase, type=&quot;probs&quot;)) ## méthode 1 : the prediction based in highest probability test_result &lt;-apply(predicted_test,1,which.max) result &lt;-as.data.frame(cbind(Data_Purchase$ProductChoice,test_result)) colnames(result) &lt;-c(&quot;Actual Class&quot;, &quot;Predicted Class&quot;) table(result$`Actual Class`,result$`Predicted Class`) ## ## 1 2 3 ## 1 302 91952 12365 ## 2 248 150429 38028 ## 3 170 90944 51390 ## 4 27 32645 16798 # bon résultat pour classe 123 mais pour classe 4 pas un seul case de classé. ## Methode 2 : normalisation avec la ditribution du prior prior &lt;-table(Data_Purchase_Model$ProductChoice)/nrow(Data_Purchase_Model) prior_mat &lt;-rep(prior,nrow(Data_Purchase_Model)) pred_ratio &lt;-predicted_test/prior_mat test_result &lt;-apply(pred_ratio,1,which.max) result &lt;-as.data.frame(cbind(Data_Purchase$ProductChoice,test_result)) colnames(result) &lt;-c(&quot;Actual Class&quot;, &quot;Predicted Class&quot;) table(result$`Actual Class`,result$`Predicted Class`) ## ## 1 2 3 4 ## 1 21734 63819 19039 27 ## 2 28692 111634 48307 72 ## 3 14183 76795 51467 59 ## 4 4713 27751 16959 47 4.6 Generalized Linear Models Pour GLM, on suppose que la variable dépendante est issue de la famille de ditribution exponentielle incluant la normal, binomial, poisson, gamma, … etc. \\[ E(Y) = \\mu = g^{-1}(X\\beta) \\] In R : glm(formula, family=familytype(link=linkfunction), data=) - binomial, (link = “logit”) : modele logistique - gaussian, (link= “identity”) : modèle linéaire - Gamma, (link= “inverse”) : analyse de survie (time to failure of a machine in the industry) - poisson, (link = “log”) : How many calls will the call center receive today? 4.7 Model Selection - **Stepwise** : ajoute séquentielement la variables la plus significative. Après chaqeu ajout,le modèle réévalue la significativité des autres variables. Step : Model with 1 best feature, add next variables that maximise the evaluation function, ... Proc?dure tr?s lourde. parfois necessaire d&#39;utiliser FIlter m?thod avant. ### Data prep ### ################# ## Data with best feature from Filter method data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) data_model &lt;-na.omit(data[,c(&quot;id&quot;,&quot;f338&quot;,&quot;f422&quot;,&quot;f724&quot;,&quot;f636&quot;,&quot;f775&quot;,&quot;f222&quot;,&quot;f93&quot;,&quot;f309&quot;,&quot;f303&quot;,&quot;f113&quot;,&quot;default&quot;),]) ### Forward ### ############### full_model &lt;-glm(default ~f338 +f422 +f724 +f636 +f775 +f222 +f93 +f309+f303 +f113,data=data_model,family=binomial(link=&quot;logit&quot;)) null_model &lt;-glm(default ~1 ,data=data_model,family=binomial(link=&quot;logit&quot;)) forwards &lt;-step(null_model,scope=list(lower=formula(null_model),upper=formula(full_model)), direction=&quot;forward&quot;) ## Start: AIC=11175.3 ## default ~ 1 ## ## Df Deviance AIC ## + f422 1 11136 11140 ## + f113 1 11150 11154 ## + f222 1 11150 11154 ## + f775 1 11165 11169 ## + f93 1 11168 11172 ## + f309 1 11171 11175 ## + f303 1 11171 11175 ## &lt;none&gt; 11173 11175 ## + f636 1 11172 11176 ## + f338 1 11173 11177 ## + f724 1 11173 11177 ## ## Step: AIC=11140.24 ## default ~ f422 ## ## Df Deviance AIC ## + f113 1 11113 11119 ## + f222 1 11114 11120 ## + f775 1 11129 11135 ## + f93 1 11131 11137 ## &lt;none&gt; 11136 11140 ## + f303 1 11135 11141 ## + f309 1 11135 11141 ## + f636 1 11135 11141 ## + f338 1 11136 11142 ## + f724 1 11136 11142 ## ## Step: AIC=11118.59 ## default ~ f422 + f113 ## ## Df Deviance AIC ## + f222 1 11096 11104 ## + f775 1 11106 11114 ## &lt;none&gt; 11113 11119 ## + f93 1 11111 11119 ## + f303 1 11112 11120 ## + f636 1 11112 11120 ## + f309 1 11112 11120 ## + f338 1 11112 11120 ## + f724 1 11113 11121 ## ## Step: AIC=11103.78 ## default ~ f422 + f113 + f222 ## ## Df Deviance AIC ## + f775 1 11090 11100 ## &lt;none&gt; 11096 11104 ## + f303 1 11095 11105 ## + f636 1 11095 11105 ## + f309 1 11095 11105 ## + f93 1 11095 11105 ## + f338 1 11096 11106 ## + f724 1 11096 11106 ## ## Step: AIC=11099.57 ## default ~ f422 + f113 + f222 + f775 ## ## Df Deviance AIC ## + f303 1 11087 11099 ## &lt;none&gt; 11090 11100 ## + f309 1 11088 11100 ## + f636 1 11089 11101 ## + f93 1 11089 11101 ## + f338 1 11090 11102 ## + f724 1 11090 11102 ## ## Step: AIC=11098.6 ## default ~ f422 + f113 + f222 + f775 + f303 ## ## Df Deviance AIC ## &lt;none&gt; 11087 11099 ## + f636 1 11086 11100 ## + f93 1 11086 11100 ## + f309 1 11086 11100 ## + f338 1 11086 11100 ## + f724 1 11087 11101 #best model with AIC criteria formula(forwards) ## default ~ f422 + f113 + f222 + f775 + f303 4.8 Regularization Algorithms 4.8.1 Ridge regression 4.8.2 Least Absolute Shrinkage and Selection Opérator LASSO 4.8.3 Elastic Net 4.8.4 Leas-Angle Regression LARS - **Lasso** dd penalty term against the complexity to reduce the degree of overfittingor the variance of the model by adding additional bas. Check formul LASSO Objective function for the penalized logistic regression: $ - [1/N y (_0 + x^T_t ) - (1 + ) ] + lambda[(1-)||||^2_2 ]$ library(&quot;glmnet&quot;) ### Data prep ### ################# data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) data_model &lt;-na.omit(data) y &lt;-as.matrix(data_model$default) # x &lt;-as.matrix(subset(data_model, select=continuous[250:260])) x &lt;-as.matrix(data_model[,250:260]) fit =glmnet(x,y, family=&quot;binomial&quot;) summary(fit) ## Length Class Mode ## a0 52 -none- numeric ## beta 572 dgCMatrix S4 ## df 52 -none- numeric ## dim 2 -none- numeric ## lambda 52 -none- numeric ## dev.ratio 52 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## classnames 2 -none- character ## call 4 -none- call ## nobs 1 -none- numeric plot (fit, xvar=&quot;dev&quot;, label=TRUE) #Fit a cross validated binomial model fit_logistic =cv.glmnet(x,y, family=&quot;binomial&quot;, type.measure=&quot;class&quot;) plot (fit_logistic) # on est sens? voir un tendance dans les points rouge. on veut le labda qui minimum le taux de mauvaise classifications print(fit_logistic$lambda.min) ## [1] 0.01919422 param &lt;-coef(fit_logistic, s=&quot;lambda.min&quot;) param &lt;-as.data.frame(as.matrix(param)) param$feature&lt;-rownames(param) #The list of variables suggested by the embedded method param_embeded &lt;-param[param[,2]&gt;0,] param_embeded ## 1 feature ## f251 0 f251 ## f252 0 f252 ## f253 0 f253 ## f254 0 f254 ## f255 0 f255 ## f256 0 f256 ## f257 0 f257 ## f258 0 f258 ## f259 0 f259 ## f260 0 f260 ## f261 0 f261 ridge 4.9 Locally estimated Scaterplot Smoothing (LOESS) "],
["unsupervised.html", "Chapter 5 Unsupervised 5.1 Dimensionality reduction algorithms 5.2 Cluster analysis 5.3 Evaluation of clustering 5.4 Association Rule Mining Algorithms 5.5 Singular Value decomposition 5.6 K-Nearest Neighbot 5.7 Others unsuppervised algorithms", " Chapter 5 Unsupervised Pas de variable dépendante, découverte des données 5.1 Dimensionality reduction algorithms PCA L’objectif est de réduire la dimension des données pour obtenir une meilleur visualisation. PCA is a transformation of the data but don’t add or delete any information. Only numerical data, if factors : transforme en vecteur numérique sinon use FDA Hughes phenomeon : With a fixed number of training samples, the predictive power reduces as the dimensionality increases. But réduire la variabilité d’un dataset. On crée de nouvelles variables orthogonales qui explique le plus possible de variances des variables. basé sur la matrice des covariances pca &lt;- prcomp(subset(iris, select = -Species)) pca ## Standard deviations: ## [1] 2.0562689 0.4926162 0.2796596 0.1543862 ## ## Rotation: ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 plot(pca) mapped_iris &lt;- as.data.frame(predict(pca, iris)) mapped_iris &lt;-cbind(mapped_iris, Species = iris$Species) ggplot() + geom_point(data=mapped_iris,aes(x = PC1, y = PC2, colour = Species)) pca_data &lt;-data[,c(&quot;f381&quot;,&quot;f408&quot;,&quot;f495&quot;,&quot;f529&quot;,&quot;f549&quot;,&quot;f539&quot;,&quot;f579&quot;,&quot;f634&quot;,&quot;f706&quot;,&quot;f743&quot;)] pca_data &lt;-na.omit(pca_data) #Normalise the data before applying PCA analysis mean=0, and sd=1 scaled_pca_data &lt;-scale(pca_data) pca_results &lt;-prcomp(scaled_pca_data) plot(pca_results) MCA principal component regression Partial least square regression supervised dimension reduction techniques use the response to guide the dimension reduction of the predictors such that the new predictors are optimally related to the response. Partial least squares (PLS) is a supervised version of PCA that reduces dimension in a way that is optimally related to the response. Specifically, the objective of PLS is to find linear functions (called latent variables) of the predictors that have optimal covariance with the response. This means that the response guides the dimension reduction such that the scores have the highest possible correlation with the response in the training data. Multidimendional scaling MDS Linear discriminant Analysis LDA Mixture discriminant Analysis MDA Quadratic discriminant analysis QDA Kernel Principal Component Analysis Principal component analysis is an effective dimension reduction technique when predictors are linearly correlated and when the resulting scores are associated with the response. However, the orthogonal partitioning of the predictor space may not provide a good predictive relationship with the response, especially if the true underlying relationship between the predictors and the response is non-linear =&gt; see scaterplot Non-negative Matrix Factorization linear projection method that is specific to features that are are positive or zero. In this case, the algorithm finds the coefficients of A such that their values are also non-negative (thus ensuring that the new features have the same property). This approach is popular for text data where predictors are word counts, imaging, and biological measures Autoencodres Autoencoders are computationally complex multivariate methods for finding representations of the predictor data and are commonly used in deep learning models (Goodfellow, Bengio, and Courville 2016). The idea is to create a nonlinear mapping between the original predictor data and a set artificial features. One situation to use an autoencoder is when there is an abundance of unlabeled data 5.2 Cluster analysis Group data in most homegenous group. Clustering toujours la même chose, le truc qui change est la metrique. Différent type de clustering : - Connectivity models : Distance connectivity between observations is the measure, e.g., hierarchical clustering. - Centroid models : Distance from mean value of each observation/cluster is the measure, e.g., k-means. - Distribution models : Significance of statistical distribution of variables in the dataset is the measure, e.g., expectation maximization algorithms. - Density models: Density in data space is the measure, e.g., DBSCAN models. - Hard Clustering: Each object belongs to exactly one cluster - Soft Clustering : Each object has some likelihood of belonging to a different cluster Remarque : pas de selection de variables dans le clustering, il faut porter de l’attention sur le dataset et les variables utilisées. A good clustering algorithm can be evaluated based on two primary objectives: - High intra-class similarity - Low inter-class similarity Choix de la mesure de similarité important # introduction to dataset Data_House_Worth &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/House Worth Data.csv&quot;,header=TRUE); str(Data_House_Worth) ## &#39;data.frame&#39;: 316 obs. of 5 variables: ## $ HousePrice : int 138800 155000 152000 160000 226000 275000 215000 392000 325000 151000 ... ## $ StoreArea : num 29.9 44 46.2 46.2 48.7 56.4 47.1 56.7 84 49.2 ... ## $ BasementArea : int 75 504 493 510 445 1148 380 945 1572 506 ... ## $ LawnArea : num 11.22 9.69 10.19 6.82 10.92 ... ## $ HouseNetWorth: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Medium&quot;: 2 3 3 3 3 1 3 1 1 3 ... Data_House_Worth$BasementArea &lt;-NULL ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth))+geom_point() Hierarchical Clustering Chaque data sont dans un cluster. Les clusters sont aggrégé hiérachiquement en fonction d’une distance la plus faible jusqu’au moment ou il ne reste qu’un cluster. Hierarchical clustering is based on the connectivity model of clusters. The steps involved in the clustering process are: Start with N clusters,(i.e., assign each element to its own cluster). Now merge pairs of clusters with the closest to other Again compute the distance (similarities) and merge with closest one. Repeat Steps 2 and 3 to exhaust the items until you get all data points in one cluster. Chose cutoff at how many clusters you want to have. library(ggplot2) library(ggdendro) ## Warning: package &#39;ggdendro&#39; was built under R version 3.3.3 # Hierachical clustering clusters &lt;-hclust(dist(Data_House_Worth[,2:3])) #Plot the dendogram plot(clusters) #create different number of cluster clusterCut_2 &lt;-cutree(clusters, 2) #table the clustering distribution with actual networth table(clusterCut_2,Data_House_Worth$HouseNetWorth) ## ## clusterCut_2 High Low Medium ## 1 104 135 51 ## 2 26 0 0 clusterCut_3 &lt;-cutree(clusters, 3) table(clusterCut_3,Data_House_Worth$HouseNetWorth) ## ## clusterCut_3 High Low Medium ## 1 0 122 1 ## 2 104 13 50 ## 3 26 0 0 # ici choix du nombre cluster = 3 par hypothèse sur le business ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) + geom_point(alpha =0.4, size =3.5) +geom_point(col = clusterCut_3) + scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;)) ## Iris Example iris_dist &lt;- dist(scale(subset(iris, select = -Species))) clustering &lt;- hclust(iris_dist) plot(clustering) ggdendrogram(clustering) + theme_dendro() clusters = cutree(clustering,k = 3) data = cbind(mapped_iris, Cluster = clusters) ggplot() + geom_point(data= data, aes(x = PC1, y = PC2, shape = Species, colour = Cluster)) K-means clustering K-means place observations into Kclusters by minimizing the wihtin-cluster sum of squares (WCSS). WCSS est la somme des distance entre chaque observation et le centre du cluster. Algorithm : - Assignment: Assign each observation to the cluster that gives the minimum within cluster sum of squares (WCSS). - Update: Update the centroid by taking the mean of all the observation in the cluster. - These two steps are iteratively executed until the assignments in any two consecutive iteration don’t change To find the optimal value of k, we use and Elbow curve that show percentage of variance explained as a functionof nombrr of cluster wss &lt;-(nrow(Data_House_Worth)-1)*sum(apply(Data_House_Worth[,2:3],2,var)) for (i in 2:15) { wss[i]&lt;-sum(kmeans(Data_House_Worth[,2:3],centers=i)$withinss) } plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;,ylab=&quot;Within groups sum of squares&quot;) # 3 cluster explain most of the variance in data. 4cluster not more interest and not in concordance with intuition # Model Cluster_kmean &lt;-kmeans(Data_House_Worth[,2:3], 3, nstart =20) table(Cluster_kmean$cluster,Data_House_Worth$HouseNetWorth) ## ## High Low Medium ## 1 46 13 50 ## 2 0 122 1 ## 3 84 0 0 Cluster_kmean$cluster &lt;-factor(Cluster_kmean$cluster) ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) + geom_point(alpha =0.4, size =3.5) +geom_point(col = Cluster_kmean$cluster) +scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;)) # Capture cluster very well Ditribution-based clustering Distribution methods are iterative methods to fit a set of dataset into clusters by optimizing distributions of datasets in clusters (i.e. Gaussian distribution). - First randomly choose Gaussian parameters and fit it to set of data points. - Iteratively optimize the distribution parameters to fit as many points it can. - Once it converges to a local minima, you can assign data points closer to that distribution of that cluster =&gt; Attention cette méthode souffre d’overfitting library(EMCluster, quietly =TRUE) ## Warning: package &#39;EMCluster&#39; was built under R version 3.3.3 #model ret &lt;-init.EM(Data_House_Worth[,2:3], nclass =3) ret ## Method: em.EMRnd.EM ## n = 316, p = 2, nclass = 3, flag = 0, logL = -1871.0408. ## nc: ## [1] 170 100 46 ## pi: ## [1] 0.5576 0.2439 0.1985 # assign class ret.new &lt;-assign.class(Data_House_Worth[,2:3], ret, return.all =FALSE) plotem(ret,Data_House_Worth[,2:3]) ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) + geom_point(alpha =0.4, size =3.5) +geom_point(col = ret.new$class) + scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;)) # good fort high and low. Density based clustering DBSCAN see more on Machine Learning Using R p 349 exemple avec Fuzzy C-Means Clustering This is the fuzzy version of the known k-means clustering algorithm as well as an online variant (Unsupervised Fuzzy Competitive learning). Observe that we are passing the value ucfl to the parameter method, which does an online update of model using Unsupervised Fuzzy Competitive Learning (UCFL). On suppose que les donn?es ce mettent a jours et a chaque nouvelle observation le modle s’update library(e1071) ## Warning: package &#39;e1071&#39; was built under R version 3.3.3 Data_House_Worth &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/House Worth Data.csv&quot;,header=TRUE) str(Data_House_Worth) ## &#39;data.frame&#39;: 316 obs. of 5 variables: ## $ HousePrice : int 138800 155000 152000 160000 226000 275000 215000 392000 325000 151000 ... ## $ StoreArea : num 29.9 44 46.2 46.2 48.7 56.4 47.1 56.7 84 49.2 ... ## $ BasementArea : int 75 504 493 510 445 1148 380 945 1572 506 ... ## $ LawnArea : num 11.22 9.69 10.19 6.82 10.92 ... ## $ HouseNetWorth: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Medium&quot;: 2 3 3 3 3 1 3 1 1 3 ... Data_House_Worth$BasementArea &lt;-NULL online_cmean &lt;-cmeans(Data_House_Worth[,2:3],3,20,verbose=TRUE, method=&quot;ufcl&quot;,m=2) ## Iteration: 1, Error: 115.4861275493 ## Iteration: 2, Error: 111.6796778222 ## Iteration: 3, Error: 108.2383922809 ## Iteration: 4, Error: 105.1380167148 ## Iteration: 5, Error: 102.3533642433 ## Iteration: 6, Error: 99.8596417854 ## Iteration: 7, Error: 97.6332760709 ## Iteration: 8, Error: 95.6523863813 ## Iteration: 9, Error: 93.8970191712 ## Iteration: 10, Error: 92.3492290987 ## Iteration: 11, Error: 90.9930658265 ## Iteration: 12, Error: 89.8145069076 ## Iteration: 13, Error: 88.8013634065 ## Iteration: 14, Error: 87.9431754552 ## Iteration: 15, Error: 87.2311085921 ## Iteration: 16, Error: 86.6578575515 ## Iteration: 17, Error: 86.2175614904 ## Iteration: 18, Error: 85.9057329426 ## Iteration: 19, Error: 85.7192017623 ## Iteration: 20, Error: 85.6560747088 # print(online_cmean) ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) + geom_point(alpha =0.4, size =3.5) +geom_point(col = online_cmean$cluster) + scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;)) 5.3 Evaluation of clustering Internal evaluation Dunn Index : the ratio between the minimal intercluster distances to the maximal intracluster distance. But high score Silhouette Coefficient : the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered external evaluation on test set Rand index : similar to classification rate in multi-class classification problems. measures how many items that are returned by the cluster and expert (labeled) are common and how many differ. \\(RI = \\frac{TP+TN}{TP/FP/FN/TN}\\) (true positive, true negative,…) Jaccard index : measures the overlap of external labels and labels generated by the cluster algorithms. The Jaccard index value varies between 0 and 1, 0 implying no overlap while 1 means identical datasets. $J= = 5.4 Association Rule Mining Algorithms Association rule learning is a method for discovering interesting relations between variables in large databases using some measures of interestingness. Pratique courante sur les transactional (supermarket, library,…). Pour mettre produit ensemble pourune promo, planning, customer segmentation, … Usefull measures : Support : is the proportion of transactions in which an item set appears Confidence : indicates the strength of a rule. is the conditional probability $conf(X=&gt;Y) = Lift : is a ratio between the observed support to the expected support. If = 1 then independent. $ Lift(X=&gt;Y) = =&gt; more information in Beginning Data Science With R p 192 or Machine learning with R chap 6.10 5.5 Singular Value decomposition 5.6 K-Nearest Neighbot 5.7 Others unsuppervised algorithms Learning Vector Quantization Self-Organizing MAP (SQM) Partitioning around Medoids PAM "],
["decision-tree.html", "Chapter 6 Decision Tree 6.1 Type of décision tree 6.2 Decision measures : measure of node purity (heterogeneity of the node) 6.3 Decision tree learning methods 6.4 Random Forests", " Chapter 6 Decision Tree Decision tree are class of non parametric model with generaly a catégorical dependant variable. Globalement c’est un abre de decision qui se split a chaque neaux selon une variable selectionné suivant différentes metrics. Decision tree consists of two types of nodes : leaf node : indicate class defined by the response variable decision node : which specifies some test on a single attributes DT use recursive divide and conquer approach. 6.1 Type of décision tree Regression tree : variables réponse continue. Objectif est de split a chaque itération en minimisant les residual sum squares RSS. Recursively split the feature vector space (X1, X2, ., Xp) into distinct and non-overlapping regions For new observations falling into the same region, the prediction is equal to the mean of all the training observations in that region. Classification tree : variables categorielle We use classification error rate for making the splits in classification trees. Instead of taking the mean of response variable in a particular region for prediction, here we use the most commonly occurring class of training observation as a prediction methodology. 6.2 Decision measures : measure of node purity (heterogeneity of the node) Gini Index : $ G = p_{ml}*(1-P_{mp}) $ where, pmk is the proportion of training observations in the mth region that are from the kth class Entropy function : $ E = - curve(-x *log2(x) -(1 -x) *log2(1 -x), xlab =&quot;x&quot;, ylab =&quot;Entropy&quot;, lwd =5) Observe that both measures are very similar, however, there are some differences: - Gini-index is more suitable to continuous attributes and entropy in case of discrete data. - Gini-index works well for minimizing misclassifications. - Entropy is slightly slower than Gini-index, as it involves logarithms (although this doesn’t really matter much given today’s fast computing machines) Information gain : Measure du changement de l’entrepy entre avant et apres le split 6.3 Decision tree learning methods Iterative Dichotomizer 3 : most popular décision tree algorithms Calculate entropy of each attribute using training observations Split the observations into subsets using the attribute with minimum entropy or maximum information gain. The selected attribute becomes the decision node. Repeat the process with the remaining attribute on the subset. pas super performant pour le multiclass classification C5.0 algorithm : il split les noeuds en 3 possibilités All observations are a single classe =&gt; identify class No class =&gt; use the most frequent class at the parent of this node mixtureof classes =&gt; a test based on single attribute (use information gain) Repete jusqu’au moment outout les observations sont correctement classifié. On utilise pruning pour réduire l’overfitting. Mais avec C50 on utilise pas pruning car algorithm iterate back and replace leaf that dosn’t increase the information gain. Classification and regression tree - CART : Use residual sum square as the node impurity measure. SI utilisation pour pure classification GINI indix peut etre plus approprié comme mesure d’impurité Start the algorithm at the root node. For each attribute X, find the subset S that minimizes the residual sum of square (RSS) of the two children and chooses the split that gives the maximum information gain. Check if relative decrease in impurity is below a prescribed threshold. If Yes, splitting stops, otherwise repeat Step 2. on peut aussi utiliser un parametre de complexité (cp) : any split that does not decrease the overall lack of fit by a factor of cp would not be attempted by the model Chi-square automated interaction detection - CHAID Ici uniquement pour variable catégoriel. variables continues sont catégorisé par optimal bining. L’algorithm fusion les catégories sinon significative avec la variables dépendante. De même si une catégorie a trop peu d’observation, elle est fusionnée avec la catégorie la plus similaire mesurée par la pval tu test chi2. CHAID détecte l’interaction entre variables dans un jeu de données. En utilisant cette technique on peut établir des relations de dépendance entre variable; L’algorithme CHAID2 se déroule en trois étapes : préparation des prédicteurs : transformation en variable catégoriel par optimal bining fusion des classes : pour chaque prédicteur, on determine les catégorie les plus semblable par rapport a la variables dependante. (chi2) Repetition de l’étape jusqu’àavoir une catégorie fusionnée significative non indépendante. Ajuste les pval par bonferonni si des classe ont été fusionnée sélection de la variable de séparation : choisi la variable avec la plus faible pval (au test indépendante chi2 ajusté avec bonferonni), la plus significative. Processus iteratif. Si pval dépasse un seuil, le processus prend fin stopping : Si node est pure:no split pval &gt; seuil : nosplit library(C50) library(splitstackshape) library(rattle) library(rpart.plot) library(data.table) library(gmodels) ### Data prep ### Data_Purchase &lt;-fread(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/PurchasePredictionDataset.csv&quot;,header=T,verbose =FALSE, showProgress =FALSE) table(Data_Purchase$ProductChoice) ## ## 1 2 3 4 ## 106603 199286 143893 50218 #Pulling out only the relevant data to this chapter Data_Purchase &lt;-Data_Purchase[,c(&quot;CUSTOMER_ID&quot;,&quot;ProductChoice&quot;,&quot;MembershipPoints&quot;,&quot;IncomeClass&quot;,&quot;CustomerPropensity&quot;,&quot;LastPurchaseDuration&quot;)] #Delete NA from subset Data_Purchase &lt;-na.omit(Data_Purchase) Data_Purchase$CUSTOMER_ID &lt;-as.character(Data_Purchase$CUSTOMER_ID) #Stratified Sampling Data_Purchase_Model&lt;-stratified(Data_Purchase, group=c(&quot;ProductChoice&quot;),size =10000,replace=FALSE) table(Data_Purchase_Model$ProductChoice) ## ## 1 2 3 4 ## 10000 10000 10000 10000 Data_Purchase_Model$ProductChoice &lt;-as.factor(Data_Purchase_Model$ProductChoice) Data_Purchase_Model$IncomeClass &lt;-as.factor(Data_Purchase_Model$IncomeClass) Data_Purchase_Model$CustomerPropensity &lt;-as.factor(Data_Purchase_Model$CustomerPropensity) #Build the decision tree on Train Data (Set_1) and then test data (Set_2) will be used for performance testing set.seed(917) train &lt;- Data_Purchase_Model[sample(nrow(Data_Purchase_Model),size=nrow(Data_Purchase_Model)*(0.7), replace =TRUE, prob =NULL),] train &lt;-as.data.frame(train) test &lt;-Data_Purchase_Model[!(Data_Purchase_Model$CUSTOMER_ID %in%train$CUSTOMER_ID),] # save(train, file=&quot;./save/train.RData&quot;) # save(test, file=&quot;./save/test.RData&quot;) library(RWeka) # WPM(&quot;refresh-cache&quot;) # WPM(&quot;install-package&quot;, &quot;simpleEducationalLearningSchemes&quot;) ### ID3 model ### # ID3 &lt;-make_Weka_classifier(&quot;weka/classifiers/trees/Id3&quot;) # ID3Model &lt;-ID3(ProductChoice ~CustomerPropensity +IncomeClass ,data = train) # # v = summary(ID3Model) # # saveRDS(v, &quot;ID3Model.rds&quot;) ID3model &lt;- readRDS(&quot;./save/ID3Model.rds&quot;) ID3model ## ## === Summary === ## ## Correctly Classified Instances 9268 33.1 % ## Incorrectly Classified Instances 18732 66.9 % ## Kappa statistic 0.1078 ## Mean absolute error 0.3646 ## Root mean squared error 0.427 ## Relative absolute error 97.2403 % ## Root relative squared error 98.6105 % ## Total Number of Instances 28000 ## ## === Confusion Matrix === ## ## a b c d &lt;-- classified as ## 4792 315 1439 509 | a = 1 ## 3812 494 1812 898 | b = 2 ## 2701 421 2485 1298 | c = 3 ## 2918 416 2193 1497 | d = 4 # library(gmodels) # purchase_pred_test &lt;-predict(ID3model, test) # CrossTable(test$ProductChoice, purchase_pred_test, prop.chisq =FALSE, # prop.c =FALSE, prop.r =FALSE, # dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) # train set accurancy : 33.3036% # test set accurancy : 0.159+0.004+0.086+ 0.073 = 33.2% # test and train are proche : sign of no overfitting ### C50 model ### model_c50 &lt;-C5.0(train[,c(&quot;CustomerPropensity&quot;,&quot;LastPurchaseDuration&quot;, &quot;MembershipPoints&quot;)], train[,&quot;ProductChoice&quot;], control =C5.0Control(CF =0.001, minCases =2)) summary(model_c50) ## ## Call: ## C5.0.default(x = train[, c(&quot;CustomerPropensity&quot;, ## &quot;LastPurchaseDuration&quot;, &quot;MembershipPoints&quot;)], y = ## train[, &quot;ProductChoice&quot;], control = C5.0Control(CF = 0.001, minCases = 2)) ## ## ## C5.0 [Release 2.07 GPL Edition] Thu Oct 25 21:20:52 2018 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 28000 cases (4 attributes) from undefined.data ## ## Decision tree: ## ## CustomerPropensity in {High,VeryHigh}: ## :...MembershipPoints &lt;= 1: 4 (1310/728) ## : MembershipPoints &gt; 1: 3 (7111/4643) ## CustomerPropensity in {Low,Medium,Unknown}: ## :...MembershipPoints &lt;= 1: 4 (3364/1898) ## MembershipPoints &gt; 1: ## :...LastPurchaseDuration &lt;= 5: 1 (10034/6327) ## LastPurchaseDuration &gt; 5: ## :...CustomerPropensity in {Low,Medium}: 3 (3509/2419) ## CustomerPropensity = Unknown: 2 (2672/1865) ## ## ## Evaluation on training data (28000 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 6 17880(63.9%) &lt;&lt; ## ## ## (a) (b) (c) (d) &lt;-classified as ## ---- ---- ---- ---- ## 3707 798 1636 914 (a): class 1 ## 2865 807 2368 976 (b): class 2 ## 2040 571 3558 736 (c): class 3 ## 1422 496 3058 2048 (d): class 4 ## ## ## Attribute usage: ## ## 100.00% CustomerPropensity ## 100.00% MembershipPoints ## 57.91% LastPurchaseDuration ## ## ## Time: 0.1 secs plot(model_c50) purchase_pred_train &lt;-predict(model_c50, train,type =&quot;class&quot;) vtrain = CrossTable(train$ProductChoice, purchase_pred_train, prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 28000 ## ## ## | predicted default ## actual default | 1 | 2 | 3 | 4 | Row Total | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 3707 | 798 | 1636 | 914 | 7055 | ## | 0.132 | 0.028 | 0.058 | 0.033 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 2865 | 807 | 2368 | 976 | 7016 | ## | 0.102 | 0.029 | 0.085 | 0.035 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 2040 | 571 | 3558 | 736 | 6905 | ## | 0.073 | 0.020 | 0.127 | 0.026 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 1422 | 496 | 3058 | 2048 | 7024 | ## | 0.051 | 0.018 | 0.109 | 0.073 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 10034 | 2672 | 10620 | 4674 | 28000 | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## ## purchase_pred_test &lt;-predict(model_c50, test) vtest = CrossTable(test$ProductChoice, purchase_pred_test, prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 20002 ## ## ## | predicted default ## actual default | 1 | 2 | 3 | 4 | Row Total | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 2661 | 539 | 1157 | 641 | 4998 | ## | 0.133 | 0.027 | 0.058 | 0.032 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 2032 | 566 | 1714 | 683 | 4995 | ## | 0.102 | 0.028 | 0.086 | 0.034 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 1446 | 398 | 2638 | 553 | 5035 | ## | 0.072 | 0.020 | 0.132 | 0.028 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 981 | 347 | 2223 | 1423 | 4974 | ## | 0.049 | 0.017 | 0.111 | 0.071 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 7120 | 1850 | 7732 | 3300 | 20002 | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## ## sum(diag(vtrain$prop.tbl)) ## [1] 0.3614286 sum(diag(vtest$prop.tbl)) ## [1] 0.3643636 ### CART MODEL ### CARTModel &lt;-rpart(ProductChoice ~IncomeClass +CustomerPropensity +LastPurchaseDuration +MembershipPoints, data=train) summary(CARTModel) ## Call: ## rpart(formula = ProductChoice ~ IncomeClass + CustomerPropensity + ## LastPurchaseDuration + MembershipPoints, data = train) ## n= 28000 ## ## CP nsplit rel error xerror xstd ## 1 0.08083075 0 1.0000000 1.0034376 0.003456583 ## 2 0.03342086 1 0.9191693 0.9250895 0.003688307 ## 3 0.01284316 2 0.8857484 0.8916687 0.003765162 ## 4 0.01000000 3 0.8729052 0.8758176 0.003797399 ## ## Variable importance ## CustomerPropensity MembershipPoints ## 55 45 ## ## Node number 1: 28000 observations, complexity param=0.08083075 ## predicted class=1 expected loss=0.7480357 P(node) =1 ## class counts: 7055 7016 6905 7024 ## probabilities: 0.252 0.251 0.247 0.251 ## left son=2 (19579 obs) right son=3 (8421 obs) ## Primary splits: ## CustomerPropensity splits as RLLLR, improve=356.96340, (0 missing) ## MembershipPoints &lt; 1.5 to the right, improve=269.07020, (0 missing) ## LastPurchaseDuration &lt; 5.5 to the left, improve=201.38810, (0 missing) ## IncomeClass splits as LLLLLLLRRR, improve= 40.14243, (0 missing) ## ## Node number 2: 19579 observations, complexity param=0.03342086 ## predicted class=1 expected loss=0.6952347 P(node) =0.69925 ## class counts: 5967 5233 4136 4243 ## probabilities: 0.305 0.267 0.211 0.217 ## left son=4 (16215 obs) right son=5 (3364 obs) ## Primary splits: ## MembershipPoints &lt; 1.5 to the right, improve=261.00700, (0 missing) ## LastPurchaseDuration &lt; 5.5 to the left, improve=104.82520, (0 missing) ## CustomerPropensity splits as -RRL-, improve= 80.00260, (0 missing) ## IncomeClass splits as LLRLLLLRRR, improve= 19.81066, (0 missing) ## ## Node number 3: 8421 observations, complexity param=0.01284316 ## predicted class=4 expected loss=0.6697542 P(node) =0.30075 ## class counts: 1088 1783 2769 2781 ## probabilities: 0.129 0.212 0.329 0.330 ## left son=6 (7111 obs) right son=7 (1310 obs) ## Primary splits: ## MembershipPoints &lt; 1.5 to the right, improve=35.80197, (0 missing) ## CustomerPropensity splits as L---R, improve=21.69469, (0 missing) ## LastPurchaseDuration &lt; 5.5 to the left, improve=18.47056, (0 missing) ## IncomeClass splits as LLLLLLLRRL, improve=13.01753, (0 missing) ## Surrogate splits: ## IncomeClass splits as RLLLLLLLLL, agree=0.845, adj=0.002, (0 split) ## ## Node number 4: 16215 observations ## predicted class=1 expected loss=0.6792476 P(node) =0.5791071 ## class counts: 5201 4536 3701 2777 ## probabilities: 0.321 0.280 0.228 0.171 ## ## Node number 5: 3364 observations ## predicted class=4 expected loss=0.5642093 P(node) =0.1201429 ## class counts: 766 697 435 1466 ## probabilities: 0.228 0.207 0.129 0.436 ## ## Node number 6: 7111 observations ## predicted class=3 expected loss=0.6529321 P(node) =0.2539643 ## class counts: 940 1504 2468 2199 ## probabilities: 0.132 0.212 0.347 0.309 ## ## Node number 7: 1310 observations ## predicted class=4 expected loss=0.5557252 P(node) =0.04678571 ## class counts: 148 279 301 582 ## probabilities: 0.113 0.213 0.230 0.444 fancyRpartPlot(CARTModel) purchase_pred_train &lt;-predict(CARTModel, train,type =&quot;class&quot;) # vtrain = CrossTable(train$ProductChoice, purchase_pred_train, prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) # Training set Accuracy = 27% # not the bast for classification ### MODEL CHAID ### #install.packages(&quot;CHAID&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) library(CHAID) ctrl &lt;- chaid_control(minsplit =200, minprob =0.1) CHAIDModel &lt;-chaid(ProductChoice ~CustomerPropensity +IncomeClass, data = train, control = ctrl) purchase_pred_train &lt;-predict(CHAIDModel, train) vtrain = CrossTable(train$ProductChoice, purchase_pred_train, prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 28000 ## ## ## | predicted default ## actual default | 1 | 2 | 3 | 4 | Row Total | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 4351 | 298 | 1405 | 1001 | 7055 | ## | 0.155 | 0.011 | 0.050 | 0.036 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 3448 | 410 | 1889 | 1269 | 7016 | ## | 0.123 | 0.015 | 0.067 | 0.045 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 2425 | 241 | 2902 | 1337 | 6905 | ## | 0.087 | 0.009 | 0.104 | 0.048 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 2560 | 244 | 2647 | 1573 | 7024 | ## | 0.091 | 0.009 | 0.095 | 0.056 | | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 12784 | 1193 | 8843 | 5180 | 28000 | ## ---------------|-----------|-----------|-----------|-----------|-----------| ## ## sum(diag(vtrain$prop.tbl)) ## [1] 0.3298571 plot(CHAIDModel) 6.4 Random Forests Fait partie des ensemble trees (boosting, bagging, .. etc). Random forests généralise les decision trees en contruistant plusieurs DT et les combinant. Soit N nbr d’observation, n nombre de DT et M le nombre de variables du dataset Choose a subset of m variables from M (m&lt;&lt;M) and buld n DT using ramdon set of m variable Grow each tree as large os possible Use majority voting to decide the class of the observation ### Data prep ### library(caret) ## Warning: package &#39;caret&#39; was built under R version 3.3.3 ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift library(gmodels) load(&quot;./save/train.RData&quot;) load(&quot;./save/test.RData&quot;) set.seed(100) ; dim(train) ; train = train[1:2000,] ## [1] 28000 6 control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=5, repeats=2) # rfModel &lt;-train(ProductChoice ~CustomerPropensity +LastPurchaseDuration +MembershipPoints, # data=train, # method=&quot;rf&quot;, # trControl=control) # saveRDS(rfModel, &quot;rfModel.rds&quot;) rfModel &lt;- readRDS(&quot;./save/rfModel.rds&quot;) purchase_pred_train &lt;-predict(rfModel, train) # vtrain = CrossTable(train$ProductChoice, purchase_pred_train, prop.chisq =FALSE, prop.c =FALSE, # prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) purchase_pred_train &lt;-predict(rfModel, test) # vtest = CrossTable(test$ProductChoice, purchase_pred_train, prop.chisq =FALSE, prop.c =FALSE, # prop.r =FALSE,dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) sum(diag(vtrain$prop.tbl)) ## [1] 0.3298571 sum(diag(vtest$prop.tbl)) ## [1] 0.3643636 # de tout les DT meilleur accurancy sur le test et le train mais probleme d&#39;overfitting ### RF on continuous variable ### library(Metrics) ## Warning: package &#39;Metrics&#39; was built under R version 3.3.3 library(randomForest) ## Warning: package &#39;randomForest&#39; was built under R version 3.3.3 ## randomForest 4.6-12 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:rattle&#39;: ## ## importance ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin RF &lt;- randomForest(dist ~ speed, data = cars) rmse(cars$dist,predict(RF, cars)) ## [1] 11.84672 "],
["neural-network.html", "Chapter 7 Neural Network 7.1 Neural Networks Basis 7.2 Neural Network Architecture 7.3 Deep Learning", " Chapter 7 Neural Network ## Instal tensorflow # devtools::install_github(&quot;rstudio/reticulate&quot;) # devtools::install_github(&quot;rstudio/tfestimators&quot;) # library(tfestimators) # install_tensorflow() # # SEE more information / exemple : https://tensorflow.rstudio.com/blog/keras-fraud-autoencoder.html Neural Network algorithms work on complex neural structures that can abstract higher level of information from a huge dataset. They are computationally heavy and hard to train. We provide a deep architecture network and image recognition (convolutional nets) : Deep Boltzmann Machine DBM Deep Belief Network DBN Convolutional Neural Network CNN Stacked auto Encoders RELU : fonction de lien activation fonction so much better than sigmoid (help with the vanishing gradient problem ???) What is deep neural netwwork : Deep Neural network consists of more hidden layers Each Input will be connected to the hidden layer and the NN will decide the connections. Pratique car : We no longer need to make any assumptions about our data; any type of data works in neural networks (categorical and numerical). They are scalable techniques, can take in billions of data points,and can capture a very high level of abstraction. Utilise le Learning and updating. Essai erreur. pour amélioréer les resultats 7.1 Neural Networks Basis Artificial neural networks have three main components to set up : Architecture: Number of layers, weights matrix, bias, connections,… Rules: Refer to the mechanism of how the neurons behave in response to signals from each other. Learning rule: The way in which the neural network’s weights change with time. Perceptron : Basic unit of ANN Takes multiple input and produce binary output. binary classification algorithm basé sur des prédicteurs linéaire et une fonction de poids \\(f(x) = 1 Si wx+b &gt;0\\) simple neural network Single perceptron algorithm Initialize de weights to some feasible values For each data point in a training set, do step 3 and 4. Calcul the output with previous step weights $$ y_j(t) = f[w(t)x_j] = f[w_0(t)x_{j,0} + w_1(t)x_{j,1} + w_2(t)x_{j,2}+ … + w_n(t)x_j,n] Update the weight : \\(w_i(t+1) = w_i(t)+(d_j-y_j(t))x_{j,i}\\) for all feature \\(0&lt;i&lt;n\\) Stop when reach stopping criteria All points in training set are exhausted a preset number of iteration iteration error (\\(= \\frac{1}{s} \\sum |d_j - y_j(t)|\\)) is less than threshold error. Sigmoid Neuron Sigmoid Neuron ($ S(t) = ) allow a continuous output. similar to logistic curve. Tout comme le perceptron, le sigmoid neuron has weight for each input et un biais global. simple neural network 7.2 Neural Network Architecture artificial neural network Artificial neural networt expand the simple peceptron to a multi layer perceptron (MLP). THis is a neural network architeture that can deal with non linear separation as output. Hidden layers : predicts connection between inputs automatically. Doesn’t have any direct input. Finding the hidden layer design and number is notstraightforward. Il existe plusieurs disign pour les hidden layer, par exemple : Feedforward Neural Networks (FFNN): each input layer is in one direction. This network makes sure that there are no loops within the neural network. (le plus general) Specialization versus Generalization: If you have too many hidden layers/complicated architecture, the neural network tend to be very specialized (so overfits). If you use simple architecture that the model will be very generalized and would not fit the data properly. Feed-Forward back propagation One of the most popular learning methodologies in neural networks. It can by use to train artificial neural network. Method works on the gradient descent principle so the neuron function should be defferential. artificial neural network We will give amathematical representation of error correction when the sigmoid function is used as the activation function. This algorithm will be correcting for error in each iteration and coverage to a point where it has no more reducible error : - 1. Feed-forward the network with input and get the output. - 2. Backward propagation of output, to calculate delta at each neuron (error). - 3. Multiply the delta and input activation function to get the gradient of weight. - 4. Update the weight by subtracting a ratio from the gradient of the weight. To update the weight bij using gradient descent, you must choose a learning rate Example : purchase prediction : NN classification #data preparation Data_Purchase_Prediction &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/PurchasePredictionDataset.csv&quot;,header=TRUE) Data_Purchase_Prediction$choice &lt;-ifelse(Data_Purchase_Prediction$ProductChoice ==1,1, ifelse(Data_Purchase_Prediction$ProductChoice ==3,0,999)) Data_Neural_Net &lt;-Data_Purchase_Prediction[Data_Purchase_Prediction$choice%in%c(&quot;0&quot;,&quot;1&quot;),] #Remove Missing Values Data_Neural_Net &lt;-na.omit(Data_Neural_Net) rownames(Data_Neural_Net) &lt;-NULL # Usually scaling the continuous variables in the intervals [0,1] or [-1,1] tends to give better results. Convert the categorical variables into binary variables. #Transforming the continuous variables cont &lt;-Data_Neural_Net[,c(&quot;PurchaseTenure&quot;,&quot;CustomerAge&quot;,&quot;MembershipPoints&quot;,&quot;IncomeClass&quot;)] maxs &lt;-apply(cont, 2, max) mins &lt;-apply(cont, 2, min) scaled_cont &lt;-as.data.frame(scale(cont, center = mins, scale = maxs -mins)) #The dependent variable dep &lt;-factor(Data_Neural_Net$choice) # Multifactor data to binaries variables Data_Neural_Net$ModeOfPayment &lt;-factor(Data_Neural_Net$ModeOfPayment) flags_ModeOfPayment =data.frame(Reduce(cbind,lapply(levels(Data_Neural_Net$ModeOfPayment), function(x){(Data_Neural_Net$ModeOfPayment ==x)*1}))) names(flags_ModeOfPayment) =levels(Data_Neural_Net$ModeOfPayment) Data_Neural_Net$CustomerPropensity &lt;-factor(Data_Neural_Net$CustomerPropensity) flags_CustomerPropensity =data.frame(Reduce(cbind,lapply(levels(Data_Neural_Net$CustomerPropensity), function(x){(Data_Neural_Net$CustomerPropensity ==x)*1}))) names(flags_CustomerPropensity) =levels(Data_Neural_Net$CustomerPropensity) cate &lt;-cbind(flags_ModeOfPayment,flags_CustomerPropensity) #Combine all data into single modeling data Dataset &lt;-cbind(dep,scaled_cont,cate); #Divide the data into train and test set.seed(917); index &lt;-sample(1:nrow(Dataset),round(0.7*nrow(Dataset))) train &lt;-Dataset[index,] test &lt;-Dataset[-index,] ##MODELING library(nnet) i &lt;-names(train) form &lt;-as.formula(paste(&quot;dep ~&quot;, paste(i[!i %in% &quot;dep&quot;], collapse =&quot; + &quot;))) # nn &lt;-nnet.formula(form,size=10,data=train) # save(nn, file=&quot;./save/nn.R&quot;) load(file=&quot;./save/nn.R&quot;) # Use 10 neuron in one hidden layer predict_class &lt;-predict(nn, newdata=test, type=&quot;class&quot;) table(test$dep,predict_class) ## predict_class ## 0 1 ## 0 28776 13863 ## 1 11964 19534 sum(diag(table(test$dep,predict_class))/nrow(test)) ## [1] 0.6516314 # misc rate : 65.1% qui est 1% de plus que logistic reg # neural net ameliore prediction sur 0 mais détérior sur 1 library(NeuralNetTools) plotnet(nn) # Plot the importance olden(nn) garson(nn) Other information Main step for a neural network model : define model structure initialize model parameters loop calculate current loss (forward propagation) calculate current gradient (backward propagation) -update parameter (gradient descent) Type of neural network function : Perceptron Back propagation Hopfield Network Radia Basis Function Network (RBFN) RELU 7.3 Deep Learning Deep learning consists of advanced algorithms having multiple layers, composed of multiple linear and non-linear transformations There are multiple deep learning architectures : automatic speech recognition, NLP, audio recognition Deep neural network Convolution deep neural network (image recognition :2 dimentional data) deep belief (image et signal processing) recurrent neural network (time series data) recursive Neural network (langugage processing) In general, adding more layers and neurons per layer increases the specialization of neural network to train data and decreases the performance on test data: Overfitting and computational cost. However R is not yet developed enough tools to run various deep learning algorithms. Another reason for that is deep learning is so resource intensive that models can be trained only on large clusters and not on workstations. 7.3.1 Example of deep learning : Classification # continuous variable are scaled and categorical varibale converted into binary variables #install.packages(&quot;C:/Users/007/Desktop/Data science with R/R/darch_0.12.0.tar.gz&quot; , repos = NULL, type=&quot;source&quot;) library(darch) library(mlbench) library(RANN) #Print the model formula form ## dep ~ PurchaseTenure + CustomerAge + MembershipPoints + IncomeClass + ## BankTransfer + Cash + CashPoints + CreditCard + DebitCard + ## MoneyWallet + Voucher + High + Low + Medium + Unknown + VeryHigh ########### NOT Run TO long !!###### #Apply the model using deep neural net with # deep_net &lt;- darch(form, train, # preProc.params = list(&quot;method&quot; = c(&quot;knnImpute&quot;)), # layers = c(0,10,30,10,0), # darch.batchSize = 1, # darch.returnBestModel.validationErrorFactor = 1, # darch.fineTuneFunction = &quot;rpropagation&quot;, # darch.unitFunction = c(&quot;tanhUnit&quot;, &quot;tanhUnit&quot;,&quot;tanhUnit&quot;,&quot;softmaxUnit&quot;), # darch.numEpochs = 15, # bootstrap = T, # bootstrap.num = 500) # deep_net &lt;-darch(form,train, # preProc.params =list(method =c(&quot;center&quot;, &quot;scale&quot;)), # layers =c(0,10,30,10,0), # darch.unitFunction =c(&quot;sigmoidUnit&quot;, &quot;tanhUnit&quot;,&quot;tanhUnit&quot;,&quot;softmaxUnit&quot;), # darch.fineTuneFunction =&quot;minimizeClassifier&quot;, # darch.numEpochs =15, # cg.length =3, cg.switchLayers =5) # library(NeuralNetTools) #plot(deep_net,&quot;net&quot;) #result &lt;-darchTest(deep_net, newdata = test) #result 7.3.2 Example : Imagine prediction : NN classification # install.packages(&quot;drat&quot;, repos=&quot;https://cran.rstudio.com&quot;) # drat:::addRepo(&quot;dmlc&quot;) # cran &lt;- getOption(&quot;repos&quot;) # cran[&quot;dmlc&quot;] &lt;- &quot;https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/&quot; # options(repos = cran) # install.packages(&quot;mxnet&quot;) # install.packages(&quot;https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip&quot;, repos = NULL) #Please refer https://github.com/dahtah/imager ##install.packages(&quot;devtools&quot;) #devtools::install_github(&quot;dahtah/imager&quot;) library(mxnet) #install imager for loading images library(imager) library(drat) #load the pre-trained model download.file(&#39;http://data.dmlc.ml/data/Inception.zip&#39;, destfile = &#39;Inception.zip&#39;) unzip(&quot;Inception.zip&quot;) model &lt;- mx.model.load(&quot;Inception/Inception_BN&quot;, iteration=39) #We also need to load in the mean image, which is used for preprocessing using mx.nd.load. mean.img = as.array(mx.nd.load(&quot;Inception/mean_224.nd&quot;)[[&quot;mean_img&quot;]]) #Load and plot the image: (Default parrot image) #im &lt;- load.image(system.file(&quot;extdata/parrots.png&quot;, package=&quot;imager&quot;)) #im &lt;-load.image(&quot;Images/russia-volcano.jpg&quot;) im = load.image(&quot;C:/Users/007/Desktop/Data science with R/R/Inception/fibrosarcome-chat.jpg&quot;) plot(im) preproc.image &lt;-function(im, mean.image) { # crop the image shape &lt;-dim(im) short.edge &lt;-min(shape[1:2]) xx &lt;-floor((shape[1] -short.edge) /2) yy &lt;-floor((shape[2] -short.edge) /2) cropped &lt;-crop.borders(im, xx, yy) # resize to 224 x 224, needed by input of the model. resized &lt;-resize(cropped, 224, 224) # convert to array (x, y, channel) arr &lt;-as.array(resized) *255 dim(arr) &lt;-c(224, 224, 3) # subtract the mean normed &lt;-arr -mean.img # Reshape to format needed by mxnet (width, height, channel, num) dim(normed) &lt;-c(224, 224, 3, 1) return(normed) } #Now pass our image to pre-process normed &lt;-preproc.image(im, mean.img) plot(normed) prob &lt;- predict(model, X=normed) #We can extract the top-5 class index. max.idx &lt;- order(prob[,1], decreasing = TRUE)[1:5] max.idx ## [1] 283 286 282 288 284 synsets &lt;-readLines(&quot;Inception/synset.txt&quot;) #And let us print the corresponding lines: print(paste0(&quot;Predicted Top-classes: &quot;, synsets[as.numeric(max.idx)])) ## [1] &quot;Predicted Top-classes: n02123159 tiger cat&quot; ## [2] &quot;Predicted Top-classes: n02124075 Egyptian cat&quot; ## [3] &quot;Predicted Top-classes: n02123045 tabby, tabby cat&quot; ## [4] &quot;Predicted Top-classes: n02127052 lynx, catamount&quot; ## [5] &quot;Predicted Top-classes: n02123394 Persian cat&quot; "],
["text-mining.html", "Chapter 8 Text Mining 8.1 TF - IDF 8.2 Text summarization : Gong &amp; Liu method (2001) via latent semantic analysis 8.3 Text analysis 8.4 Other topic", " Chapter 8 Text Mining To Read : https://www.tidytextmining.com/index.html Natural language processing (NLP) 8.1 TF - IDF Term frequency counts the number of occurrences of a term t in a document. Inverse document frequency : $ idf = log_2 where |D| denotes the total number of documents and \\(|{d|t\\in d}|\\) is the number of documents where the term t appears Certain terms that occur too frequently have little power in determining the reliance of a document. IDF weigh down the too frequently occurring word (et inverserment). A tf-idf matrix is a numerical representation of a collection of documents (represented by row) and words contained in it (represented by columns). In bag-of-words (BoW) featurization, a text document is converted into a flat vector of counts. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in : bow(w, d) = # times word w appears in document d tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears) If a word appears in many 61documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher. Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filter‐ing methods Text Mining TM : USE bag of word each word one feature + PCA pour reduire dimension bag-of-n-grams : is a sequence of n tokens. After tokenization, the counting mechanism can collate individual tokens into word counts. n-grams retain more of the original sequence structure of the text. For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door. here are usually a lot more distinct n-grams (n &gt; 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model. Filtering for Cleaner Features : TM generate a lot a new feature, How separate the noise from the signal? Stopword : For classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics. Frequent words : Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus Rare words : To a statistical model, a word that appears in only one or two documents is more like noise than useful information. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain Stemming : Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form (“swimmer,” “swimming,” and “swim,”) =&lt; lemmmization The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix. Parsing and Tokenization Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure -tokenization : This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word Chunking and part-of-speech tagging we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” Part of Speech (POS) tagging This could help in classifying named entities in text into categories like persons, company, locations, expression of time, and so on. Word Cloud The word cloud helps in visualizing the words most frequently being used in the reviews library(data.table) library(caTools) fine_food_data &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Food_Reviews.csv&quot;, stringsAsFactors =FALSE) fine_food_data$Score &lt;-as.factor(fine_food_data$Score) head(fine_food_data[,10],2) ## [1] &quot;I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than most.&quot; ## [2] &quot;Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \\&quot;Jumbo\\&quot;.&quot; # Data preparation # Randomly split data and use only 10% of the dataset set.seed(90) split =sample.split(fine_food_data$Score, SplitRatio =0.10) fine_food_data =subset(fine_food_data, split ==TRUE) select_col &lt;-c(&quot;Id&quot;,&quot;HelpfulnessNumerator&quot;,&quot;HelpfulnessDenominator&quot;,&quot;Score&quot;,&quot;Summary&quot;,&quot;Text&quot;) fine_food_data_selected &lt;-fine_food_data[,select_col] dim(fine_food_data_selected) ## [1] 3518 6 # Summary text ##original fine_food_data_selected[2,6] ## [1] &quot;McCann&#39;s Instant Oatmeal is great if you must have your oatmeal but can only scrape together two or three minutes to prepare it. There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation. Still, the McCann&#39;s is as good as it gets for instant oatmeal. It&#39;s even better than the organic, all-natural brands I have tried. All the varieties in the McCann&#39;s variety pack taste good. It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.&lt;br /&gt;&lt;br /&gt;McCann&#39;s use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product. Real sugar tastes better and is not as harmful as the other stuff. One thing I do not like, though, is McCann&#39;s use of thickeners. Oats plus water plus heat should make a creamy, tasty oatmeal without the need for guar gum. But this is a convenience product. Maybe the guar gum is why, after sitting in the bowl a while, the instant McCann&#39;s becomes too thick and gluey.&quot; library(LSAfun) genericSummary(fine_food_data_selected[2,6],k=1) ## [1] &quot; There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation&quot; genericSummary(fine_food_data_selected[2,6],k=2) ## [1] &quot; There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation&quot; ## [2] &quot; It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue&quot; # TF and IDF library(tm) fine_food_data_corpus &lt;-VCorpus(VectorSource(fine_food_data_selected$Text)) ## Standardize the text - Pre-Processing fine_food_data_text_dtm &lt;-DocumentTermMatrix(fine_food_data_corpus, control=list(tolower =TRUE, removeNumbers =TRUE, stopwords =TRUE, removePunctuation =TRUE, stemming =TRUE )) #save frequently-appearing terms( more than 500 times) to a character vector fine_food_data_text_freq &lt;-findFreqTerms(fine_food_data_text_dtm, 500) # create DTMs with only the frequent terms fine_food_data_text_dtm &lt;-fine_food_data_text_dtm[ , fine_food_data_text_freq] tm::inspect(fine_food_data_text_dtm[1:5,1:10]) ## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt; ## Non-/sparse entries: 8/42 ## Sparsity : 84% ## Maximal term length: 6 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs also bag buy can coffe dog eat find flavor food ## 1 1 0 0 0 0 0 0 0 0 0 ## 2 0 0 1 2 0 0 0 0 0 0 ## 3 0 0 0 0 2 0 0 0 0 0 ## 4 0 0 0 0 0 0 1 1 0 0 ## 5 0 0 0 0 0 0 0 1 2 0 #Create a tf-idf matrix fine_food_data_tfidf &lt;-weightTfIdf(fine_food_data_text_dtm, normalize=FALSE) tm::inspect(fine_food_data_tfidf[1:5,1:10]) ## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt; ## Non-/sparse entries: 8/42 ## Sparsity : 84% ## Maximal term length: 6 ## Weighting : term frequency - inverse document frequency (tf-idf) ## Sample : ## Terms ## Docs also bag buy can coffe dog eat find flavor ## 1 3.04583 0 0.000000 0.000000 0.00000 0 0.000000 0.000000 0.000000 ## 2 0.00000 0 2.635882 4.525741 0.00000 0 0.000000 0.000000 0.000000 ## 3 0.00000 0 0.000000 0.000000 5.82035 0 0.000000 0.000000 0.000000 ## 4 0.00000 0 0.000000 0.000000 0.00000 0 2.960361 2.992637 0.000000 ## 5 0.00000 0 0.000000 0.000000 0.00000 0 0.000000 2.992637 4.024711 ## Terms ## Docs food ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 #Part of Speech tagging fine_food_data_corpus&lt;-Corpus(VectorSource(fine_food_data_selected$Text[1:3])) fine_food_data_cleaned &lt;-tm_map(fine_food_data_corpus, PlainTextDocument) ##tolwer fine_food_data_cleaned &lt;-tm_map(fine_food_data_cleaned, tolower) fine_food_data_cleaned &lt;-tm_map(fine_food_data_cleaned, removeWords, stopwords(&quot;english&quot;)) fine_food_data_cleaned &lt;-tm_map(fine_food_data_cleaned,removePunctuation) fine_food_data_cleaned &lt;-tm_map(fine_food_data_cleaned, removeNumbers) fine_food_data_cleaned &lt;-tm_map(fine_food_data_cleaned, stripWhitespace) library(openNLP) library(NLP) fine_food_data_string &lt;-NLP::as.String(fine_food_data_cleaned[[1]]) sent_token_annotator &lt;-Maxent_Sent_Token_Annotator() word_token_annotator &lt;-Maxent_Word_Token_Annotator() fine_food_data_string_an &lt;-annotate(fine_food_data_string, list(sent_token_annotator, word_token_annotator)) pos_tag_annotator &lt;-Maxent_POS_Tag_Annotator() fine_food_data_string_an2 &lt;-annotate(fine_food_data_string, pos_tag_annotator, fine_food_data_string_an) head(annotate(fine_food_data_string, Maxent_POS_Tag_Annotator(probs =TRUE), fine_food_data_string_an2)) ## id type start end features ## 1 sentence 1 524 constituents=&lt;&lt;integer,77&gt;&gt; ## 2 word 1 9 POS=NNS, POS=NNS, POS_prob=0.7822268 ## 3 word 11 20 POS=VBP, POS=VBP, POS_prob=0.3488425 ## 4 word 22 30 POS=NN, POS=NN, POS_prob=0.8055908 ## 5 word 32 39 POS=JJ, POS=JJ, POS_prob=0.6114238 ## 6 word 41 45 POS=NN, POS=NN, POS_prob=0.9833723 fine_food_data_string_an2w &lt;-subset(fine_food_data_string_an2, type == &quot;word&quot;) tags &lt;-sapply(fine_food_data_string_an2w$features, `[[`, &quot;POS&quot;) table(tags) ## tags ## , CC CD IN JJ JJS NN NNS RB VB VBD VBG VBN VBP VBZ ## 1 2 1 1 10 2 28 9 5 1 6 2 4 2 3 plot(table(tags), type =&quot;h&quot;, xlab=&quot;Part-Of_Speech&quot;, ylab =&quot;Frequency&quot;) head(sprintf(&quot;%s/%s&quot;, fine_food_data_string[fine_food_data_string_an2w], tags),15) ## [1] &quot;twizzlers/NNS&quot; &quot;strawberry/VBP&quot; &quot;childhood/NN&quot; ## [4] &quot;favorite/JJ&quot; &quot;candy/NN&quot; &quot;made/VBD&quot; ## [7] &quot;lancaster/NN&quot; &quot;pennsylvania/NN&quot; &quot;y/RB&quot; ## [10] &quot;s/VBZ&quot; &quot;candies/NNS&quot; &quot;inc/CC&quot; ## [13] &quot;one/CD&quot; &quot;oldest/JJS&quot; &quot;confectionery/NN&quot; # wordcloud library(SnowballC) library(wordcloud) library(slam) fine_food_data_corpus &lt;-VCorpus(VectorSource(fine_food_data_selected$Text)) fine_food_data_text_tdm &lt;-TermDocumentMatrix(fine_food_data_corpus, control =list(tolower =TRUE, removeNumbers =TRUE, stopwords =TRUE, removePunctuation =TRUE, stemming =TRUE )) wc_tdm &lt;- rollup(fine_food_data_text_tdm,2,na.rm=TRUE,FUN=sum) matrix_c &lt;-as.matrix(wc_tdm) wc_freq &lt;-sort(rowSums(matrix_c)) wc_tmdata &lt;-data.frame(words=names(wc_freq), wc_freq) wc_tmdata &lt;-na.omit(wc_tmdata) wordcloud(tail(wc_tmdata$words,100), tail(wc_tmdata$wc_freq,100), random.order=FALSE, colors=brewer.pal(8, &quot;Dark2&quot;)) 8.2 Text summarization : Gong &amp; Liu method (2001) via latent semantic analysis Decompose the document D into individual sentences and use these sentences to form the candidate sentence set S and set k = 1. Construct the terms by sentences matrix A for the document D. Perform the SVD on A to obtain the singular value matrix, and the right singular vector matrix V^t. In the singular vector space, each sentence i is represented by the column vector. Select the k’th right singular vector from matrix V^t. Select the sentence that has the largest index value with the k’th right singular vector and include it in the summary. If k reaches the predefined number, terminate the operation otherwise, increment k by 1 and go back to Step 4 8.3 Text analysis we will introduce you to the powerful world of text analytics by using a third-party API ( (Application Programming Interface) called from within R. We will be using Microsoft Cognitive Services API to show some real-time analysis of text from the Twitter feed of a news agency. Microsoft Cognitive Services is a machine intelligence service. This service provide a cloud-based APIs for developers to do lot of high-end functions like face recognition, speech recognition, text mining, video feed analysis, and many others. We will be using their free developer service to show some text analytics features like : - Sentiment analysis: Sentiment analysis will tell us what kind of emotions the tweets are carrying. The Microsoft API returns a value between 0 and 1, where 1 means highly positive sentiment while 0 means highly negative sentiment. - Topic detection: What the topic of discussion is a document? - Language detection: Can you just provide something written and it shows you which language it is? - Summarization: Can we automatically summarize a big document to make it manageable to read Exemple : Use twitter to analyse Attention besoin d’un compte Microsoft cognitive ##&quot; NEED microsoft account, don&#39;t realy work&quot; # # library(&quot;twitteR&quot;) # # See Machine learning with R p 424 to use twitter for text analytics # # #install.packages(&quot;mscstexta4r&quot;) # library(mscstexta4r) # # Sys.setenv(MSCS_TEXTANALYTICS_URL =&quot;https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment&quot;) # Sys.setenv(MSCS_TEXTANALYTICS_KEY =&quot;2673988d37f941f89440d665ae6dad9b&quot;) # # #Initialize the service # textaInit() # # # Load Packages # require(tm) # require(NLP) # require(openNLP) # #Read the Forbes article into R environment # y &lt;-paste(scan(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/india_after_independence.txt&quot;, what=&quot;character&quot;,sep=&quot; &quot;),collapse=&quot; &quot;) # # convert_text_to_sentences &lt;-function(text, lang =&quot;en&quot;) { # # Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language &#39;en&#39;. # sentence_token_annotator &lt;-Maxent_Sent_Token_Annotator(language = lang) # # Convert text to class String from package NLP # text &lt;-as.String(text) # # Sentence boundaries in text # sentence.boundaries &lt;-annotate(text, sentence_token_annotator) # # Extract sentences # sentences &lt;-text[sentence.boundaries] # # return sentences # return(sentences) # } # # # Convert the text into sentences # article_text =convert_text_to_sentences(y, lang =&quot;en&quot;) # # # ### SEntiment analysis ### # #import tweet # # tweets = read.csv(file=&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Twitter Feed From TimesNow.csv&quot;) # # # document_lang &lt;-rep(&quot;en&quot;, length(tweets$text)) # tweets$text= as.character(tweets$text) # # tryCatch({ # # Perform sentiment analysis # output_1 &lt;-textaSentiment( # documents = tweets$text, # Input sentences or documents # languages = document_lang # # &quot;en&quot;(English, default)|&quot;es&quot;(Spanish)|&quot;fr&quot;(French)|&quot;pt&quot;(Portuguese) # ) # }, error = function(err) { # # Print error # geterrmessage() # }) # merged &lt;-output_1$results # # library(httr) # library(jsonlite) # #Setup # cogapikey&lt;-&quot;2673988d37f941f89440d665ae6dad9b&quot; # cogapi&lt;-&quot;https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages&quot; # # text=c(&quot;is this english?&quot; # ,&quot;tak er der mere kage&quot; # ,&quot;merci beaucoup&quot; # ,&quot;guten morgen&quot; # ,&quot;bonjour&quot; # ,&quot;merde&quot; # ,&quot;That&#39;s terrible&quot; # ,&quot;R is awesome&quot;) # # # Prep data # df&lt;-data_frame(id=1:8,text) # mydata&lt;-list(documents= df) # # # cogapi&lt;-&quot;https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment&quot; # # Construct a request # response&lt;-POST(cogapi, # add_headers(`Ocp-Apim-Subscription-Key`=cogapikey), # body=toJSON(mydata)) # # # Process reponse # respcontent&lt;-content(response, as=&quot;text&quot;) # # fromJSON(respcontent)$documents %&gt;% # mutate(id=as.numeric(id)) -&gt; # responses 8.4 Other topic Named entity recognition NER OOptical character recognition OCR "],
["bayes-analysis.html", "Chapter 9 Bayes Analysis 9.1 Introduction au bayseien 9.2 NAive bayes 9.3 Other bayes model", " Chapter 9 Bayes Analysis 9.1 Introduction au bayseien Bayes Theorem \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] Ou P(A) est le prior, P(A|B) le posterior, P(B) est le marginal likelihood, et P(B|A) est le likelihood. Prior : the certainty of an event occurring before some evidence is considered Posterior : The probability of the event A happening conditioned on another event 9.2 NAive bayes Utilisé pour la classification des documents, filtre spam. Naive Bayes essentially assumes that each explanatory variable is independent of the others and uses the distribution of these for each category of data to construct the distribution of the response variable given the explanatory variables. utilisation pure et simple du theoreme de baye Avantage du baysien: real time implementation. just update information. library(data.table) library(gmodels) library(splitstackshape) library(e1071) Data_Purchase &lt;-fread(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/PurchasePredictionDataset.csv&quot;,header=T,verbose =FALSE, showProgress =FALSE) str(Data_Purchase) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 500000 obs. of 12 variables: ## $ CUSTOMER_ID : chr &quot;000001&quot; &quot;000002&quot; &quot;000003&quot; &quot;000004&quot; ... ## $ ProductChoice : int 2 3 2 3 2 3 2 2 2 3 ... ## $ MembershipPoints : int 6 2 4 2 6 6 5 9 5 3 ... ## $ ModeOfPayment : chr &quot;MoneyWallet&quot; &quot;CreditCard&quot; &quot;MoneyWallet&quot; &quot;MoneyWallet&quot; ... ## $ ResidentCity : chr &quot;Madurai&quot; &quot;Kolkata&quot; &quot;Vijayawada&quot; &quot;Meerut&quot; ... ## $ PurchaseTenure : int 4 4 10 6 3 3 13 1 9 8 ... ## $ Channel : chr &quot;Online&quot; &quot;Online&quot; &quot;Online&quot; &quot;Online&quot; ... ## $ IncomeClass : chr &quot;4&quot; &quot;7&quot; &quot;5&quot; &quot;4&quot; ... ## $ CustomerPropensity : chr &quot;Medium&quot; &quot;VeryHigh&quot; &quot;Unknown&quot; &quot;Low&quot; ... ## $ CustomerAge : int 55 75 34 26 38 71 72 27 33 29 ... ## $ MartialStatus : int 0 0 0 0 1 0 0 0 0 1 ... ## $ LastPurchaseDuration: int 4 15 15 6 6 10 5 4 15 6 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; set.seed(917) # Data preparation table(Data_Purchase$ProductChoice) ## ## 1 2 3 4 ## 106603 199286 143893 50218 Data_Purchase &lt;-Data_Purchase[,c(&quot;CUSTOMER_ID&quot;,&quot;ProductChoice&quot;,&quot;MembershipPoints&quot;, &quot;IncomeClass&quot;,&quot;CustomerPropensity&quot;,&quot;LastPurchaseDuration&quot;)] #Delete NA from subset Data_Purchase &lt;-na.omit(Data_Purchase) Data_Purchase$CUSTOMER_ID &lt;-as.character(Data_Purchase$CUSTOMER_ID) #Stratified Sampling Data_Purchase_Model&lt;-stratified(Data_Purchase, group=c(&quot;ProductChoice&quot;), size=10000,replace=FALSE) table(Data_Purchase_Model$ProductChoice) ## ## 1 2 3 4 ## 10000 10000 10000 10000 Data_Purchase_Model$ProductChoice &lt;-as.factor(Data_Purchase_Model$ProductChoice) Data_Purchase_Model$IncomeClass &lt;-as.factor(Data_Purchase_Model$IncomeClass) Data_Purchase_Model$CustomerPropensity &lt;-as.factor(Data_Purchase_Model$CustomerPropensity) train &lt;-Data_Purchase_Model[sample(nrow(Data_Purchase_Model), size=nrow(Data_Purchase_Model)*(0.7), replace =TRUE, prob =NULL),] train &lt;-as.data.frame(train) test &lt;-as.data.frame(Data_Purchase_Model[!(Data_Purchase_Model$CUSTOMER_ID %in%train$CUSTOMER_ID),]) # model NB model_naiveBayes &lt;-naiveBayes(train[,c(3,4,5)], train[,2]) #evaluation model_naiveBayes_pred &lt;-predict(model_naiveBayes, train) vtrain = CrossTable(model_naiveBayes_pred, train[,2],prop.chisq =FALSE,dnn =c(&#39;predicted&#39;, &#39;actual&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 28000 ## ## ## | actual ## predicted | 1 | 2 | 3 | 4 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 4231 | 3384 | 2491 | 2420 | 12526 | ## | 0.338 | 0.270 | 0.199 | 0.193 | 0.447 | ## | 0.602 | 0.486 | 0.357 | 0.345 | | ## | 0.151 | 0.121 | 0.089 | 0.086 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 363 | 417 | 265 | 258 | 1303 | ## | 0.279 | 0.320 | 0.203 | 0.198 | 0.047 | ## | 0.052 | 0.060 | 0.038 | 0.037 | | ## | 0.013 | 0.015 | 0.009 | 0.009 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 1312 | 1601 | 2402 | 1851 | 7166 | ## | 0.183 | 0.223 | 0.335 | 0.258 | 0.256 | ## | 0.187 | 0.230 | 0.344 | 0.264 | | ## | 0.047 | 0.057 | 0.086 | 0.066 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 1122 | 1568 | 1824 | 2491 | 7005 | ## | 0.160 | 0.224 | 0.260 | 0.356 | 0.250 | ## | 0.160 | 0.225 | 0.261 | 0.355 | | ## | 0.040 | 0.056 | 0.065 | 0.089 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 7028 | 6970 | 6982 | 7020 | 28000 | ## | 0.251 | 0.249 | 0.249 | 0.251 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## ## model_naiveBayes_pred &lt;-predict(model_naiveBayes, test) vtest = CrossTable(model_naiveBayes_pred, test[,2],prop.chisq =FALSE,dnn =c(&#39;predicted&#39;, &#39;actual&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 19795 ## ## ## | actual ## predicted | 1 | 2 | 3 | 4 | Row Total | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 1 | 3036 | 2500 | 1767 | 1689 | 8992 | ## | 0.338 | 0.278 | 0.197 | 0.188 | 0.454 | ## | 0.614 | 0.502 | 0.358 | 0.343 | | ## | 0.153 | 0.126 | 0.089 | 0.085 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 2 | 257 | 294 | 215 | 159 | 925 | ## | 0.278 | 0.318 | 0.232 | 0.172 | 0.047 | ## | 0.052 | 0.059 | 0.044 | 0.032 | | ## | 0.013 | 0.015 | 0.011 | 0.008 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 3 | 888 | 1062 | 1664 | 1332 | 4946 | ## | 0.180 | 0.215 | 0.336 | 0.269 | 0.250 | ## | 0.180 | 0.213 | 0.337 | 0.270 | | ## | 0.045 | 0.054 | 0.084 | 0.067 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## 4 | 763 | 1127 | 1292 | 1750 | 4932 | ## | 0.155 | 0.229 | 0.262 | 0.355 | 0.249 | ## | 0.154 | 0.226 | 0.262 | 0.355 | | ## | 0.039 | 0.057 | 0.065 | 0.088 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 4944 | 4983 | 4938 | 4930 | 19795 | ## | 0.250 | 0.252 | 0.249 | 0.249 | | ## -------------|-----------|-----------|-----------|-----------|-----------| ## ## sum(diag(vtrain$prop.tbl)) ## [1] 0.34075 sum(diag(vtest$prop.tbl)) ## [1] 0.3406921 9.3 Other bayes model Gausian Naive Bayes Multinomial Naive Bayes Bayesian Belief Network Bayesien Network "],
["time-series.html", "Chapter 10 Time Series", " Chapter 10 Time Series "],
["others-ml-models.html", "Chapter 11 Others ML models 11.1 Support Vector Machines 11.2 Hadoop introduction 11.3 Machine Learning in R with Spark 11.4 Machine learning in R with H20", " Chapter 11 Others ML models 11.1 Support Vector Machines SVM A distance-based algorithm, trés bon pour la classification binaire dans les big dataset. Can deal with nonlinearity, overlapping classes, … etc. Classe separation : SVM Cherche the optimal hyperplane separating 2 classe by maximizing the margin between the closest points. The point lying on the margins are the Support Vectors. The line passing through the midpoint of the margins is the optimal hyperplane. Overlapping classe : Si point on the wring side, il peut etre pond?rer pour r?duire sont influence. On utilise the Hinge loss function qui est proportionnel a la distince from the margin/ Non liearity : Si une séparation linéaire ne peut etre trouvé, les observations sont projeté dans un espace a plus haute dimension using a kernel function ou les observations deviennent linéairement séparable. One popular Gaussian family kernel is the radial basis function. A radial basis function (RBF) is a real-valued function whose value depends only on the distance from the origin $ K(x,y) = (- ) $ . Il existe d’autre fonction kernel, kernel refet to a window function that is zero-valued outside of some chosen interval. c’est donc un probleme de minisation des distances Binary SVM Classifier library(e1071) library(rpart) library(gmodels) ### data pre ### breast_cancer_data &lt;-read.table(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/breast-cancer-wisconsin.data.txt&quot;,sep=&quot;,&quot;) breast_cancer_data$V11 =as.factor(breast_cancer_data$V11) # split data into a train and test set index &lt;-1:nrow(breast_cancer_data) test_data_index &lt;-sample(index, trunc(length(index)/3)) test_data &lt;-breast_cancer_data[test_data_index,] train_data &lt;-breast_cancer_data[-test_data_index,] # model svm.model &lt;-svm(V11 ~., data = train_data, cost =100, gamma =1) # note : in realworld dataset accurancy of 100% is not possible # but un medical dignostic il est important d&#39;etre proche svm_pred_train &lt;-predict(svm.model, train_data[,-11]) CrossTable(train_data$V11, svm_pred_train, prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE, dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 466 ## ## ## | predicted default ## actual default | 2 | 4 | Row Total | ## ---------------|-----------|-----------|-----------| ## 2 | 292 | 0 | 292 | ## | 0.627 | 0.000 | | ## ---------------|-----------|-----------|-----------| ## 4 | 0 | 174 | 174 | ## | 0.000 | 0.373 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 292 | 174 | 466 | ## ---------------|-----------|-----------|-----------| ## ## svm_pred_test &lt;-predict(svm.model, test_data[,-11]) CrossTable(test_data$V11, svm_pred_test,prop.chisq =FALSE, prop.c =FALSE, prop.r =FALSE, dnn =c(&#39;actual default&#39;, &#39;predicted default&#39;)) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 233 ## ## ## | predicted default ## actual default | 2 | 4 | Row Total | ## ---------------|-----------|-----------|-----------| ## 2 | 151 | 15 | 166 | ## | 0.648 | 0.064 | | ## ---------------|-----------|-----------|-----------| ## 4 | 0 | 67 | 67 | ## | 0.000 | 0.288 | | ## ---------------|-----------|-----------|-----------| ## Column Total | 151 | 82 | 233 | ## ---------------|-----------|-----------|-----------| ## ## multiclasse SVM : SVM Peut etre étendu a du multiclasse by creating multible binary classifier. create binary classifiers between one class and the rest of the classes between every pair of classe possibles For any new cases, the SVM classifier adopts a winner-takes-all strategy, in which the class with highest output is assigned. library( &#39;e1071&#39; ) Data_House_Worth &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6//House Worth Data.csv&quot;,header=TRUE) # model svm_multi_model &lt;-svm( HouseNetWorth ~StoreArea +LawnArea, Data_House_Worth ) svm_multi_model ## ## Call: ## svm(formula = HouseNetWorth ~ StoreArea + LawnArea, data = Data_House_Worth) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## gamma: 0.5 ## ## Number of Support Vectors: 120 res &lt;-predict( svm_multi_model, newdata=Data_House_Worth ) table(Data_House_Worth$HouseNetWorth,res) ## res ## High Low Medium ## High 122 1 7 ## Low 6 122 7 ## Medium 1 7 43 sum(diag(table(Data_House_Worth$HouseNetWorth,res)))/nrow(Data_House_Worth) ## [1] 0.9082278 SVM est un non probalistic binary classifier mais trés efficace. Peut sappliquer a la classification d’image, d’hypertext, character recognition, … 11.2 Hadoop introduction Hadoop framework consists of the following three modules Hadoop Distributed File System : This is the storage part of Hadoop Hadoop YARN: This is also known as the data operating system. Hadoop MapReduce: MapReduce decides the execution logic of what needs to be done with the data. The logic should be designed in such a way that it can execute in parallel with smaller chunks of data residing in a distributed cluster of machines. 11.3 Machine Learning in R with Spark At a high level, it provides tools such as: ML algorithms: Common learning algorithms such as classification, regression, clustering, and collaborative filtering Featurization: Feature extraction, transformation, dimensionality reduction, and selection Pipelines: Tools for constructing, evaluating, and tuning ML pipelines Persistence: Saving and loading algorithms, models, and pipelines Utilities: Linear algebra, statistics, data handling, etc. Need to be download : follow “data science using R, p 541” 11.4 Machine learning in R with H20 H2O is an open source high performance cluster for big data analysis. These techniques are not feasible to be executed on individual machines and need high-power computing. H2O is a Java Virtual Machine that is optimized for doing “in-memory” processing of distributed, parallel machine learning algorithms on clusters. # install.packages(&quot;h2o&quot;) # Load the h2o library in R #library(h2o) #Initiate a cluster in your machine #localH2O =h2o.init # The function demo runs all at once and outputs the entire output at one go. However, for better understanding of what the function does, we have split the output and explainedeach part in detail. # demo = demo(h2o.deeplearning) # more demo : # demo(package = &quot;h2o&quot;) Additional parameters are: Hidden, which specifies the hidden layer sizes, Activation, which specifies the type of activation function; the demo uses a Tanh function epochs, which directs the neural network with “How many times the dataset should be iterated (streamed) "],
["caret-package-1.html", "Chapter 12 Caret Package 12.1 Pre-Processing 12.2 Data Splitting 12.3 Model Training and tuning 12.4 Best available Models 12.5 Parallel Processing 12.6 Subsampling for class imbalances 12.7 Variables importance 12.8 measurung performance 12.9 feature selection", " Chapter 12 Caret Package library(caret) ## Warning: package &#39;caret&#39; was built under R version 3.3.3 ## Loading required package: lattice ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.3 orange &lt;- read.csv(&#39;https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv&#39;) path = &quot;C:/Website Rmarkdown/bookdown/save/&quot; 12.1 Pre-Processing Serveral function can be use to preprocess the data. Caret package assume that variables are numeric. Factor have been converted to dummy. Create Dummy variables dummyVars() : create dummy from one or more factors. In caret, one-hot-encodings can be created using dummyVars(). Just pass in all the features to dummyVars() as the training data and all the factor columns will automatically be converted to one-hot-encodings. Near Zero Variance PrÃ©dictors Si variables as un seul facteur ou trÃ¨s peu de varianc, elles peuvent biasÃ© une les modÃ¨les prÃ©dictifs. Si predictor trop unballanced, when we split data in subsample for crossvalidation or other subsample, predictor may become zero variance. some metric : frÃ©quency ratio : frequency of the most prevalent value over the second most frequent. (proche de 1 si bien equilibrÃ©) percent of unique values : number of unique values divided by the total number of samples. Approaches zero as the granularity of the data increases Identifying correlated predictors findCorrelation() function uses the following algorithm to flag predictors for removal. Linear dependencies findLinearCombos() function uses the QR decomposition of a matrix to enumerate sets of linear combinations ## Dummy variable library(earth) ## Warning: package &#39;earth&#39; was built under R version 3.3.3 ## Loading required package: plotmo ## Loading required package: plotrix ## Loading required package: TeachingDemos ## Warning: package &#39;TeachingDemos&#39; was built under R version 3.3.3 data(etitanic) head(model.matrix(survived ~ ., data = etitanic)) ## (Intercept) pclass2nd pclass3rd sexmale age sibsp parch ## 1 1 0 0 0 29.0000 0 0 ## 2 1 0 0 1 0.9167 1 2 ## 3 1 0 0 0 2.0000 1 2 ## 4 1 0 0 1 30.0000 1 2 ## 5 1 0 0 0 25.0000 1 2 ## 6 1 0 0 1 48.0000 0 0 # Use dummyVars to create dummy dummies &lt;- dummyVars(survived ~ ., data = etitanic) head(predict(dummies, newdata = etitanic)) ## pclass.1st pclass.2nd pclass.3rd sex.female sex.male age sibsp parch ## 1 1 0 0 1 0 29.0000 0 0 ## 2 1 0 0 0 1 0.9167 1 2 ## 3 1 0 0 1 0 2.0000 1 2 ## 4 1 0 0 0 1 30.0000 1 2 ## 5 1 0 0 1 0 25.0000 1 2 ## 6 1 0 0 0 1 48.0000 0 0 ## Near zero variance data(mdrr) data.frame(table(mdrrDescr$nR11)) ## Var1 Freq ## 1 0 501 ## 2 1 4 ## 3 2 23 nzv &lt;- nearZeroVar(mdrrDescr, saveMetrics= TRUE) nzv[nzv$nzv,][1:10,] ## freqRatio percentUnique zeroVar nzv ## nTB 23.00000 0.3787879 FALSE TRUE ## nBR 131.00000 0.3787879 FALSE TRUE ## nI 527.00000 0.3787879 FALSE TRUE ## nR03 527.00000 0.3787879 FALSE TRUE ## nR08 527.00000 0.3787879 FALSE TRUE ## nR11 21.78261 0.5681818 FALSE TRUE ## nR12 57.66667 0.3787879 FALSE TRUE ## D.Dr03 527.00000 0.3787879 FALSE TRUE ## D.Dr07 123.50000 5.8712121 FALSE TRUE ## D.Dr08 527.00000 0.3787879 FALSE TRUE nzv &lt;- nearZeroVar(mdrrDescr) filteredDescr &lt;- mdrrDescr[, -nzv] dim(filteredDescr) ## [1] 528 297 ## correlated predictors descrCor &lt;- cor(filteredDescr) highCorr &lt;- sum(abs(descrCor[upper.tri(descrCor)]) &gt; .999) # there are 65 descriptors that are almost perfectly correlated summary(descrCor[upper.tri(descrCor)]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.99610 -0.05373 0.25010 0.26080 0.65530 1.00000 # remove var with corr above 0.75 highlyCorDescr &lt;- findCorrelation(descrCor, cutoff = .75) filteredDescr &lt;- filteredDescr[,-highlyCorDescr] descrCor2 &lt;- cor(filteredDescr) summary(descrCor2[upper.tri(descrCor2)]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.70730 -0.05378 0.04418 0.06692 0.18860 0.74460 ## Linear dependencies # comboInfo &lt;- findLinearCombos(ltfrDesign) # comboInfo # ltfrDesign[, -comboInfo$remove] preProcess Operation on predictor like centering, scaling, … can be use in train() function Imputation KNN : For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g. using the mean) Bagged tree : For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. TRansforming predictor : need to be centered and scaled PCA boxcox : data need to be greater than zero exponential transformation, Yeo-Johnson .. library(AppliedPredictiveModeling) data(schedulingData) # The data are a mix of categorical and numeric predictors. # Yeo-Johnson transformation on the continuous predictors then center and scale them. pp_hpc &lt;- preProcess(schedulingData[, -8], method = c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;)) pp_hpc ## Created from 4331 samples and 7 variables ## ## Pre-processing: ## - centered (5) ## - ignored (2) ## - scaled (5) ## - Yeo-Johnson transformation (5) ## ## Lambda estimates for Yeo-Johnson transformation: ## -0.08, -0.03, -1.05, -1.1, 1.44 # Use predict to get transformed data transformed &lt;- predict(pp_hpc, newdata = schedulingData[, -8]) head(transformed) ## Protocol Compounds InputFields Iterations NumPending Hour Day ## 1 E 1.2289592 -0.6324580 -0.0615593 -0.554123 0.004586516 Tue ## 2 E -0.6065826 -0.8120473 -0.0615593 -0.554123 -0.043733201 Tue ## 3 E -0.5719534 -1.0131504 -2.7894869 -0.554123 -0.034967177 Thu ## 4 E -0.6427737 -1.0047277 -0.0615593 -0.554123 -0.964170752 Fri ## 5 E -0.5804713 -0.9564504 -0.0615593 -0.554123 -0.902085020 Fri ## 6 E -0.5804713 -0.9564504 -0.0615593 -0.554123 0.698108782 Wed # the predictor for the number of pending jobs, has a very sparse and unbalanced distribution: mean(schedulingData$NumPending == 0) ## [1] 0.7561764 # We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations : pp_no_nzv &lt;- preProcess(schedulingData[, -8], method = c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;, &quot;nzv&quot;)) pp_no_nzv ## Created from 4331 samples and 7 variables ## ## Pre-processing: ## - centered (4) ## - ignored (2) ## - removed (1) ## - scaled (4) ## - Yeo-Johnson transformation (4) ## ## Lambda estimates for Yeo-Johnson transformation: ## -0.08, -0.03, -1.05, 1.44 predict(pp_no_nzv, newdata = schedulingData[1:6, -8]) ## Protocol Compounds InputFields Iterations Hour Day ## 1 E 1.2289592 -0.6324580 -0.0615593 0.004586516 Tue ## 2 E -0.6065826 -0.8120473 -0.0615593 -0.043733201 Tue ## 3 E -0.5719534 -1.0131504 -2.7894869 -0.034967177 Thu ## 4 E -0.6427737 -1.0047277 -0.0615593 -0.964170752 Fri ## 5 E -0.5804713 -0.9564504 -0.0615593 -0.902085020 Fri ## 6 E -0.5804713 -0.9564504 -0.0615593 0.698108782 Wed Class distance calculations caret contain fonction to generate new predictors variables based on distance to class centroids (see linear discriminant analysis). For each level of a factor variable, the class centroid and covariance matrix is calculated. For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non-linear models when the true decision boundary is actually linear. 12.2 Data Splitting Simple splitting based on the outcome createDataPartition : create balanced split of the data list = FALSE avoids returning the data as a list times, that can create multiple splits at once the data indices are returned in a list of integer vectors createResample can be used to make simple bootstrap samples createFolds can be used to generate balanced crossvalidation groupings from a set of data. splitting based on the predictors maxDissim can be used to create subsamples using a maximum dissimilarity approach. Data splitting for time series createTimeSlices initialWindow: the initial number of consecutive values in each training set sample horizon: The number of consecutive values in test set sample fixedWindow: A logical: if FALSE, the training set always start at the first sample and the training set size will vary over data splits. splitting with important groups see documentation package library(mlbench) ## Warning: package &#39;mlbench&#39; was built under R version 3.3.3 load(file = &quot;C:/Website Rmarkdown/bookdown/save/Sonar.RData&quot;) # inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) # training &lt;- Sonar[ inTraining,] training = sonar_train # testing &lt;- Sonar[-inTraining,] testing = sonar_test 12.3 Model Training and tuning ** train() function** evaluate, using resampling, the effect of model tuning parameters on performance Choose the optimalâ model across these parameters Estimate model performance from a training set Train() algorithm for each parameter set for each resampling iteration fit predict on train set out of sample end calculate average perfomance end determine optimal paramater set fit final model information to done model type parameter value resampling solution k fold crossvalidation : Te original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. And repeated. The k results can then be averaged to produce a single estimation. Leave one out Bootstrap : random sampling with replacement 12.3.1 Exemple : Basic tuning for boosted tree model GBM fitControl &lt;- trainControl( method = &quot;repeatedcv&quot; , number = 10, ## 10-fold CV : number of fold or number of resampling iteration repeats = 10) ## repeated ten times # gbmFit1 &lt;- train(Class ~ ., data = training, # method = &quot;gbm&quot;, # trControl = fitControl, # ## This last option is actually one for gbm() that passes through # verbose = FALSE) # save(gbmFit1, file= paste(path, &quot;gbmfit1.RData&quot;,sep=&quot;&quot; )) load(file= paste(path, &quot;gbmfit1.RData&quot;,sep=&quot;&quot; )) gbmFit1 ## Stochastic Gradient Boosting ## ## 125 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 113, 111, 113, 112, 113, 113, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.7951923 0.5889691 ## 1 100 0.8030403 0.6043046 ## 1 150 0.8131868 0.6246497 ## 2 50 0.8161630 0.6305226 ## 2 100 0.8253938 0.6491769 ## 2 150 0.8308059 0.6599040 ## 3 50 0.8129945 0.6245649 ## 3 100 0.8247344 0.6475546 ## 3 150 0.8336630 0.6657373 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 150, ## interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. For a gradient boosting machine (GBM) model, there are three main tuning parameters: number of iterations, i.e.trees, (called n.trees in the gbm function) complexity of the tree, called interaction.depth learning rate: how quickly the algorithm adapts, called shrinkage the minimum number of training set samples in a node to commence splitting (n.minobsinnode) Train() can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p. 12.3.2 Customizing the Tuning Process **Alternate Tuning Grid*s** : tuneGrid option in train Par dÃ©faut train chose model with largest perfomance value. Il existe d’autr methode de recherche pour le uning des paramÃ¨tre comme random search ( option search = “random” in the call to trainControl) Plotting the resampling profile Plot function to examine the relationship between the estimates of the performance and the tuning parameters. trainControl : generates parameters that further control how models are created Method = boot, cv, repeatedcv , … oob = out-of-bag estimates (for DT or RF) number and repeats (only if repeatedcv): number controls with the numbe r of folds in K-fold cross-validation or number of resampling iterations for bootstrapping allowParallel: a logical that governs whether train should use parallel processing (if availible). summaryFunction that specifies a function for computing performance for user defined performance metrics Alternate Performance Metrics defaut ; RMSE, MSA, R2 for regression and accurancy , kappa for classification. twoClassSummary() function, will compute the sensitivity, specificity and area under the ROC curve: Choosing the Final model train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. User-defined functions can be used. tolerance function could be used to find a less complex model. Extracting Predictions and Class Probabilities objects produced by the train function contain the optimized model in the finalModel sub-object predict.train, the type options are standardized to be “class” and “prob” Fitting Models Without Parameter Tuning In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method = “none” option in trainControl gbmGrid &lt;- expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:30)*50, shrinkage = 0.1, n.minobsinnode = 20) # gbmFit2 &lt;- train(Class ~ ., data = training, # method = &quot;gbm&quot;, # trControl = fitControl, # verbose = FALSE, # ## Now specify the exact models # ## to evaluate: # tuneGrid = gbmGrid) # # save(gbmFit2,file= paste(path, &quot;gbmFit2.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;gbmFit2.RData&quot;,sep=&quot;&quot; )) gbmFit2 ## Stochastic Gradient Boosting ## ## 125 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 112, 113, 113, 113, 113, 113, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.7952747 0.5890794 ## 1 100 0.8237271 0.6458003 ## 1 150 0.8209524 0.6401562 ## 1 200 0.8243407 0.6472826 ## 1 250 0.8191117 0.6363364 ## 1 300 0.8352656 0.6690219 ## 1 350 0.8219139 0.6419521 ## 1 400 0.8243407 0.6467396 ## 1 450 0.8229304 0.6436160 ## 1 500 0.8267857 0.6516025 ## 1 550 0.8261996 0.6507117 ## 1 600 0.8246062 0.6471799 ## 1 650 0.8245330 0.6471900 ## 1 700 0.8245513 0.6470475 ## 1 750 0.8229945 0.6441716 ## 1 800 0.8230037 0.6439807 ## 1 850 0.8253205 0.6488246 ## 1 900 0.8254396 0.6489119 ## 1 950 0.8228205 0.6438626 ## 1 1000 0.8205128 0.6393080 ## 1 1050 0.8219780 0.6421932 ## 1 1100 0.8244139 0.6468354 ## 1 1150 0.8242949 0.6467066 ## 1 1200 0.8227473 0.6435008 ## 1 1250 0.8234615 0.6450454 ## 1 1300 0.8232784 0.6446687 ## 1 1350 0.8218681 0.6418844 ## 1 1400 0.8235256 0.6452181 ## 1 1450 0.8260256 0.6502181 ## 1 1500 0.8259615 0.6499707 ## 5 50 0.8094322 0.6172790 ## 5 100 0.8355952 0.6698408 ## 5 150 0.8370696 0.6729793 ## 5 200 0.8377106 0.6739154 ## 5 250 0.8417491 0.6821800 ## 5 300 0.8418040 0.6819201 ## 5 350 0.8349359 0.6679888 ## 5 400 0.8348077 0.6677639 ## 5 450 0.8355220 0.6694073 ## 5 500 0.8347985 0.6683104 ## 5 550 0.8293407 0.6570683 ## 5 600 0.8322436 0.6627737 ## 5 650 0.8285165 0.6553292 ## 5 700 0.8292216 0.6567569 ## 5 750 0.8275549 0.6532773 ## 5 800 0.8283883 0.6549422 ## 5 850 0.8291575 0.6565849 ## 5 900 0.8299267 0.6580735 ## 5 950 0.8299908 0.6581782 ## 5 1000 0.8282692 0.6547392 ## 5 1050 0.8332051 0.6647494 ## 5 1100 0.8330128 0.6643746 ## 5 1150 0.8290385 0.6563774 ## 5 1200 0.8306410 0.6596469 ## 5 1250 0.8298718 0.6580788 ## 5 1300 0.8298718 0.6580788 ## 5 1350 0.8282692 0.6549547 ## 5 1400 0.8291667 0.6567222 ## 5 1450 0.8282692 0.6549874 ## 5 1500 0.8307051 0.6598866 ## 9 50 0.8039377 0.6058817 ## 9 100 0.8198901 0.6380325 ## 9 150 0.8278938 0.6544494 ## 9 200 0.8328846 0.6640918 ## 9 250 0.8241026 0.6467630 ## 9 300 0.8203205 0.6384411 ## 9 350 0.8204487 0.6391572 ## 9 400 0.8197527 0.6375170 ## 9 450 0.8205678 0.6393621 ## 9 500 0.8267308 0.6514263 ## 9 550 0.8210897 0.6400578 ## 9 600 0.8219872 0.6420808 ## 9 650 0.8226923 0.6437323 ## 9 700 0.8219139 0.6421279 ## 9 750 0.8220513 0.6422191 ## 9 800 0.8189744 0.6358772 ## 9 850 0.8239011 0.6459592 ## 9 900 0.8254396 0.6490944 ## 9 950 0.8253114 0.6490100 ## 9 1000 0.8237729 0.6458428 ## 9 1050 0.8246703 0.6476778 ## 9 1100 0.8244780 0.6473060 ## 9 1150 0.8230678 0.6445128 ## 9 1200 0.8208150 0.6399585 ## 9 1250 0.8256227 0.6495035 ## 9 1300 0.8254396 0.6491686 ## 9 1350 0.8294139 0.6569814 ## 9 1400 0.8231227 0.6444333 ## 9 1450 0.8230678 0.6442198 ## 9 1500 0.8260897 0.6503138 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 20 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 300, ## interaction.depth = 5, shrinkage = 0.1 and n.minobsinnode = 20. trellis.par.set(caretTheme()) ggplot(gbmFit2, metric = &quot;Kappa&quot;) fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, classProbs = TRUE, ## Estimate class probabilities ## Evaluate performance using ## the following function summaryFunction = twoClassSummary) # gbmFit3 &lt;- train(Class ~ ., data = training, # method = &quot;gbm&quot;, # trControl = fitControl, # verbose = FALSE, # tuneGrid = gbmGrid, # ## Specify which metric to optimize # metric = &quot;ROC&quot;) # save(gbmFit3,file= paste(path, &quot;gbmFit3.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;gbmFit3.RData&quot;,sep=&quot;&quot; )) gbmFit3 ## Stochastic Gradient Boosting ## ## 125 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 113, 112, 113, 113, 111, 113, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.8777211 0.8304762 0.7542857 ## 1 100 0.8940363 0.8545238 0.8016667 ## 1 150 0.8937075 0.8583333 0.8042857 ## 1 200 0.8940023 0.8583333 0.7964286 ## 1 250 0.8933333 0.8626190 0.8011905 ## 1 300 0.8944898 0.8678571 0.8061905 ## 1 350 0.8995181 0.8692857 0.8092857 ## 1 400 0.8976474 0.8676190 0.8169048 ## 1 450 0.8972222 0.8633333 0.8104762 ## 1 500 0.8977438 0.8614286 0.8123810 ## 1 550 0.8960714 0.8597619 0.8088095 ## 1 600 0.8984580 0.8597619 0.8057143 ## 1 650 0.8985034 0.8514286 0.8057143 ## 1 700 0.8979535 0.8533333 0.8071429 ## 1 750 0.8969558 0.8535714 0.8071429 ## 1 800 0.8960034 0.8535714 0.8038095 ## 1 850 0.8954478 0.8519048 0.8054762 ## 1 900 0.8944501 0.8530952 0.8021429 ## 1 950 0.8962302 0.8533333 0.8002381 ## 1 1000 0.8965873 0.8516667 0.8121429 ## 1 1050 0.8968707 0.8516667 0.8038095 ## 1 1100 0.8956009 0.8516667 0.8092857 ## 1 1150 0.8960374 0.8483333 0.8057143 ## 1 1200 0.8954819 0.8485714 0.8104762 ## 1 1250 0.8936961 0.8435714 0.8095238 ## 1 1300 0.8954025 0.8497619 0.8057143 ## 1 1350 0.8938946 0.8464286 0.8040476 ## 1 1400 0.8923866 0.8464286 0.8040476 ## 1 1450 0.8916327 0.8480952 0.8054762 ## 1 1500 0.8923073 0.8497619 0.8054762 ## 5 50 0.8746542 0.8269048 0.7692857 ## 5 100 0.8948016 0.8423810 0.7840476 ## 5 150 0.8944331 0.8502381 0.7980952 ## 5 200 0.8927664 0.8588095 0.7959524 ## 5 250 0.8954989 0.8554762 0.8023810 ## 5 300 0.8937925 0.8552381 0.8054762 ## 5 350 0.8918481 0.8554762 0.8073810 ## 5 400 0.8942120 0.8602381 0.8092857 ## 5 450 0.8898696 0.8602381 0.8126190 ## 5 500 0.8920181 0.8504762 0.8076190 ## 5 550 0.8919274 0.8504762 0.8059524 ## 5 600 0.8931236 0.8488095 0.8028571 ## 5 650 0.8930499 0.8523810 0.8042857 ## 5 700 0.8940420 0.8440476 0.8073810 ## 5 750 0.8946372 0.8473810 0.8040476 ## 5 800 0.8959070 0.8440476 0.8073810 ## 5 850 0.8949603 0.8485714 0.8090476 ## 5 900 0.8946882 0.8421429 0.8076190 ## 5 950 0.8942120 0.8433333 0.8092857 ## 5 1000 0.8926644 0.8435714 0.8076190 ## 5 1050 0.8933390 0.8483333 0.8107143 ## 5 1100 0.8941723 0.8469048 0.8107143 ## 5 1150 0.8947279 0.8483333 0.8171429 ## 5 1200 0.8940079 0.8452381 0.8171429 ## 5 1250 0.8909977 0.8469048 0.8138095 ## 5 1300 0.8926984 0.8416667 0.8090476 ## 5 1350 0.8918651 0.8435714 0.8140476 ## 5 1400 0.8937358 0.8454762 0.8104762 ## 5 1450 0.8936961 0.8450000 0.8090476 ## 5 1500 0.8944104 0.8483333 0.8138095 ## 9 50 0.8792517 0.8414286 0.7738095 ## 9 100 0.8923696 0.8421429 0.7907143 ## 9 150 0.8922846 0.8633333 0.7971429 ## 9 200 0.8911905 0.8561905 0.7921429 ## 9 250 0.8906009 0.8611905 0.8014286 ## 9 300 0.8902324 0.8614286 0.7945238 ## 9 350 0.8909127 0.8595238 0.7957143 ## 9 400 0.8907937 0.8611905 0.7959524 ## 9 450 0.8890873 0.8583333 0.7978571 ## 9 500 0.8908333 0.8580952 0.7959524 ## 9 550 0.8895125 0.8547619 0.8026190 ## 9 600 0.8905045 0.8511905 0.8009524 ## 9 650 0.8901814 0.8466667 0.8011905 ## 9 700 0.8901474 0.8483333 0.8009524 ## 9 750 0.8924490 0.8469048 0.8059524 ## 9 800 0.8942347 0.8471429 0.8061905 ## 9 850 0.8926190 0.8407143 0.8045238 ## 9 900 0.8939966 0.8464286 0.8045238 ## 9 950 0.8926927 0.8435714 0.8028571 ## 9 1000 0.8917857 0.8388095 0.7995238 ## 9 1050 0.8926134 0.8466667 0.8059524 ## 9 1100 0.8923753 0.8438095 0.8109524 ## 9 1150 0.8935261 0.8421429 0.8142857 ## 9 1200 0.8928401 0.8390476 0.8126190 ## 9 1250 0.8920522 0.8359524 0.8109524 ## 9 1300 0.8923299 0.8373810 0.8126190 ## 9 1350 0.8924093 0.8407143 0.8126190 ## 9 1400 0.8921429 0.8407143 0.8092857 ## 9 1450 0.8932143 0.8409524 0.8109524 ## 9 1500 0.8926247 0.8392857 0.8109524 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 20 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 350, ## interaction.depth = 1, shrinkage = 0.1 and n.minobsinnode = 20. whichTwoPct &lt;- tolerance(gbmFit3$results, metric = &quot;ROC&quot;, tol = 2, maximize = TRUE) gbmFit3$results[whichTwoPct,1:6] ## shrinkage interaction.depth n.minobsinnode n.trees ROC Sens ## 2 0.1 1 20 100 0.8940363 0.8545238 # This indicates that we can get a less complex model with an area under the ROC curve of 0.901 (compared to the pick the best value of 0.914). # predict(gbmFit3, newdata = head(testing)) # predict(gbmFit3, newdata = head(testing), type = &quot;prob&quot;) # Fitting Models Without Parameter Tuning fitControl &lt;- trainControl(method = &quot;none&quot;, classProbs = TRUE) set.seed(825) # gbmFit4 &lt;- train(Class ~ ., data = training, # method = &quot;gbm&quot;, # trControl = fitControl, # verbose = FALSE, # ## Only a single model can be passed to the # ## function when no resampling is used: # tuneGrid = data.frame(interaction.depth = 4, # n.trees = 100, # shrinkage = .1, # n.minobsinnode = 20), # metric = &quot;ROC&quot;) # save(gbmFit4,file= paste(path, &quot;gbmFit4.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;gbmFit4.RData&quot;,sep=&quot;&quot; )) gbmFit4 ## Stochastic Gradient Boosting ## ## 125 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: None Random hyperparameter search To use random search, another option is available in trainControl called search. Possible values of this argument are “grid” and “random”. The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the tuneLength option to train. fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 3, classProbs = TRUE, summaryFunction = twoClassSummary, search = &quot;random&quot;, verboseIter = FALSE) # rda_fit &lt;- train(Class ~ ., data = training, # method = &quot;rda&quot;, # metric = &quot;ROC&quot;, # tuneLength = 30, # trControl = fitControl) # # save(rda_fit,file= paste(path, &quot;rda_fit.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;rda_fit.RData&quot;,sep=&quot;&quot; )) rda_fit ## Regularized Discriminant Analysis ## ## 125 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 3 times) ## Summary of sample sizes: 100, 99, 101, 100, 100, 100, ... ## Resampling results across tuning parameters: ## ## gamma lambda ROC Sens Spec ## 0.03177874 0.767664044 0.8443677 0.8153846 0.7529915 ## 0.03868192 0.499283304 0.8470113 0.8158120 0.7692308 ## 0.11834801 0.974493793 0.8548214 0.8094017 0.7414530 ## 0.12391186 0.018063038 0.8332950 0.8209402 0.7688034 ## 0.13442487 0.868918547 0.8590757 0.8572650 0.7307692 ## 0.19249104 0.335761243 0.8475619 0.8320513 0.7380342 ## 0.23568481 0.064135040 0.8410585 0.8324786 0.7431624 ## 0.23814584 0.986270274 0.8543694 0.8205128 0.7692308 ## 0.25082994 0.674919744 0.8686199 0.8688034 0.7307692 ## 0.28285931 0.576888058 0.8632287 0.8423077 0.7098291 ## 0.29099029 0.474277013 0.8614152 0.8423077 0.7059829 ## 0.29601805 0.002963208 0.8413516 0.8324786 0.7482906 ## 0.33633553 0.283586169 0.8554350 0.8376068 0.7115385 ## 0.41798776 0.881581948 0.8562158 0.8414530 0.7264957 ## 0.45885413 0.701431940 0.8620781 0.8893162 0.7102564 ## 0.48684373 0.545997273 0.8705375 0.8735043 0.6837607 ## 0.48845661 0.377704420 0.8678364 0.8585470 0.6786325 ## 0.51491517 0.592224877 0.8669242 0.8893162 0.7047009 ## 0.53206420 0.339941226 0.8661900 0.8585470 0.6794872 ## 0.54020648 0.253930177 0.8619521 0.8585470 0.6846154 ## 0.56009903 0.183772303 0.8602756 0.8427350 0.6846154 ## 0.56472058 0.995162379 0.8480304 0.7833333 0.7585470 ## 0.58045730 0.773613530 0.8595880 0.8628205 0.7106838 ## 0.67085142 0.287354882 0.8641984 0.8841880 0.6521368 ## 0.69503284 0.348973440 0.8597304 0.8841880 0.6410256 ## 0.72206263 0.653406920 0.8582512 0.8576923 0.6619658 ## 0.76035804 0.183676074 0.8548050 0.8632479 0.6465812 ## 0.81091174 0.317173641 0.8526052 0.8581197 0.6141026 ## 0.86234436 0.272931617 0.8494549 0.8482906 0.6192308 ## 0.98847635 0.580160726 0.7629684 0.7153846 0.6350427 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0.4868437 and lambda ## = 0.5459973. # view of the random search ggplot(rda_fit) + theme(legend.position = &quot;top&quot;) 12.3.3 Exploring and Comparing Resampling Distributions Within Model explore relationships between tuning parameters and the resampling results for a specific model xyplot and stripplot can be used to plot resampling statistics against (numeric) tuning parameters. histogram and densityplot can also be used to look at distributions of the tuning parameters Between models resample() can be use to collect the resampling result and make statistical statements about their performance differences of different model. several lattice plot methods that can be used to visualize the resampling distributions # svmFit &lt;- train(Class ~ ., data = training, # method = &quot;svmRadial&quot;, # trControl = fitControl, # preProc = c(&quot;center&quot;, &quot;scale&quot;), # tuneLength = 8, # metric = &quot;ROC&quot;) # # save(svmFit,file= paste(path, &quot;svmFit.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;svmFit.RData&quot;,sep=&quot;&quot; )) resamps &lt;- resamples(list(GBM = rda_fit, SVM = svmFit)) summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: GBM, SVM ## Number of resamples: 15 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.6923 0.8429 0.8750 0.8705 0.9071 0.9808 0 ## SVM 0.8403 0.9046 0.9423 0.9335 0.9744 0.9941 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.7500 0.8397 0.9167 0.8735 0.9231 1 0 ## SVM 0.8333 0.9231 1.0000 0.9577 1.0000 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.3846 0.5833 0.6667 0.6838 0.8013 0.9231 0 ## SVM 0.4615 0.7083 0.7692 0.7735 0.9167 0.9231 0 theme1 &lt;- trellis.par.get() theme1$plot.symbol$col = rgb(.2, .2, .2, .4) theme1$plot.symbol$pch = 16 theme1$plot.line$col = rgb(1, 0, 0, .7) theme1$plot.line$lwd &lt;- 2 trellis.par.set(theme1) bwplot(resamps, layout = c(3, 1)) dotplot(resamps, metric = &quot;ROC&quot;) splom(resamps) 12.4 Best available Models Logit ababoost random forest xgboost SVM trainRowNumbers &lt;- createDataPartition(orange$Purchase, p=0.8, list=FALSE) trainData &lt;- orange[trainRowNumbers,] testData &lt;- orange[-trainRowNumbers,] x = trainData[, 2:18] y = trainData$Purchase # library(skimr) # skimmed &lt;- skim_to_wide(trainData) # skimmed[, c(1:5, 9:11, 13, 15:16)] preProcess_missingdata_model &lt;- preProcess(trainData, method=&#39;knnImpute&#39;) preProcess_missingdata_model ## Created from 823 samples and 18 variables ## ## Pre-processing: ## - centered (16) ## - ignored (2) ## - 5 nearest neighbor imputation (16) ## - scaled (16) trainData &lt;- predict(preProcess_missingdata_model, newdata = trainData) anyNA(trainData) ## [1] FALSE # Creating dummy variables is converting a categorical variable to as many binary variables as here are categories. dummies_model &lt;- dummyVars(Purchase ~ ., data=trainData) trainData_mat &lt;- predict(dummies_model, newdata = trainData) ## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev ## = object$lvls): variable &#39;Purchase&#39; is not a factor trainData &lt;- data.frame(trainData_mat) preProcess_range_model &lt;- preProcess(trainData, method=&#39;range&#39;) trainData &lt;- predict(preProcess_range_model, newdata = trainData) # Append the Y variable trainData$Purchase &lt;- y featurePlot(x = trainData[, 1:18], y = trainData$Purchase, plot = &quot;box&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) featurePlot(x = trainData[, 1:18], y = trainData$Purchase, plot = &quot;density&quot;, strip=strip.custom(par.strip.text=list(cex=.7)), scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;))) # feature selection subsets &lt;- c(1:5, 10, 15, 18) ctrl &lt;- rfeControl(functions = rfFuncs, method = &quot;repeatedcv&quot;, repeats = 5, verbose = FALSE) # lmProfile &lt;- rfe(x=trainData[, 1:4], y=trainData$Purchase, # sizes = subsets, # rfeControl = ctrl) # lmProfile # modeling ## get some info modelLookup(&#39;earth&#39;) ## model parameter label forReg forClass probModel ## 1 earth nprune #Terms TRUE TRUE TRUE ## 2 earth degree Product Degree TRUE TRUE TRUE model_mars = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred fitted &lt;- predict(model_mars) plot(model_mars, main=&quot;Model Accuracies with MARS&quot;) varimp_mars &lt;- varImp(model_mars) plot(varimp_mars, main=&quot;Variable Importance with MARS&quot;) # same imputation for test set testData2 &lt;- predict(preProcess_missingdata_model, testData) testData3 &lt;- predict(dummies_model, testData2) ## Warning in model.frame.default(Terms, newdata, na.action = na.action, xlev ## = object$lvls): variable &#39;Purchase&#39; is not a factor testData4 &lt;- predict(preProcess_range_model, testData3) predicted &lt;- predict(model_mars, testData4) confusionMatrix(reference = testData$Purchase, data = predicted, mode=&#39;everything&#39;, positive=&#39;MM&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 112 17 ## MM 18 66 ## ## Accuracy : 0.8357 ## 95% CI : (0.779, 0.8828) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 7.167e-13 ## ## Kappa : 0.6553 ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.7952 ## Specificity : 0.8615 ## Pos Pred Value : 0.7857 ## Neg Pred Value : 0.8682 ## Precision : 0.7857 ## Recall : 0.7952 ## F1 : 0.7904 ## Prevalence : 0.3897 ## Detection Rate : 0.3099 ## Detection Prevalence : 0.3944 ## Balanced Accuracy : 0.8284 ## ## &#39;Positive&#39; Class : MM ## # tuning model ## by tunelength fitControl &lt;- trainControl( method = &#39;cv&#39;, # k-fold cross validation number = 5, # number of folds savePredictions = &#39;final&#39;, # saves predictions for optimal tuning parameter classProbs = T, # should class probabilities be returned summaryFunction=twoClassSummary # results summary function ) # tuneLength corresponds to the number of unique values for the tuning parameters caret will consider while forming the hyper parameter combinations. # Step 1: Tune hyper parameters by setting tuneLength model_mars2 = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;, tuneLength = 5, metric=&#39;ROC&#39;, trControl = fitControl) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred model_mars2 ## Multivariate Adaptive Regression Spline ## ## 857 samples ## 18 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 685, 685, 686, 685, 687 ## Resampling results across tuning parameters: ## ## nprune ROC Sens Spec ## 2 0.8724471 0.8699084 0.7036183 ## 6 0.8961577 0.8660989 0.7336047 ## 11 0.8938492 0.8584799 0.7395749 ## 15 0.8888883 0.8623077 0.7276346 ## 20 0.8894142 0.8661172 0.7216644 ## ## Tuning parameter &#39;degree&#39; was held constant at a value of 1 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 6 and degree = 1. # Step 2: Predict on testData and Compute the confusion matrix predicted2 &lt;- predict(model_mars2, testData4) confusionMatrix(reference = testData$Purchase, data = predicted2, mode=&#39;everything&#39;, positive=&#39;MM&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 112 17 ## MM 18 66 ## ## Accuracy : 0.8357 ## 95% CI : (0.779, 0.8828) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 7.167e-13 ## ## Kappa : 0.6553 ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.7952 ## Specificity : 0.8615 ## Pos Pred Value : 0.7857 ## Neg Pred Value : 0.8682 ## Precision : 0.7857 ## Recall : 0.7952 ## F1 : 0.7904 ## Prevalence : 0.3897 ## Detection Rate : 0.3099 ## Detection Prevalence : 0.3944 ## Balanced Accuracy : 0.8284 ## ## &#39;Positive&#39; Class : MM ## # by tunegrid marsGrid &lt;- expand.grid(nprune = c(2, 4, 6, 8, 10), degree = c(1, 2, 3)) # Step 2: Tune hyper parameters by setting tuneGrid model_mars3 = train(Purchase ~ ., data=trainData, method=&#39;earth&#39;, metric=&#39;ROC&#39;, tuneGrid = marsGrid, trControl = fitControl) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred model_mars3 ## Multivariate Adaptive Regression Spline ## ## 857 samples ## 18 predictor ## 2 classes: &#39;CH&#39;, &#39;MM&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 687, 685, 685, 686, 685 ## Resampling results across tuning parameters: ## ## degree nprune ROC Sens Spec ## 1 2 0.8725588 0.8700366 0.7093623 ## 1 4 0.8952736 0.8796703 0.7333786 ## 1 6 0.8944987 0.8642674 0.7483492 ## 1 8 0.8907373 0.8681319 0.7483944 ## 1 10 0.8902256 0.8738462 0.7483944 ## 2 2 0.8615232 0.8146337 0.7904116 ## 2 4 0.8929948 0.8815568 0.7483492 ## 2 6 0.8888741 0.8548535 0.7395296 ## 2 8 0.8879558 0.8566850 0.7454093 ## 2 10 0.8871599 0.8433333 0.7543193 ## 3 2 0.8253030 0.8491026 0.6795115 ## 3 4 0.8952165 0.8815385 0.7513795 ## 3 6 0.8898686 0.8586081 0.7483492 ## 3 8 0.8932491 0.8547436 0.7633650 ## 3 10 0.8857792 0.8489744 0.7482587 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nprune = 4 and degree = 1. # Step 3: Predict on testData and Compute the confusion matrix predicted3 &lt;- predict(model_mars3, testData4) confusionMatrix(reference = testData$Purchase, data = predicted3, mode=&#39;everything&#39;, positive=&#39;MM&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction CH MM ## CH 110 18 ## MM 20 65 ## ## Accuracy : 0.8216 ## 95% CI : (0.7635, 0.8705) ## No Information Rate : 0.6103 ## P-Value [Acc &gt; NIR] : 2.139e-11 ## ## Kappa : 0.6266 ## Mcnemar&#39;s Test P-Value : 0.8711 ## ## Sensitivity : 0.7831 ## Specificity : 0.8462 ## Pos Pred Value : 0.7647 ## Neg Pred Value : 0.8594 ## Precision : 0.7647 ## Recall : 0.7831 ## F1 : 0.7738 ## Prevalence : 0.3897 ## Detection Rate : 0.3052 ## Detection Prevalence : 0.3991 ## Balanced Accuracy : 0.8146 ## ## &#39;Positive&#39; Class : MM ## # Compare model # model_adaboost = train(Purchase ~ ., data=trainData[1:100,], method=&#39;adaboost&#39;, tuneLength=2, trControl = fitControl) # model_rf = train(Purchase ~ ., data=trainData[1:100,], method=&#39;rf&#39;, tuneLength=5, trControl = fitControl) # model_xgbDART = train(Purchase ~ ., data=trainData, method=&#39;xgbDART&#39;, tuneLength=5, trControl = fitControl, verbose=F) # model_svmRadial = train(Purchase ~ ., data=trainData, method=&#39;svmRadial&#39;, tuneLength=15, trControl = fitControl) # models_compare &lt;- resamples(list(ADABOOST=model_adaboost, RF=model_rf)) #, XGBDART=model_xgbDART, MARS=model_mars3, SVM=model_svmRadial)) # save(models_compare,file= paste(path, &quot;models_compare.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;models_compare.RData&quot;,sep=&quot;&quot; )) summary(models_compare) ## ## Call: ## summary.resamples(object = models_compare) ## ## Models: ADABOOST, RF ## Number of resamples: 5 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.5000 0.5417 0.5741 0.6172 0.6759 0.7941 0 ## RF 0.2353 0.4352 0.5278 0.5439 0.6324 0.8889 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0.8333 0.8889 0.8889 0.9105 0.9412 1 0 ## RF 0.9444 1.0000 1.0000 0.9889 1.0000 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## ADABOOST 0 0 0 0 0 0 0 ## RF 0 0 0 0 0 0 0 scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) bwplot(models_compare, scales=scales) # ensemble predictions from multiple models using caretEnsemble library(caretEnsemble) ## Warning: package &#39;caretEnsemble&#39; was built under R version 3.3.3 ## ## Attaching package: &#39;caretEnsemble&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## autoplot trainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=3, repeats=2, savePredictions=TRUE, classProbs=TRUE) algorithmList &lt;- c(&#39;rf&#39;, &#39;adaboost&#39;) #, &#39;earth&#39;, &#39;xgbDART&#39;, &#39;svmRadial&#39;) # models &lt;- caretList(Purchase ~ ., data=trainData[1:50,], trControl=trainControl, methodList=algorithmList) # save(models,file= paste(path, &quot;models.RData&quot;,sep=&quot;&quot; )) load(file=paste(path, &quot;models.RData&quot;,sep=&quot;&quot; )) results &lt;- resamples(models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rf, adaboost ## Number of resamples: 6 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf 0.7500 0.7537 0.7647 0.7794 0.8088 0.8235 0 ## adaboost 0.6471 0.6921 0.7353 0.7402 0.8006 0.8235 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rf -0.1034 -0.1018 -0.04839 -0.01887 0.00000 0.1905 0 ## adaboost -0.2143 -0.1734 -0.13060 -0.05963 -0.02419 0.2941 0 # Combine the predictions of multiple models to form a final prediction # Create the trainControl set.seed(101) stackControl &lt;- trainControl(method=&quot;repeatedcv&quot;, number=3, repeats=3, savePredictions=TRUE, classProbs=TRUE) # Ensemble the predictions of `models` to form a new combined prediction based on glm stack.glm &lt;- caretStack(models, method=&quot;glm&quot;, metric=&quot;Accuracy&quot;, trControl=stackControl) # /!\\ The ensembles tend to perform better if the predictions are less correlated with each other. # Predict on testData stack_predicteds &lt;- predict(stack.glm, newdata=testData4) 12.5 Parallel Processing # library(doParallel) # cl &lt;- detectCores() # cl &lt;- makePSOCKcluster(3) # registerDoParallel(cl) # # ## All subsequent models are then run in parallel # model &lt;- train(Class ~ ., data = training, method = &quot;rf&quot;) # # ## When you are done: # stopCluster(cl) # registerDoSEQ() 12.6 Subsampling for class imbalances Examples of sampling methods : down-sampling: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class (downSample option) up-sampling: randomly sample (with replacement) the minority class to be the same size as the majority class (upSample option) hybrid methods: techniques such as SMOTE and ROSE down-sample the majority class and synthesize new data points in the minority class. There are two packages (DMwR and ROSE) that implement these procedures In practice, one could take the training set and, before model fitting, sample the data. During model may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance. The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below. Deux aporoche - Use sampling before model crossvalidation - use sampling in the model crossvalidation - Repeating the subsampling procedures for every resample produces results that are more consistent with the test set. 12.7 Variables importance Model Specific Metrics Linear Models: the absolute value of the t-statistic Random Forest .. Model Independent Metrics the importance of each predictor is evaluated individually using a “filter” approach. The function automatically scales the importance scores to be between 0 and 100. Using scale = FALSE avoids this normalization step. Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve. # library(gbm) # gbmImp &lt;- varImp(gbmFit3, scale = FALSE) # gbmImp # # roc_imp &lt;- filterVarImp(x = training[, -ncol(training)], y = training$Class) # head(roc_imp) # # roc_imp2 &lt;- varImp(svmFit, scale = FALSE) # roc_imp2 # # plot(gbmImp, top = 20) 12.8 measurung performance Measure for Regression postResample() function : estimate RMSE, MAE Measure for predicted classes confusionMatrix() function : compute a cross-tabulation of the observed and predicted classes. IF Generating the predicted classes based on 50% cutoff for the probabilities. this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument. If there are three or more classes, confusionMatrix will show the confusion matrix and a set of “one-versus-all” results add png : http://topepo.github.io/caret/measuring-performance.html Measure for class probabilities twoClassSummary() function computes the area under the ROC curve and the specificity and sensitivity under the 50% cutoff this function uses the first class level to define the “event” of interest. To change this, use the lev option to the function. there must be columns in the data for each of the class probabilities (named the same as the outcome’s class levels) For multi-class problems mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities multiClassSummary() : computes a number of relevant metrics: Lift Curves The function requires a set of sample probability predictions and the true class labels # Regression data(BostonHousing) bh_index &lt;- createDataPartition(BostonHousing$medv, p = .75, list = FALSE) bh_tr &lt;- BostonHousing[ bh_index, ] bh_te &lt;- BostonHousing[-bh_index, ] lm_fit &lt;- train(medv ~ . + rm:lstat, data = bh_tr, method = &quot;lm&quot;) bh_pred &lt;- predict(lm_fit, bh_te) lm_fit ## Linear Regression ## ## 381 samples ## 13 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 381, 381, 381, 381, 381, 381, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 4.714064 0.7462923 3.216896 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE postResample(pred = bh_pred, obs = bh_te$medv) ## RMSE Rsquared MAE ## 3.3278532 0.8676174 2.5663551 # classification ## create dataset true_class &lt;- factor(sample(paste0(&quot;Class&quot;, 1:2), size = 1000, prob = c(.2, .8), replace = TRUE)) true_class &lt;- sort(true_class) class1_probs &lt;- rbeta(sum(true_class == &quot;Class1&quot;), 4, 1) class2_probs &lt;- rbeta(sum(true_class == &quot;Class2&quot;), 1, 2.5) test_set &lt;- data.frame(obs = true_class, Class1 = c(class1_probs, class2_probs)) test_set$Class2 &lt;- 1 - test_set$Class1 test_set$pred &lt;- factor(ifelse(test_set$Class1 &gt;= .5, &quot;Class1&quot;, &quot;Class2&quot;)) ggplot(test_set, aes(x = Class1)) + geom_histogram(binwidth = .05) + facet_wrap(~obs) + xlab(&quot;Probability of Class #1&quot;) confusionMatrix(data = test_set$pred, reference = test_set$obs) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Class1 Class2 ## Class1 184 142 ## Class2 11 663 ## ## Accuracy : 0.847 ## 95% CI : (0.8232, 0.8688) ## No Information Rate : 0.805 ## P-Value [Acc &gt; NIR] : 0.0003334 ## ## Kappa : 0.6115 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9436 ## Specificity : 0.8236 ## Pos Pred Value : 0.5644 ## Neg Pred Value : 0.9837 ## Prevalence : 0.1950 ## Detection Rate : 0.1840 ## Detection Prevalence : 0.3260 ## Balanced Accuracy : 0.8836 ## ## &#39;Positive&#39; Class : Class1 ## twoClassSummary(test_set, lev = levels(test_set$obs)) ## ROC Sens Spec ## 0.9542857 0.9435897 0.8236025 prSummary(test_set, lev = levels(test_set$obs)) ## AUC Precision Recall F ## 0.8430141 0.5644172 0.9435897 0.7063340 mnLogLoss(test_set, lev = levels(test_set$obs)) ## logLoss ## 0.369638 # lift curves lift_training &lt;- twoClassSim(1000) lift_testing &lt;- twoClassSim(1000) ctrl &lt;- trainControl(method = &quot;cv&quot;, classProbs = TRUE, summaryFunction = twoClassSummary) fda_lift &lt;- train(Class ~ ., data = lift_training, method = &quot;fda&quot;, metric = &quot;ROC&quot;, tuneLength = 20, trControl = ctrl) lda_lift &lt;- train(Class ~ ., data = lift_training, method = &quot;lda&quot;, metric = &quot;ROC&quot;, trControl = ctrl) # c5_lift &lt;- train(Class ~ ., data = lift_training, # method = &quot;C5.0&quot;, metric = &quot;ROC&quot;, # tuneLength = 10, # trControl = ctrl, # control = C5.0Control(earlyStopping = FALSE)) ## Generate the test set results lift_results &lt;- data.frame(Class = lift_testing$Class) lift_results$FDA &lt;- predict(fda_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] lift_results$LDA &lt;- predict(lda_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] #lift_results$C5.0 &lt;- predict(c5_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] head(lift_results) ## Class FDA LDA ## 1 Class2 0.007109969 0.02113239 ## 2 Class2 0.388105122 0.05378358 ## 3 Class1 0.483342537 0.36243116 ## 4 Class2 0.206283858 0.10469846 ## 5 Class1 0.973166737 0.92394495 ## 6 Class1 0.640375854 0.59014966 trellis.par.set(caretTheme()) lift_obj &lt;- lift(Class ~ FDA + LDA , data = lift_results) ggplot(lift_obj, values = 60) 12.9 feature selection 12.9.1 Overview Wrapper evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. caret has wrapper methods based on recursive feature elimination, genetic algorithms, and simulated annealing. Filter evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. Only predictors with important relationships would then be included in a classification model 12.9.2 Univariate approach 12.9.3 recursive feature elimination rfe function x : matrix of predictor variables y : a vector of outcomes sizes : specifie the subset sizes that should be tested rfeControl : a list of options that can be used There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs), random forests (rfFuncs), naive Bayes (nbFuncs), bagged trees (treebagFuncs) and functions that can be used with caret’s train function (caretFuncs). RFE works in 3 broad steps: Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset. Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subgroups of most important predictors determined from step 1. Ranking of the predictors is recalculated in each iteration. Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors. 12.9.4 genetic algorimth 12.9.5 simulated annealing "]
]
