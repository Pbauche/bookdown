[
["index.html", "Handout_V2 Chapter 1 Information 1.1 Usefull ressource 1.2 todo 1.3 interesting stuff 1.4 Hint", " Handout_V2 Pierre Bauche 2018-09-04 Chapter 1 Information 1.1 Usefull ressource Feature engenering : http://www.feat.engineering/review-predictive-modeling-process.html Semi supervized learning : https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/ - Machine Learning wth unlabbeled data : psuedo labeling Book Artificial Intelligence: A Modern Approach&quot; by Stuart Russell and Peter Norvig. 1.2 todo create Blogdown tydiverse package Tensorflow for deep learning : open source software library developed at google for complex computation 1.3 interesting stuff polynomial regression using kernel smoothing Logistic regression using 5 fold stratified cross validation blockchain data science reinforcement learning adversarial training -Deep Learning microsoft service Tensorflow : read exemple RKWard : free and open source Graphical User Interface for the R software ML with H2O, lime, Keras spark R image recognizing with R sentiment analysis = microsoft azure data privacy data quality control Web developpement Shiny : (microsoft azure serveur pour upload Rmarckodw : blogdown (Hugo) Insurance fraud underwriting models : predict somebody’s insurance risk marketing predition customer segmentation 1.4 Hint "],
["feature-engeneering.html", "Chapter 2 Feature engeneering 2.1 Input feature 2.2 Missing Value 2.3 Outlier Detection 2.4 Sampling and resampling 2.5 variables selections 2.6 Example 2.7 Method Summary 2.8 tips", " Chapter 2 Feature engeneering 2.1 Input feature A feature is a numeric representation of raw data. Feature engineering is the process of formulating the most appropriate features given the data, the model, and the task. If features are’t good enought then model canot be good. Features selection is important. If they are to many fearture, model use noise or irrelevant information or redundant. IF they are not enought feature, model don’t have the information . Create new input : Combine feature reduire dimention reduire colinéarité predictor qui ont du sens use the input interaction kmeans clustering as feature : attetion pas inclure la target risque overfitting a data point can also be represented by a dense vector of its inverse distance to each cluster center. This retains more information than simple binary cluster assignment n-day average (in time series) : peut reduire la variabilite etle noise ratio Tips Use knowledge to construct a better set of features (business) Visualizing the correlation and check de relation between input and output when output is numeric between different input Normalize the feature if metrics differt or unknow 2.1.1 Numeric Data Predictors that are on a continuous scale are subject to somes issues that can be mitigated through the choose of model. Models that are smooth functions of input features or model hat use euclidian distance (regression, clustering, …) are sensitive to the scale. Models based on space-partitioning trees (decision trees, gradient boosted machines, random forests) are not sensitive to scale. There are a variety of modifications that can be made to an individual predictor that might improve its utility in a model. scaling : not change the shape of the distribution \\(/frac{x-min(x)}{max(x)-min(x)}\\) Feature scaling is useful in situations where a set of input features differs wildly in scale. standardization on N(0,1) : \\(/frac{x-mean(x)}{sqrt(var())}\\) essential when the distance or dot products between predictors are used (such as K-nearest neighbors or support vector machines) essential when the variables are required to be a a common scale in order to apply a penalty (e.g. the lasso or ridge regression) normalisation : divide by the euclienne l² norme (=sums the squares of the values of the features across data points). SO the feature column has norm = 1 Discretization : fixed width quantile binning Variables scaled and standardized are comparable Some models need gaussian input : scale + transform Power transforms : variance-stabilizing transformations** Power transforms change the distribution of the variable to more symetric distribution log sqrt inverse boxcox : generalisation : Only work for positive variable johnson transform logit transformations : This transformation changes the scale from zero and one to values between negative and positive infinity 2.1.2 count data Raw counts that span several orders of magnitude are problematic for many models.In a linear model, the same linear coefficient would have to work for all possible values of the count. Large counts could also wreak havoc in unsupervised learning methods such as k-means clustering, which uses Euclidean distance as a similarity function to measure the similarity between data points. A large count in one element of the data vector would outweigh the similarity in all other elements, which could throw off the entire similarity measurement. count transform binarise 0/1 if value quantizing the count or group the counts fixed-width binning, each bin contains a specific numeric range (ex age) If count have multiple magnitudes, group by powers of 10 ( 0–9, 10–99, 100–999, 1000–9999, etc) Quantile binning : adaptively positioning the bins based on the distribution of the data log transform 2.1.3 categorical data Use Dummy or keep factors with somes levels is same for most modeling. It suggest using the predictors without converting to dummy variables and, if the model appears promising, to also try refitting using dummy variables. unordered categorical data dummy coding : in Feature engineering, il recommande de flag chaque variable categorielle en varible binaire effect coding : -1 0 1 : -1 si different de categorie de reference. Effect coding is very similar to dummy coding, but results in linear regression models that are even simpler to interpret. Dealing with Large Categorical Variables do nothing dummy : create many variable with zero value for rare categories and add zero-variance predictor( computentional intencive ) delete rare value recode and regroup categorical data Compress the features. There are two choices: Feature hashing, popular with linear models. A hash function is a deterministic function that maps a potentially unbounded integer to a finite integer range [1, m]. Feature hashing compresses the original feature vector into an m-dimensional vector. It Converte large cat var into small hash feature (but hashing feature are uninterpretable) Bin counting, popular with linear models as well as trees. Rather than using the value of the categorical variable as the feature, use the conditional probability of the target under that value. In other words, instead of encoding the identity of the categorical value, we compute the association statistics between that value and the target that we wish to predict Ordered data how measure de force to pass between each categorie ? linear quadratic 2.1.4 Date Time : Lubridate package Use as.POSIXct() and UTC (universal coordinated time)in time zone. create new variables : weekend (0/1), bankholiday (0/1), … 2.2 Missing Value Do nothing remove impute by mean : doesn’t impact analysis by singular value decomposition : approximate true value by regression :approximate true value Check lien 5 methode impute missing value 2.3 Outlier Detection 2.4 Sampling and resampling Modern statistical methods assume that the underlying data comes from a random distribution. The performance measurements of models derived from data are also subject to random noise. the sample can be generalized for the population with statistical confidence. Is an approximatation. Weak law of large numbers : \\(\\bar{X_n} =&gt; \\mu\\) Central limit theorem : distribution standardis? tend vers une normale asymptotiquement model sampling : population data is already collected and you want to reduce time and the computational cost of analysis, along with improve the inference of your models survey sampling : create a sample design and then survey the population only to collect sample to save data collection costs. Type of sampling methods : Boostrap sampling : sampling with replacement Jackknife = leave one out sampling + calculate average of the estimation Vfold crossvalidation : Resampling methods that can generate V different versions of the training set (same size) that can be used to evaluate model on test set. Each of the V assessment sets contains 1/V of the training set and each of these exclude different data points. Suppose V = 10, then there are 10 different versions of 90% of the data and also 10 versions of the remaining 10% for each corresponding resample. in the end, there are V estimates of performance for the model and each was calculated on a different assessment set. The cross-validation estimate of performance is computed by averaging the V individual metrics. Monte Carlo : Produces splits that are likely to contain overlap. For each resample, a random sample is taken with π proportion of the training set going into the analysis set and the remaining samples allocated to the assessment set bootstrap : A bootstrap resample of the data is defined to be a simple random sample that is the same size as the training set where the data are sampled with replacement 2.5 variables selections How do we cleanly separate the signal from the noise? First Filter Na filter : column with to many NA Variance filter : Column with not enought variance to explain dataset corrélation filter : e will remove predictors that are highly correlated (r2 &gt; 0.9) with other predictors. see corrplot Variance treshold : Variable with high variability also have higher information in them. We remove all variables havant variance less than a treshold. 2.5.1 Filter methods : Select variables sans modélisation. Methode univariée. Order feature by importance. Methode robust contre overfitting mais peut selectionner variables redondantes. It is best to do prefiltering conservatively, so as not to inadvertently eliminate useful features before they even make it to the model training step Chi square test Correlation coefficients information gain metrics fisher score variance treshold 2.5.2 Wrapper Methods: Test differentes combinaisons de feature selon crit?re de performance. Predictive model is used to evaluate the set of feature by accurancy metric. Méthode efficace pour la mod?lisation. Peut causé de l’overfitting. forward/backward selection recursive feature elimation algorithm … see supervised analysis 2.5.3 Embedded Methods : Next step to wrapper methods. Introduce a penalty factor to the evaluation criteria of the model to bias the model toward lower complexity. Balance between complexity and accurancy. Less computationally expensive than Wrapper. Less prone to overfitting. These methods perform feature selection as part of the model training process Lasso Ridge regression … Decision tree Gradiant descent methods 2.5.4 Dimension reduction : See unsuppervized section PCA see unsupervised analysis : Due to the orthogonality constraint in the objective function, PCA transformation produces a nice side effect: the transformed features are no longer correlated. svd k-means as a featurization procedure, a data point can be represented by its cluster membership 2.6 Example 2.6.1 Credit risk modeling Feature ranking Fit logistic model Calculate Gini coefficient rearrange variables ? combine, weighted sums, etc Need to understand variable individually ? use Filtering method data dirty ? detect outlier Data selection? use first ranking, forward selection and last Embedded method. Compare with crit?rion (misclassi, MSE, AIC, etc) improve performance? bootstrap : subsample your data et redo analysis ### Data Prep ### ################# library(MLmetrics) data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) #Create the default variable data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) print(table(data$default)*100/nrow(data)) ## ## 0 1 ## 90.635 9.365 # Without prior kwowledge : if more than 30 variable is continuous continuous &lt;-character() categorical &lt;-character() i = names(data)[1] p&lt;-1 q&lt;-1 for (i in names(data)){ unique_levels =length(unique(data[,i])) if(i %in% c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)){ next; }else if (unique_levels &lt;=30 |is.character(data[,i])){ categorical[p] &lt;-i p=p+1 data[[i]] &lt;-factor(data[[i]]) }else{ continuous[q] &lt;-i q=q+1 }} cat(&quot;\\nTotal number of continuous variables in feature set &quot;,length(continuous) -1) ## ## Total number of continuous variables in feature set 714 cat(&quot;\\nTotal number of categorical variable in feature set &quot;,length(categorical) -2) ## ## Total number of categorical variable in feature set 52 # Gini coef performance_metric_gini &lt;-data.frame(feature =character(), Gini_value =numeric()) # for (feature in names(data)){ # if(feature %in%c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)) { # next # } else { # tryCatch( # {glm_model &lt;-glm(default ~get(feature),data=data,family=binomial(link=&quot;logit&quot;)); # predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;); # Gini_value &lt;-Gini(predicted_values,data$default); # performance_metric_gini &lt;-rbind(performance_metric_gini,cbind(feature,Gini_value));},error=function(e){}) # } # } # # saveRDS(performance_metric_gini, &quot;performance_metric_gini.rds&quot;) performance_metric_gini &lt;- readRDS(&quot;./save/performance_metric_gini.rds&quot;) performance_metric_gini$Gini_value &lt;-as.numeric(as.character(performance_metric_gini$Gini_value)) Ranked_Features &lt;-performance_metric_gini[order(-performance_metric_gini$Gini_value),] head(Ranked_Features) ## feature Gini_value ## 389 f404 0.2579189 ## 710 f766 0.2578312 ## 585 f630 0.2415352 ## 584 f629 0.2354368 ## 321 f333 0.2352707 ## 56 f64 0.2348747 # Note : When you are running loops over large datasets, it is possible that the loop might stop due to some errors. to escape that, consider using the trycatch() function in r ################################################### ### Try logistic regression with top 5 features ### ################################################### glm_model &lt;-glm(default ~f766 +f404 +f629 +f630 +f281 +f322,data=data,family=binomial(link=&quot;logit&quot;)) predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;) Gini_value &lt;-Gini(predicted_values,data$default) summary(glm_model) ## ## Call: ## glm(formula = default ~ f766 + f404 + f629 + f630 + f281 + f322, ## family = binomial(link = &quot;logit&quot;), data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6928 -0.4946 -0.4102 -0.3329 3.0013 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.502550 4.939282 -0.304 0.7610 ## f766 -0.010228 4.916519 -0.002 0.9983 ## f404 -1.395602 4.908606 -0.284 0.7762 ## f629 -0.306456 0.172632 -1.775 0.0759 . ## f630 -0.165047 0.128300 -1.286 0.1983 ## f281 0.007759 0.019386 0.400 0.6890 ## f322 0.264196 0.128472 2.056 0.0397 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 12415 on 19938 degrees of freedom ## Residual deviance: 12040 on 19932 degrees of freedom ## (61 observations deleted due to missingness) ## AIC: 12054 ## ## Number of Fisher Scoring iterations: 5 Gini_value ## [1] 0.2697868 # Every features aren&#39;t always significant. Indication that features themselves are highly correlated. Gini coef has not improved. So investigate multicorrelation. # Variable ranking method is univariate and lead to the selection of a redundant variables. top_6_feature &lt;-data.frame(data$f766,data$f404,data$f629,data$f630,data$f281,data$f322) cor(top_6_feature, use=&quot;complete&quot;) ## data.f766 data.f404 data.f629 data.f630 data.f281 ## data.f766 1.0000000 0.9996754 0.6777553 0.6378040 0.8205665 ## data.f404 0.9996754 1.0000000 0.6774434 0.6374457 0.8204153 ## data.f629 0.6777553 0.6774434 1.0000000 0.9155376 0.6628148 ## data.f630 0.6378040 0.6374457 0.9155376 1.0000000 0.6202698 ## data.f281 0.8205665 0.8204153 0.6628148 0.6202698 1.0000000 ## data.f322 -0.7706228 -0.7707861 -0.5450001 -0.5048133 -0.7371242 ## data.f322 ## data.f766 -0.7706228 ## data.f404 -0.7707861 ## data.f629 -0.5450001 ## data.f630 -0.5048133 ## data.f281 -0.7371242 ## data.f322 1.0000000 2.6.2 variance treshold approach # Attention, les variables ne sont pas standardisées, on ne peut pas les comparer directement. On utilise le coeficient de variation :$c= \\fraq{\\sigma}{\\mu}$ # Calculate CV coefficient_of_variance &lt;-data.frame(feature =character(), cov =numeric()) for (feature in names(data)){ if(feature %in%c(&quot;id&quot;,&quot;loss&quot;,&quot;default&quot;)){next }else if(feature %in% continuous){ tryCatch({ cov &lt;-abs(sd(data[[feature]], na.rm =TRUE)/mean(data[[feature]],na.rm =TRUE)); if(cov !=Inf){ coefficient_of_variance &lt;-rbind(coefficient_of_variance,cbind(feature, cov)); } else {next} },error=function(e){}) }else{next} } coefficient_of_variance$cov &lt;-as.numeric(as.character(coefficient_of_variance$cov)) Ranked_Features_cov &lt;-coefficient_of_variance[order(-coefficient_of_variance$cov),] head(Ranked_Features_cov) ## feature cov ## 294 f338 128.05980 ## 377 f422 111.93083 ## 664 f724 69.64913 ## 349 f393 55.39446 ## 712 f775 47.64456 ## 350 f394 46.68719 ## Logistic model glm_model &lt;-glm(default ~f338 +f422 +f724 +f636 +f775 +f723,data=data, family=binomial(link=&quot;logit&quot;)); predicted_values &lt;-predict.glm(glm_model,newdata=data,type=&quot;response&quot;) Gini_value &lt;-Gini(predicted_values,data$default) cat(&quot;The Gini Coefficient for the fitted model is &quot;,Gini_value); ## The Gini Coefficient for the fitted model is 0.1465253 Contrairement au Ranking avec Gini, les variables ne sont pas dominés par leur structure de correlation. Mais les variables ne sont pas toutes significatives individuellement et le coef GINI pas particuliérement amélioré. Avec variance treshlod on espére selectionné des variables indépendantes 2.7 Method Summary Variable quanti Variable quali Graph Time series, barplot, boxplot, histographe, QQplot, scaterplot barplot, boxplot Test t-test sur la moyenne, chi2 sur la variance, test normalité, corrélation, test F variance, test de levene test proportion, test ajustement, test indépendance Modélisation Régression linéaire régression logistique, analyse discriminante, abre décision Parametric : assume thaht sample data is drawn from a known probabilité distribution based on fixed set of parameters. For instance, linear regression assumes normal distribution, whereas logistic assumes binomial distribution, etc. This assumption allows the methods to be applied to small datasets as well. involve a two-step model-based approach : Chose model (ex : linear) and estimate (ex: ols) reduce the probleme of model estimation to a probleme of parameter estimation but if the chosen model is too far from the true f, then the estimate will be poor Non parametric : not assume any probabilty distribution or prior. Contruct empirical distributions from data. (= Kernel regression, NPMR) Models can also be evaluated in terms of variance and bias. A model has high variance if small changes to the underlying data used to estimate the parameters cause a sizable change in those parameters (or in the structure of the model) Model bias reflects the ability of a model to conform to the underlying theoretical structure of the data. A low bias model is one that can be highly flexible and has the capacity to fit a variety of different shapes and patterns. A high bias model would be unable to estimate values close to their true theoretical counterparts. Linear methods often have high bias since, without modification, cannot describe nonlinear patterns in the predictor variables. Tree-based models, support vector machines, neural networks, and others can be very adaptable to the data and have low bias. 2.8 tips Tidyverse package Given below are some of the rare feature engineering tricks implemented in the winning solutions of several data science competitions. - Transform data to Image - Meta-leaks - Representation learning features Mean encodings - Transforming target variable "],
["data-visualization.html", "Chapter 3 Data visualization 3.1 Descriptive 3.2 Spacial map", " Chapter 3 Data visualization 3.1 Descriptive #change theme =&gt; + theme() iris %&gt;% qplot(Petal.Width, Petal.Length , color = Species, data = .) cars %&gt;% ggplot(aes(x = speed, y = ..count..)) + geom_histogram(bins = 10) + geom_density() cars %&gt;% ggplot(aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = &quot;lm&quot;) # If non linear smooth : method = &#39;loess&#39; Multiple line longley %&gt;% ggplot(aes(x = Year)) + geom_point(aes(y = Unemployed)) + geom_point(aes(y = Armed.Forces), color = &quot;blue&quot;) + geom_line(aes(y = Unemployed)) + geom_line(aes(y = Armed.Forces), color = &quot;blue&quot;) Scaling cars %&gt;% ggplot(aes(x = speed, y = dist)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_x_reverse(&quot;Speed&quot;) + scale_y_continuous(&quot;Stopping Distance&quot;) iris %&gt;% ggplot(aes(x = Species, y = Petal.Length)) + geom_boxplot() + geom_jitter(width = 0.1, height = 0.1) + scale_x_discrete(labels = c(&quot;setosa&quot; = &quot;Setosa&quot;, &quot;versicolor&quot; = &quot;Versicolor&quot;, &quot;virginica&quot; = &quot;Virginica&quot;)) Correlation plot Pearson correlation : relation lin?aire \\[\\rho(X,Y)=\\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y} \\] library(corrplot) correlation_world &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 4/Correlation/Correlation Data.csv&quot;) corrplot(cor(correlation_world[,2:6],method =&quot;pearson&quot;),diag =FALSE, title =&quot;Correlation Plot&quot;, method =&quot;ellipse&quot;, tl.cex =0.7, tl.col =&quot;black&quot;, cl.ratio =0.2 ) 3.2 Spacial map Static map qmap from ggplot Esay way : Machine learning with R, chap4 Interactive map : leaflet More information on https://rstudio.github.io/leaflet for more complex cartographie, check geoJSON https://rstudio.github.io/leaflet/json.html gps geolocalisation : https://github.com/AugustT/shiny_geolocation library(leaflet) lat = seq(50, 51 ,by= 0.005) lon = seq(4,5, by=0.005) coords &lt;- as.data.frame(cbind(Longitude = sample(lon,50), Latitude = sample(lat,50))) coords$V3 = as.factor(rep(&quot;Amandine&quot;,50)) coords$V4 = rep(seq(1,5,by=1),10) # simple use # Possibilité d&#39;utilise d&#39;autre map que google open street map m &lt;- leaflet() %&gt;% setView(lng = 4.8, lat = 50.5, zoom = 10) m %&gt;% addProviderTiles(providers$Stamen.Toner) m %&gt;% addProviderTiles(providers$Esri.NatGeoWorldMap) # cartographe library(maps) mapStates = map(&quot;state&quot;, fill = TRUE, plot = FALSE) leaflet(data = mapStates) %&gt;% addTiles() %&gt;% addPolygons(fillColor = topo.colors(10, alpha = NULL), stroke = FALSE) # modifier les markeret pop up leaflet(coords) %&gt;% addTiles() %&gt;% addMarkers(clusterOptions = markerClusterOptions(), popup = coords$V4) leaflet(coords) %&gt;% addTiles() %&gt;% addCircles(lng = ~Longitude, lat = ~Latitude, weight = 1, radius = ~V4^2 * 30, popup = ~V3 ) # rectangle zone leaflet() %&gt;% addTiles() %&gt;% addRectangles( lng1=-118.456554, lat1=34.078039, lng2=-118.436383, lat2=34.062717, fillColor = &quot;transparent&quot; ) "],
["regression.html", "Chapter 4 Regression 4.1 introduction 4.2 Linear regression 4.3 ANOVA 4.4 Polynomiale regression 4.5 Logistique 4.6 Generalized Linear Models 4.7 Model Selection 4.8 Regularization Algorithms 4.9 Locally estimated Scaterplot Smoothing (LOESS)", " Chapter 4 Regression load(&quot;./save/BreastCancer.Rdata&quot;) #import Data_Purchase_Prediction &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/PurchasePredictionDataset.csv&quot;,header=TRUE) library(IDPmisc) library(Metrics) library(MASS) library(&quot;lattice&quot;) library(car) 4.1 introduction First supervized learning. Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y. Dependante variable is available and regression use others predictives variables to estimate regression coefficient. 4.2 Linear regression Model : \\(Y=\\alpha + X \\beta + \\epsilon\\) Linéaire: on suppose distribition normal \\(\\alpha\\) :intercepte : la reponse moyenne si les variables explicatives sont zéro Remarque Categorical data : set to as factor Check Missing value : delete, impute, new catégorie Hypothèses : \\(rang(X) = p\\) =&gt; Rang est connu, exclus la multicolinéarité X est une matrice déterminée \\(\\epsilon\\) sont des erreurs indépendantes \\(E(\\epsilon) = 0\\) =&gt; erreur de moyenne nulle (normalité des résidus) \\(var(\\epsilon) = \\sigma_2 In\\) =&gt; variance Homoskédastique non autocorrélé Estimation et propriétés des estimateurs : Estimation par moindres carrés ordinaires : Minimise les squares error. Estimateur le plus efficace dans la classe des estimateurs non biaisé :BLUE \\(E[Y] = X \\beta\\) \\(Var(Y) = \\sigma In\\) \\(E[\\hat(\\beta)] = \\beta\\) \\(var(\\hat(\\beta)) = \\sigma (X&#39;X)^(-1)\\) Si \\(\\epsilon ~ N(0, \\sigma In)\\), alors \\(\\hat(\\beta) ~ N(\\beta, \\sigma^2 (X&#39;X)^(-1))\\) \\[SSTO = SSR + SSE\\] \\[\\sum{(Y_i - \\bar{Y})^2} = \\sum{(\\hat{Y}_i - \\bar{Y})^2} + \\sum{(Y_i - \\hat{Y})^2}\\] Diagnostiques : F-test : \\(H_0 : \\beta_i = 0 \\forall i\\) stat de test : \\(\\frac{(SSTO - SSE)/(p-1)}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F(p-1,n-1)\\) coefficient de détermination multiple \\(R^2\\) : mesure de qualité d’ajustement \\(\\frac{SSR/SSTO}\\) Multicolinéarité : forte corrélation entre variables explicatives Conséquence : Interprétation des coéfficients impossibles Diagnostiques : variance des coefficients très larges, coefficients varient beaucoup a l’ajouts/retrait de variables, coefficients ont signes non intruitifs Calcule des VIF (variance inflation factor) : si mpoyenne des VIF &gt; 1 ou un VIF &gt;10) \\(tolérance = 1-R²\\) et \\(VIF = \\frac{1}{tolérance}\\) Solution : Supprimer des variables, regression de Ridge (permet l’inversion de la matrice X’X qui est impossible en cas de multicolinéarité parfaite) Linéarité : Graph des résidus Vs régresseurs Si forme connue : transformer les regressieurs (log, sqrt) ou ajouté un terme (quadratique, log, d’interaction, …) Homoskédasticité : graph résidus vs valeurs prédites, test de Breush et Pagan, BreushPAgan, Berlett test, arch test Variance des erreurs indépendante des variable explicative Estimation reste correcte sous homoskédasticité : utilisé une variance corrigé : Régression de white Erreur Non indépendante : test d’autocorrélation Dubin watson test, plot acf If résidual show definite relationship with prior résidual (like autocorrelation), the noise isn’t random and we still have some information that we can extract and put in the model Problème de modèle : passer en log lin, oubli de régresseur (qui est autocorrélé), inclure des lag de la variable dépendante Normalité des erreurs : QQplot, test de Jarque Berra, KS test estimation correcte mais interprétation des tests et des IC sont faussées car basé sur la normalité théorie des grand nombre, si assez observations, estimateur OLS est assymptiquement normal et les test et IC tendent assymptotiquement Influential Point Analysis: Les valeurs abérantes peuvent crée des biais dans les estimateurs. Si trop extreme, on peut les deletes, check, impute, … DFFITS DFBETAS Distance de Cooks : \\[ D_i = \\frac{e²_i}{s²p} [\\frac{h_i}{(1-h_i)²}]\\] where \\(s²= (n-p)^{-1}e^Te\\) est la moyenne des erreurs quadratiques de la regression. Et \\(h_i =x^T(x^Tx)^{-1}\\). Avec cutoff \\(D_i &gt; 4/(n-k-1)\\) ou k est le nombre de paramètre Distance de Cook mesure l’effet of deleting a given observation. Si supprimer des observations cause grosse influence, alors ce point est suppiser etre outlier. Evaluation : RMSE = sqrt(mean($residuals)^2) ou $residuals = actual-predicted Interprétation : Pour une augmentation de une unité de speed, dist augmente de 3.9324. Intercepte donne la dist si speed vaut zero reglin = lm(dist~ speed, data=cars) summary(reglin) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 y = cars$dist x = cars$speed res &lt;-stack(data.frame(Observed = y, Predicted=fitted(reglin))) res &lt;-cbind(res, x =rep(x, 2)) #Plot using lattice xyplot(function) library(&quot;lattice&quot;) xyplot(values ~x, data = res, group = ind, auto.key =TRUE) sqrt(mean(residuals(reglin)^2)) ## [1] 15.06886 rmse(cars$dist,predict(reglin)) ## [1] 15.06886 # Normalité des résidus sresid = studres(reglin) sresid=NaRV.omit(sresid) hist(sresid, freq=FALSE, main=&quot;Distribution of Studentized Residuals&quot;,breaks=25) xfit&lt;-seq(min(sresid),max(sresid),length=40) yfit&lt;-dnorm(xfit) lines(xfit, yfit) ## ADD QQplot ## test normalité (attention juste indicateur) ks.test(reglin$residuals,pnorm,alternative=&quot;two.sided&quot;) ## ## One-sample Kolmogorov-Smirnov test ## ## data: reglin$residuals ## D = 0.49833, p-value = 3.283e-11 ## alternative hypothesis: two-sided shapiro.test(reglin$residuals) ## ## Shapiro-Wilk normality test ## ## data: reglin$residuals ## W = 0.94509, p-value = 0.02152 # Multicolinnéarité : VIF # vif(reglin) # residual autocorrelation : H0 = pas d&#39;autocorrélation durbinWatsonTest(reglin) ## lag Autocorrelation D-W Statistic p-value ## 1 0.1604322 1.676225 0.19 ## Alternative hypothesis: rho != 0 plot(acf(reglin$residuals)) # Homoskédasticité : breush pagan test # h0 : variance hétéscedastic ncvTest(reglin) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 4.650233 Df = 1 p = 0.03104933 #plot resi vs fit : detect non liearité, heterocedasticity, outlier # if random = ok plot(reglin$residuals,reglin$fitted.values) # cook&#39;s distance cutoff &lt;-4/((nrow(cars)-length(reglin$coefficients)-1)) plot(reglin, which=4, cook.levels=cutoff) # taille du cercle proportionnel a la distance de cook influencePlot(reglin, id.method=&quot;identify&quot;,main=&quot;Influence Plot&quot;, sub=&quot;Circle size is proportional to Cook&#39;s Distance&quot;, id.location=NULL) outlierTest(reglin) ## ## No Studentized residuals with Bonferonni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferonni p ## 49 3.184993 0.0025707 0.12853 # now investigate vs mean of data variable 4.3 ANOVA 4.4 Polynomiale regression Si la relation entre variables explicatives et variable dépendante n’est pas linéaire. Possibilité d’augmenter la relation dans des haut degré polynomials mais will cause overfitting. \\[ y_i = \\alpha_0 + \\alpha_i x_i + \\alpha_2 x²_i+ ... + \\epsilon_i\\] Exemple : Dependant variable = price of a commodity Explicative variable = quantiée vendue The general principle is if the price is too cheap, people will not buy the commodity thinking it’s not of good quality, but if the price is too high, people will not buy due to cost consideration. Let’s try to quantify this relationship using linear and quadratic regression y &lt;-as.numeric(c(&quot;3.3&quot;,&quot;2.8&quot;,&quot;2.9&quot;,&quot;2.3&quot;,&quot;2.6&quot;,&quot;2.1&quot;,&quot;2.5&quot;,&quot;2.9&quot;,&quot;2.4&quot;,&quot;3.0&quot;,&quot;3.1&quot;,&quot;2.8&quot;,&quot;3.3&quot;,&quot;3.5&quot;,&quot;3&quot;)) x&lt;-as.numeric(c(&quot;50&quot;,&quot;55&quot;,&quot;49&quot;,&quot;68&quot;,&quot;73&quot;,&quot;71&quot;,&quot;80&quot;,&quot;84&quot;,&quot;79&quot;,&quot;92&quot;,&quot;91&quot;,&quot;90&quot;,&quot;110&quot;,&quot;103&quot;,&quot;99&quot;)); linear_reg &lt;-lm(y~x) summary(linear_reg) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.66844 -0.25994 0.03346 0.20895 0.69004 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.232652 0.445995 5.006 0.00024 *** ## x 0.007546 0.005463 1.381 0.19046 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3836 on 13 degrees of freedom ## Multiple R-squared: 0.128, Adjusted R-squared: 0.06091 ## F-statistic: 1.908 on 1 and 13 DF, p-value: 0.1905 plot(y) lines(linear_reg$fitted.values) quad_reg &lt;-lm(y~x +I(x^2) ) summary(quad_reg) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43380 -0.13005 0.00493 0.20701 0.33776 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8737010 1.1648621 5.901 7.24e-05 *** ## x -0.1189525 0.0309061 -3.849 0.00232 ** ## I(x^2) 0.0008145 0.0001976 4.122 0.00142 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2569 on 12 degrees of freedom ## Multiple R-squared: 0.6391, Adjusted R-squared: 0.5789 ## F-statistic: 10.62 on 2 and 12 DF, p-value: 0.002211 plot(y) lines(quad_reg$fitted.values) # improvement in R square, quadratic term significant 4.5 Logistique 4.5.1 General Variable dépendante binaire : binomially distribued binomial distribution probability mass function : \\(f(k;n,p) = P(X=k) = \\left( \\begin{array}{c} n \\\\ k \\end{array} \\right) p^k (1-p)^{n-k}\\) Trois classe de modèle logistiques: binomial logistic regression : var dépendante soit 0 soit 1 multinomial logistic regression : 3 ouplus niveu pour la variable dépendante (on utilise ditribution multinomiale) ordered logistic regression Transformation logit : fonction de lien pour la regression : \\(logit = \\frac{e^t}{e^t+1}=\\frac{1}{1+e^{-t}}\\) LA cote : représente la relation entre presence/absence d’un event odd = P(A)/(1-P(A)) un odd de 2 pour un event A mean l’event est deux fois plus probable qu’il se réalise que rien ne se réalise. Odd Ratio : rapport des cotes = Odd(A) / Odd(B) SI OR = 2 : Chanque que B se réalise sont deux fois suppérieur a celle de A 4.5.2 Binomial Logistic MODEL Model : \\[ logit(p_i) = \\ln(\\frac{p_i}{1-p_i}) = \\beta_0 + \\beta X \\] Hypothèses : Estimation par MLE ou itérative avec optimisation du logLoss Diagnostiques : Si but est classification : check les predictions et classement Si but est analyse des coefficients : vérification des hypothèsese stat Wald test : same a t-test in reg lin. Test sur les levels des variables sont individuellements significatifs. Suit une distri chi-square. pseudo R-square : Mesure la proportion de variance expliqué par le modele. Mesure la différence entre la déviance un model null et fitted. Calcul par le likelihood ratio : \\[R²_i = \\frac{D_{null} - D_{fitted}}{D_{null}}\\] ou D est la déviance : $ D = - 2ln $ Bivariate plot : observed and predictied vs variable explicative. Plot donne info sur comme le model sur comporte selon les différent niveau Matrice de classification : - Spécificity = combien de negatif le model prédit correctement - sensitivity = combien de positif le model prédit correctement library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.3.3 library(mlbench) ## Warning: package &#39;mlbench&#39; was built under R version 3.3.3 BreastCancer$Cl.thickness = as.numeric(as.character(BreastCancer$Cl.thickness)) BreastCancer$IsMalignant = ifelse( BreastCancer$Class== &quot;benign&quot;, 0, 1) ggplot(data =BreastCancer, aes(x = Cl.thickness, y = IsMalignant)) + geom_jitter(height = 0.05, width = 0.3, alpha=0.4) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) reglog = glm(IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;, data = BreastCancer) summary(reglog) ## ## Call: ## glm(formula = IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;, ## data = BreastCancer) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1986 -0.4261 -0.1704 0.1730 2.9118 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.16017 0.37795 -13.65 &lt;2e-16 *** ## Cl.thickness 0.93546 0.07377 12.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 900.53 on 698 degrees of freedom ## Residual deviance: 464.05 on 697 degrees of freedom ## AIC: 468.05 ## ## Number of Fisher Scoring iterations: 6 table(BreastCancer$Class, ifelse(predict(reglog, BreastCancer) &lt; 0.5, 0, 1)) ## ## 0 1 ## benign 453 5 ## malignant 94 147 4.5.3 Multinomial Logistic Regression Variable dépendante a plus de une catégorie et suit une distribution multinomiale. On fait une regression logistic pour chaque classe et combine dans un seul equation sous contrainte que la somme des probabilités vallent 1. Estimation par iterative optimization of the LogLoss function. But : clairement de la classification. Deux méthode possible : Pick de highest probability : classe dans la classe qui a le plus haute probabilité par rapport au autres classe. Méthode soufre de la “Class imbalance probleme” (si les classes sont non equilibré, tendance à toujours assigner dans la plus grande classe) Ratio of probabilities : prendre la ratio des probabilité prédite et la prior distribution and choisir la classe basé sur le plus haut ratio. Cette méthode normalise les probabilité par le ratio du prior pour réduire le biais liéà la distribution du pior Data_Purchase&lt;-na.omit(Data_Purchase_Prediction) rownames(Data_Purchase)&lt;-NULL #Random Sample for easy computation Data_Purchase_Model&lt;-Data_Purchase[sample(nrow(Data_Purchase),10000),] # prior distribution table(Data_Purchase_Model$ProductChoice) ## ## 1 2 3 4 ## 2215 3843 2910 1032 # multinomial model library(nnet) mnl_model &lt;-multinom (ProductChoice ~MembershipPoints +IncomeClass + CustomerPropensity +LastPurchaseDuration +CustomerAge +MartialStatus, data = Data_Purchase) ## # weights: 44 (30 variable) ## initial value 672765.880864 ## iter 10 value 615285.850873 ## iter 20 value 607471.781374 ## iter 30 value 607231.472034 ## final value 604217.503433 ## converged mnl_model ## Call: ## multinom(formula = ProductChoice ~ MembershipPoints + IncomeClass + ## CustomerPropensity + LastPurchaseDuration + CustomerAge + ## MartialStatus, data = Data_Purchase) ## ## Coefficients: ## (Intercept) MembershipPoints IncomeClass CustomerPropensityLow ## 2 0.77137077 -0.02940732 0.00127305 -0.3960318 ## 3 0.01775506 0.03340207 0.03540194 -0.8573716 ## 4 -1.15109893 -0.12366367 0.09016678 -0.6427954 ## CustomerPropensityMedium CustomerPropensityUnknown ## 2 -0.2745419 -0.5715016 ## 3 -0.4038433 -1.1824810 ## 4 -0.4035627 -0.9769569 ## CustomerPropensityVeryHigh LastPurchaseDuration CustomerAge ## 2 0.2553831 0.04117902 0.001638976 ## 3 0.5645137 0.05539173 0.005042405 ## 4 0.5897717 0.07047770 0.009664668 ## MartialStatus ## 2 -0.033879645 ## 3 -0.007461956 ## 4 0.122011042 ## ## Residual Deviance: 1208435 ## AIC: 1208495 # Modele converge en 30itérations. #Predict the probabilities predicted_test &lt;-as.data.frame(predict(mnl_model, newdata = Data_Purchase, type=&quot;probs&quot;)) ## méthode 1 : the prediction based in highest probability test_result &lt;-apply(predicted_test,1,which.max) result &lt;-as.data.frame(cbind(Data_Purchase$ProductChoice,test_result)) colnames(result) &lt;-c(&quot;Actual Class&quot;, &quot;Predicted Class&quot;) table(result$`Actual Class`,result$`Predicted Class`) ## ## 1 2 3 ## 1 302 91952 12365 ## 2 248 150429 38028 ## 3 170 90944 51390 ## 4 27 32645 16798 # bon résultat pour classe 123 mais pour classe 4 pas un seul case de classé. ## Methode 2 : normalisation avec la ditribution du prior prior &lt;-table(Data_Purchase_Model$ProductChoice)/nrow(Data_Purchase_Model) prior_mat &lt;-rep(prior,nrow(Data_Purchase_Model)) pred_ratio &lt;-predicted_test/prior_mat test_result &lt;-apply(pred_ratio,1,which.max) result &lt;-as.data.frame(cbind(Data_Purchase$ProductChoice,test_result)) colnames(result) &lt;-c(&quot;Actual Class&quot;, &quot;Predicted Class&quot;) table(result$`Actual Class`,result$`Predicted Class`) ## ## 1 2 3 4 ## 1 21552 64045 18996 26 ## 2 28459 111965 48210 71 ## 3 14057 76923 51468 56 ## 4 4680 27802 16941 47 4.6 Generalized Linear Models Pour GLM, on suppose que la variable dépendante est issue de la famille de ditribution exponentielle incluant la normal, binomial, poisson, gamma, … etc. \\[ E(Y) = \\mu = g^{-1}(X\\beta) \\] In R : glm(formula, family=familytype(link=linkfunction), data=) - binomial, (link = “logit”) : modele logistique - gaussian, (link= “identity”) : modèle linéaire - Gamma, (link= “inverse”) : analyse de survie (time to failure of a machine in the industry) - poisson, (link = “log”) : How many calls will the call center receive today? 4.7 Model Selection - **Stepwise** : ajoute séquentielement la variables la plus significative. Après chaqeu ajout,le modèle réévalue la significativité des autres variables. Step : Model with 1 best feature, add next variables that maximise the evaluation function, ... Proc?dure tr?s lourde. parfois necessaire d&#39;utiliser FIlter m?thod avant. ### Data prep ### ################# ## Data with best feature from Filter method data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) data_model &lt;-na.omit(data[,c(&quot;id&quot;,&quot;f338&quot;,&quot;f422&quot;,&quot;f724&quot;,&quot;f636&quot;,&quot;f775&quot;,&quot;f222&quot;,&quot;f93&quot;,&quot;f309&quot;,&quot;f303&quot;,&quot;f113&quot;,&quot;default&quot;),]) ### Forward ### ############### full_model &lt;-glm(default ~f338 +f422 +f724 +f636 +f775 +f222 +f93 +f309+f303 +f113,data=data_model,family=binomial(link=&quot;logit&quot;)) null_model &lt;-glm(default ~1 ,data=data_model,family=binomial(link=&quot;logit&quot;)) forwards &lt;-step(null_model,scope=list(lower=formula(null_model),upper=formula(full_model)), direction=&quot;forward&quot;) ## Start: AIC=11175.3 ## default ~ 1 ## ## Df Deviance AIC ## + f422 1 11136 11140 ## + f113 1 11150 11154 ## + f222 1 11150 11154 ## + f775 1 11165 11169 ## + f93 1 11168 11172 ## + f309 1 11171 11175 ## + f303 1 11171 11175 ## &lt;none&gt; 11173 11175 ## + f636 1 11172 11176 ## + f338 1 11173 11177 ## + f724 1 11173 11177 ## ## Step: AIC=11140.24 ## default ~ f422 ## ## Df Deviance AIC ## + f113 1 11113 11119 ## + f222 1 11114 11120 ## + f775 1 11129 11135 ## + f93 1 11131 11137 ## &lt;none&gt; 11136 11140 ## + f303 1 11135 11141 ## + f309 1 11135 11141 ## + f636 1 11135 11141 ## + f338 1 11136 11142 ## + f724 1 11136 11142 ## ## Step: AIC=11118.59 ## default ~ f422 + f113 ## ## Df Deviance AIC ## + f222 1 11096 11104 ## + f775 1 11106 11114 ## &lt;none&gt; 11113 11119 ## + f93 1 11111 11119 ## + f303 1 11112 11120 ## + f636 1 11112 11120 ## + f309 1 11112 11120 ## + f338 1 11112 11120 ## + f724 1 11113 11121 ## ## Step: AIC=11103.78 ## default ~ f422 + f113 + f222 ## ## Df Deviance AIC ## + f775 1 11090 11100 ## &lt;none&gt; 11096 11104 ## + f303 1 11095 11105 ## + f636 1 11095 11105 ## + f309 1 11095 11105 ## + f93 1 11095 11105 ## + f338 1 11096 11106 ## + f724 1 11096 11106 ## ## Step: AIC=11099.57 ## default ~ f422 + f113 + f222 + f775 ## ## Df Deviance AIC ## + f303 1 11087 11099 ## &lt;none&gt; 11090 11100 ## + f309 1 11088 11100 ## + f636 1 11089 11101 ## + f93 1 11089 11101 ## + f338 1 11090 11102 ## + f724 1 11090 11102 ## ## Step: AIC=11098.6 ## default ~ f422 + f113 + f222 + f775 + f303 ## ## Df Deviance AIC ## &lt;none&gt; 11087 11099 ## + f636 1 11086 11100 ## + f93 1 11086 11100 ## + f309 1 11086 11100 ## + f338 1 11086 11100 ## + f724 1 11087 11101 #best model with AIC criteria formula(forwards) ## default ~ f422 + f113 + f222 + f775 + f303 4.8 Regularization Algorithms 4.8.1 Ridge regression 4.8.2 Least Absolute Shrinkage and Selection Opérator LASSO 4.8.3 Elastic Net 4.8.4 Leas-Angle Regression LARS - **Lasso** dd penalty term against the complexity to reduce the degree of overfittingor the variance of the model by adding additional bas. Check formul LASSO Objective function for the penalized logistic regression: $ - [1/N y (_0 + x^T_t ) - (1 + ) ] + lambda[(1-)||||^2_2 ]$ library(&quot;glmnet&quot;) ### Data prep ### ################# data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;)) data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1) data_model &lt;-na.omit(data) y &lt;-as.matrix(data_model$default) # x &lt;-as.matrix(subset(data_model, select=continuous[250:260])) x &lt;-as.matrix(data_model[,250:260]) fit =glmnet(x,y, family=&quot;binomial&quot;) summary(fit) ## Length Class Mode ## a0 52 -none- numeric ## beta 572 dgCMatrix S4 ## df 52 -none- numeric ## dim 2 -none- numeric ## lambda 52 -none- numeric ## dev.ratio 52 -none- numeric ## nulldev 1 -none- numeric ## npasses 1 -none- numeric ## jerr 1 -none- numeric ## offset 1 -none- logical ## classnames 2 -none- character ## call 4 -none- call ## nobs 1 -none- numeric plot (fit, xvar=&quot;dev&quot;, label=TRUE) #Fit a cross validated binomial model fit_logistic =cv.glmnet(x,y, family=&quot;binomial&quot;, type.measure=&quot;class&quot;) plot (fit_logistic) # on est sens? voir un tendance dans les points rouge. on veut le labda qui minimum le taux de mauvaise classifications print(fit_logistic$lambda.min) ## [1] 0.01919422 param &lt;-coef(fit_logistic, s=&quot;lambda.min&quot;) param &lt;-as.data.frame(as.matrix(param)) param$feature&lt;-rownames(param) #The list of variables suggested by the embedded method param_embeded &lt;-param[param[,2]&gt;0,] param_embeded ## 1 feature ## f251 0 f251 ## f252 0 f252 ## f253 0 f253 ## f254 0 f254 ## f255 0 f255 ## f256 0 f256 ## f257 0 f257 ## f258 0 f258 ## f259 0 f259 ## f260 0 f260 ## f261 0 f261 ridge 4.9 Locally estimated Scaterplot Smoothing (LOESS) "]
]
