<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Handout_V2</title>
  <meta name="description" content="Handout_V2">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Handout_V2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Handout_V2" />
  
  
  

<meta name="author" content="Pierre Bauche">


<meta name="date" content="2018-10-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="neural-network.html">
<link rel="next" href="bayes-analysis.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.1/leaflet.js"></script>
<script src="libs/leaflet-providers-1.1.17/leaflet-providers.js"></script>
<script src="libs/leaflet-providers-plugin-2.0.1/leaflet-providers-plugin.js"></script>
<link href="libs/leaflet-markercluster-1.0.5/MarkerCluster.css" rel="stylesheet" />
<link href="libs/leaflet-markercluster-1.0.5/MarkerCluster.Default.css" rel="stylesheet" />
<script src="libs/leaflet-markercluster-1.0.5/leaflet.markercluster.js"></script>
<script src="libs/leaflet-markercluster-1.0.5/leaflet.markercluster.freezable.js"></script>
<script src="libs/leaflet-markercluster-1.0.5/leaflet.markercluster.layersupport.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">somethings</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Information</strong></a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#usefull-ressource"><i class="fa fa-check"></i><b>1.1</b> Usefull ressource</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#todo"><i class="fa fa-check"></i><b>1.2</b> todo</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#interesting-stuff"><i class="fa fa-check"></i><b>1.3</b> interesting stuff</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#hint"><i class="fa fa-check"></i><b>1.4</b> Hint</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="feature-engeneering.html"><a href="feature-engeneering.html"><i class="fa fa-check"></i><b>2</b> Feature engeneering</a><ul>
<li class="chapter" data-level="2.1" data-path="feature-engeneering.html"><a href="feature-engeneering.html#input-feature"><i class="fa fa-check"></i><b>2.1</b> Input feature</a><ul>
<li class="chapter" data-level="2.1.1" data-path="feature-engeneering.html"><a href="feature-engeneering.html#numeric-data"><i class="fa fa-check"></i><b>2.1.1</b> Numeric Data</a></li>
<li class="chapter" data-level="2.1.2" data-path="feature-engeneering.html"><a href="feature-engeneering.html#count-data"><i class="fa fa-check"></i><b>2.1.2</b> count data</a></li>
<li class="chapter" data-level="2.1.3" data-path="feature-engeneering.html"><a href="feature-engeneering.html#categorical-data"><i class="fa fa-check"></i><b>2.1.3</b> categorical data</a></li>
<li class="chapter" data-level="2.1.4" data-path="feature-engeneering.html"><a href="feature-engeneering.html#date-time-lubridate-package"><i class="fa fa-check"></i><b>2.1.4</b> Date Time : Lubridate package</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="feature-engeneering.html"><a href="feature-engeneering.html#missing-value"><i class="fa fa-check"></i><b>2.2</b> Missing Value</a></li>
<li class="chapter" data-level="2.3" data-path="feature-engeneering.html"><a href="feature-engeneering.html#outlier-detection"><i class="fa fa-check"></i><b>2.3</b> Outlier Detection</a></li>
<li class="chapter" data-level="2.4" data-path="feature-engeneering.html"><a href="feature-engeneering.html#sampling-and-resampling"><i class="fa fa-check"></i><b>2.4</b> Sampling and resampling</a></li>
<li class="chapter" data-level="2.5" data-path="feature-engeneering.html"><a href="feature-engeneering.html#variables-selections"><i class="fa fa-check"></i><b>2.5</b> variables selections</a><ul>
<li class="chapter" data-level="2.5.1" data-path="feature-engeneering.html"><a href="feature-engeneering.html#filter-methods"><i class="fa fa-check"></i><b>2.5.1</b> Filter methods :</a></li>
<li class="chapter" data-level="2.5.2" data-path="feature-engeneering.html"><a href="feature-engeneering.html#wrapper-methods"><i class="fa fa-check"></i><b>2.5.2</b> Wrapper Methods:</a></li>
<li class="chapter" data-level="2.5.3" data-path="feature-engeneering.html"><a href="feature-engeneering.html#embedded-methods"><i class="fa fa-check"></i><b>2.5.3</b> Embedded Methods :</a></li>
<li class="chapter" data-level="2.5.4" data-path="feature-engeneering.html"><a href="feature-engeneering.html#dimension-reduction"><i class="fa fa-check"></i><b>2.5.4</b> Dimension reduction :</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="feature-engeneering.html"><a href="feature-engeneering.html#example"><i class="fa fa-check"></i><b>2.6</b> Example</a><ul>
<li class="chapter" data-level="2.6.1" data-path="feature-engeneering.html"><a href="feature-engeneering.html#credit-risk-modeling"><i class="fa fa-check"></i><b>2.6.1</b> Credit risk modeling</a></li>
<li class="chapter" data-level="2.6.2" data-path="feature-engeneering.html"><a href="feature-engeneering.html#variance-treshold-approach"><i class="fa fa-check"></i><b>2.6.2</b> variance treshold approach</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="feature-engeneering.html"><a href="feature-engeneering.html#method-summary"><i class="fa fa-check"></i><b>2.7</b> Method Summary</a></li>
<li class="chapter" data-level="2.8" data-path="feature-engeneering.html"><a href="feature-engeneering.html#tips"><i class="fa fa-check"></i><b>2.8</b> tips</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>3</b> Data visualization</a><ul>
<li class="chapter" data-level="3.1" data-path="data-visualization.html"><a href="data-visualization.html#descriptive"><i class="fa fa-check"></i><b>3.1</b> Descriptive</a></li>
<li class="chapter" data-level="3.2" data-path="data-visualization.html"><a href="data-visualization.html#caret-package"><i class="fa fa-check"></i><b>3.2</b> Caret Package</a></li>
<li class="chapter" data-level="3.3" data-path="data-visualization.html"><a href="data-visualization.html#spacial-map"><i class="fa fa-check"></i><b>3.3</b> Spacial map</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>4</b> Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="regression.html"><a href="regression.html#introduction"><i class="fa fa-check"></i><b>4.1</b> introduction</a></li>
<li class="chapter" data-level="4.2" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear regression</a></li>
<li class="chapter" data-level="4.3" data-path="regression.html"><a href="regression.html#anova"><i class="fa fa-check"></i><b>4.3</b> ANOVA</a></li>
<li class="chapter" data-level="4.4" data-path="regression.html"><a href="regression.html#polynomiale-regression"><i class="fa fa-check"></i><b>4.4</b> Polynomiale regression</a></li>
<li class="chapter" data-level="4.5" data-path="regression.html"><a href="regression.html#logistique"><i class="fa fa-check"></i><b>4.5</b> Logistique</a><ul>
<li class="chapter" data-level="4.5.1" data-path="regression.html"><a href="regression.html#general"><i class="fa fa-check"></i><b>4.5.1</b> General</a></li>
<li class="chapter" data-level="4.5.2" data-path="regression.html"><a href="regression.html#binomial-logistic-model"><i class="fa fa-check"></i><b>4.5.2</b> Binomial Logistic MODEL</a></li>
<li class="chapter" data-level="4.5.3" data-path="regression.html"><a href="regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>4.5.3</b> Multinomial Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="regression.html"><a href="regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>4.6</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="4.7" data-path="regression.html"><a href="regression.html#model-selection"><i class="fa fa-check"></i><b>4.7</b> Model Selection</a></li>
<li class="chapter" data-level="4.8" data-path="regression.html"><a href="regression.html#regularization-algorithms"><i class="fa fa-check"></i><b>4.8</b> Regularization Algorithms</a><ul>
<li class="chapter" data-level="4.8.1" data-path="regression.html"><a href="regression.html#ridge-regression"><i class="fa fa-check"></i><b>4.8.1</b> Ridge regression</a></li>
<li class="chapter" data-level="4.8.2" data-path="regression.html"><a href="regression.html#least-absolute-shrinkage-and-selection-operator-lasso"><i class="fa fa-check"></i><b>4.8.2</b> Least Absolute Shrinkage and Selection Opérator LASSO</a></li>
<li class="chapter" data-level="4.8.3" data-path="regression.html"><a href="regression.html#elastic-net"><i class="fa fa-check"></i><b>4.8.3</b> Elastic Net</a></li>
<li class="chapter" data-level="4.8.4" data-path="regression.html"><a href="regression.html#leas-angle-regression-lars"><i class="fa fa-check"></i><b>4.8.4</b> Leas-Angle Regression LARS</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="regression.html"><a href="regression.html#locally-estimated-scaterplot-smoothing-loess"><i class="fa fa-check"></i><b>4.9</b> Locally estimated Scaterplot Smoothing (LOESS)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>5</b> Unsupervised</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-algorithms"><i class="fa fa-check"></i><b>5.1</b> Dimensionality reduction algorithms</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised.html"><a href="unsupervised.html#cluster-analysis"><i class="fa fa-check"></i><b>5.2</b> Cluster analysis</a></li>
<li class="chapter" data-level="5.3" data-path="unsupervised.html"><a href="unsupervised.html#evaluation-of-clustering"><i class="fa fa-check"></i><b>5.3</b> Evaluation of clustering</a></li>
<li class="chapter" data-level="5.4" data-path="unsupervised.html"><a href="unsupervised.html#association-rule-mining-algorithms"><i class="fa fa-check"></i><b>5.4</b> Association Rule Mining Algorithms</a></li>
<li class="chapter" data-level="5.5" data-path="unsupervised.html"><a href="unsupervised.html#singular-value-decomposition"><i class="fa fa-check"></i><b>5.5</b> Singular Value decomposition</a></li>
<li class="chapter" data-level="5.6" data-path="unsupervised.html"><a href="unsupervised.html#k-nearest-neighbot"><i class="fa fa-check"></i><b>5.6</b> K-Nearest Neighbot</a></li>
<li class="chapter" data-level="5.7" data-path="unsupervised.html"><a href="unsupervised.html#others-unsuppervised-algorithms"><i class="fa fa-check"></i><b>5.7</b> Others unsuppervised algorithms</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>6</b> Decision Tree</a><ul>
<li class="chapter" data-level="6.1" data-path="decision-tree.html"><a href="decision-tree.html#type-of-decision-tree"><i class="fa fa-check"></i><b>6.1</b> Type of décision tree</a></li>
<li class="chapter" data-level="6.2" data-path="decision-tree.html"><a href="decision-tree.html#decision-measures-measure-of-node-purity-heterogeneity-of-the-node"><i class="fa fa-check"></i><b>6.2</b> Decision measures : measure of node purity (heterogeneity of the node)</a></li>
<li class="chapter" data-level="6.3" data-path="decision-tree.html"><a href="decision-tree.html#decision-tree-learning-methods"><i class="fa fa-check"></i><b>6.3</b> Decision tree learning methods</a></li>
<li class="chapter" data-level="6.4" data-path="decision-tree.html"><a href="decision-tree.html#random-forests"><i class="fa fa-check"></i><b>6.4</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-network.html"><a href="neural-network.html"><i class="fa fa-check"></i><b>7</b> Neural Network</a><ul>
<li class="chapter" data-level="7.1" data-path="neural-network.html"><a href="neural-network.html#neural-networks-basis"><i class="fa fa-check"></i><b>7.1</b> Neural Networks Basis</a></li>
<li class="chapter" data-level="7.2" data-path="neural-network.html"><a href="neural-network.html#neural-network-architecture"><i class="fa fa-check"></i><b>7.2</b> Neural Network Architecture</a></li>
<li class="chapter" data-level="7.3" data-path="neural-network.html"><a href="neural-network.html#deep-learning"><i class="fa fa-check"></i><b>7.3</b> Deep Learning</a><ul>
<li class="chapter" data-level="7.3.1" data-path="neural-network.html"><a href="neural-network.html#example-of-deep-learning-classification"><i class="fa fa-check"></i><b>7.3.1</b> Example of deep learning : Classification</a></li>
<li class="chapter" data-level="7.3.2" data-path="neural-network.html"><a href="neural-network.html#example-imagine-prediction-nn-classification"><i class="fa fa-check"></i><b>7.3.2</b> Example : Imagine prediction : NN classification</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>8</b> Text Mining</a><ul>
<li class="chapter" data-level="8.1" data-path="text-mining.html"><a href="text-mining.html#tf---idf"><i class="fa fa-check"></i><b>8.1</b> TF - IDF</a></li>
<li class="chapter" data-level="8.2" data-path="text-mining.html"><a href="text-mining.html#text-summarization-gong-liu-method-2001-via-latent-semantic-analysis"><i class="fa fa-check"></i><b>8.2</b> Text summarization : Gong &amp; Liu method (2001) via latent semantic analysis</a></li>
<li class="chapter" data-level="8.3" data-path="text-mining.html"><a href="text-mining.html#text-analysis"><i class="fa fa-check"></i><b>8.3</b> Text analysis</a></li>
<li class="chapter" data-level="8.4" data-path="text-mining.html"><a href="text-mining.html#other-topic"><i class="fa fa-check"></i><b>8.4</b> Other topic</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayes-analysis.html"><a href="bayes-analysis.html"><i class="fa fa-check"></i><b>9</b> Bayes Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="bayes-analysis.html"><a href="bayes-analysis.html#introduction-au-bayseien"><i class="fa fa-check"></i><b>9.1</b> Introduction au bayseien</a></li>
<li class="chapter" data-level="9.2" data-path="bayes-analysis.html"><a href="bayes-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>9.2</b> NAive bayes</a></li>
<li class="chapter" data-level="9.3" data-path="bayes-analysis.html"><a href="bayes-analysis.html#other-bayes-model"><i class="fa fa-check"></i><b>9.3</b> Other bayes model</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>10</b> Time Series</a></li>
<li class="chapter" data-level="11" data-path="others-ml-models.html"><a href="others-ml-models.html"><i class="fa fa-check"></i><b>11</b> Others ML models</a><ul>
<li class="chapter" data-level="11.1" data-path="others-ml-models.html"><a href="others-ml-models.html#support-vector-machines"><i class="fa fa-check"></i><b>11.1</b> Support Vector Machines</a></li>
<li class="chapter" data-level="11.2" data-path="others-ml-models.html"><a href="others-ml-models.html#hadoop-introduction"><i class="fa fa-check"></i><b>11.2</b> Hadoop introduction</a></li>
<li class="chapter" data-level="11.3" data-path="others-ml-models.html"><a href="others-ml-models.html#machine-learning-in-r-with-spark"><i class="fa fa-check"></i><b>11.3</b> Machine Learning in R with Spark</a></li>
<li class="chapter" data-level="11.4" data-path="others-ml-models.html"><a href="others-ml-models.html#machine-learning-in-r-with-h20"><i class="fa fa-check"></i><b>11.4</b> Machine learning in R with H20</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="caret-package-1.html"><a href="caret-package-1.html"><i class="fa fa-check"></i><b>12</b> Caret Package</a><ul>
<li class="chapter" data-level="12.1" data-path="caret-package-1.html"><a href="caret-package-1.html#pre-processing"><i class="fa fa-check"></i><b>12.1</b> Pre-Processing</a></li>
<li class="chapter" data-level="12.2" data-path="caret-package-1.html"><a href="caret-package-1.html#data-splitting"><i class="fa fa-check"></i><b>12.2</b> Data Splitting</a></li>
<li class="chapter" data-level="12.3" data-path="caret-package-1.html"><a href="caret-package-1.html#model-training-and-tuning"><i class="fa fa-check"></i><b>12.3</b> Model Training and tuning</a><ul>
<li class="chapter" data-level="12.3.1" data-path="caret-package-1.html"><a href="caret-package-1.html#exemple-basic-tuning-for-boosted-tree-model-gbm"><i class="fa fa-check"></i><b>12.3.1</b> Exemple : Basic tuning for boosted tree model GBM</a></li>
<li class="chapter" data-level="12.3.2" data-path="caret-package-1.html"><a href="caret-package-1.html#customizing-the-tuning-process"><i class="fa fa-check"></i><b>12.3.2</b> Customizing the Tuning Process</a></li>
<li class="chapter" data-level="12.3.3" data-path="caret-package-1.html"><a href="caret-package-1.html#exploring-and-comparing-resampling-distributions"><i class="fa fa-check"></i><b>12.3.3</b> Exploring and Comparing Resampling Distributions</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="caret-package-1.html"><a href="caret-package-1.html#best-available-models"><i class="fa fa-check"></i><b>12.4</b> Best available Models</a></li>
<li class="chapter" data-level="12.5" data-path="caret-package-1.html"><a href="caret-package-1.html#parallel-processing"><i class="fa fa-check"></i><b>12.5</b> Parallel Processing</a></li>
<li class="chapter" data-level="12.6" data-path="caret-package-1.html"><a href="caret-package-1.html#subsampling-for-class-imbalances"><i class="fa fa-check"></i><b>12.6</b> Subsampling for class imbalances</a></li>
<li class="chapter" data-level="12.7" data-path="caret-package-1.html"><a href="caret-package-1.html#variables-importance"><i class="fa fa-check"></i><b>12.7</b> Variables importance</a></li>
<li class="chapter" data-level="12.8" data-path="caret-package-1.html"><a href="caret-package-1.html#measurung-performance"><i class="fa fa-check"></i><b>12.8</b> measurung performance</a></li>
<li class="chapter" data-level="12.9" data-path="caret-package-1.html"><a href="caret-package-1.html#feature-selection"><i class="fa fa-check"></i><b>12.9</b> feature selection</a><ul>
<li class="chapter" data-level="12.9.1" data-path="caret-package-1.html"><a href="caret-package-1.html#overview"><i class="fa fa-check"></i><b>12.9.1</b> Overview</a></li>
<li class="chapter" data-level="12.9.2" data-path="caret-package-1.html"><a href="caret-package-1.html#univariate-approach"><i class="fa fa-check"></i><b>12.9.2</b> Univariate approach</a></li>
<li class="chapter" data-level="12.9.3" data-path="caret-package-1.html"><a href="caret-package-1.html#recursive-feature-elimination"><i class="fa fa-check"></i><b>12.9.3</b> recursive feature elimination</a></li>
<li class="chapter" data-level="12.9.4" data-path="caret-package-1.html"><a href="caret-package-1.html#genetic-algorimth"><i class="fa fa-check"></i><b>12.9.4</b> genetic algorimth</a></li>
<li class="chapter" data-level="12.9.5" data-path="caret-package-1.html"><a href="caret-package-1.html#simulated-annealing"><i class="fa fa-check"></i><b>12.9.5</b> simulated annealing</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Handout_V2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="text-mining" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Text Mining</h1>
<ul>
<li>To Read :
<ul>
<li><a href="https://www.tidytextmining.com/index.html" class="uri">https://www.tidytextmining.com/index.html</a></li>
<li>Natural language processing (NLP)</li>
</ul></li>
</ul>
<div id="tf---idf" class="section level2">
<h2><span class="header-section-number">8.1</span> TF - IDF</h2>
<ul>
<li>Term frequency counts the number of occurrences of a term t in a document.</li>
<li>Inverse document frequency : $ idf = log_2  where |D| denotes the total number of documents and <span class="math inline">\(|{d|t\in d}|\)</span> is the number of documents where the term t appears</li>
</ul>
<p>Certain terms that occur too frequently have little power in determining the reliance of a document. IDF weigh down the too frequently occurring word (et inverserment). A tf-idf matrix is a numerical representation of a collection of documents (represented by row) and words contained in it (represented by columns).</p>
<ul>
<li><strong>In bag-of-words (BoW) featurization</strong>, a text document is converted into a flat vector of counts. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in :
<ul>
<li>bow(w, d) = # times word w appears in document d</li>
<li>tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)</li>
</ul></li>
</ul>
<p>If a word appears in many 61documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher. Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filter‐ing methods</p>
<blockquote>
<p>Text Mining TM : USE bag of word each word one feature + PCA pour reduire dimension</p>
</blockquote>
<ul>
<li><strong>bag-of-n-grams</strong> : is a sequence of n tokens. After tokenization, the counting mechanism can collate individual tokens into word counts. n-grams retain more of the original sequence structure of the text.
<ul>
<li>For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door. here are usually a lot more distinct n-grams (n &gt; 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model.</li>
</ul></li>
<li><strong>Filtering for Cleaner Features</strong> : TM generate a lot a new feature, How separate the noise from the signal?
<ul>
<li>Stopword : For classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.</li>
<li>Frequent words : Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus</li>
<li>Rare words : To a statistical model, a word that appears in only one or two documents is more like noise than useful information. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain</li>
<li>Stemming : Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form (“swimmer,” “swimming,” and “swim,”) =&lt; lemmmization</li>
</ul></li>
</ul>
<blockquote>
<p>The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix.</p>
</blockquote>
<ul>
<li><strong>Parsing and Tokenization</strong>
<ul>
<li>Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure -tokenization : This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word</li>
</ul></li>
<li><strong>Chunking and part-of-speech tagging</strong>
<ul>
<li>we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.”</li>
</ul></li>
<li><strong>Part of Speech (POS) tagging</strong>
<ul>
<li>This could help in classifying named entities in text into categories like persons, company, locations, expression of time, and so on.</li>
</ul></li>
<li><strong>Word Cloud</strong>
<ul>
<li>The word cloud helps in visualizing the words most frequently being used in the reviews</li>
</ul></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(data.table)
<span class="kw">library</span>(caTools)

fine_food_data &lt;-<span class="kw">read.csv</span>(<span class="st">&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Food_Reviews.csv&quot;</span>, <span class="dt">stringsAsFactors =</span><span class="ot">FALSE</span>)
fine_food_data<span class="op">$</span>Score &lt;-<span class="kw">as.factor</span>(fine_food_data<span class="op">$</span>Score)

<span class="kw">head</span>(fine_food_data[,<span class="dv">10</span>],<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] &quot;I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.&quot;
## [2] &quot;Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \&quot;Jumbo\&quot;.&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data preparation</span>

<span class="co"># Randomly split data and use only 10% of the dataset</span>
<span class="kw">set.seed</span>(<span class="dv">90</span>)
split =<span class="kw">sample.split</span>(fine_food_data<span class="op">$</span>Score, <span class="dt">SplitRatio =</span><span class="fl">0.10</span>)
fine_food_data =<span class="kw">subset</span>(fine_food_data, split <span class="op">==</span><span class="ot">TRUE</span>)
select_col &lt;-<span class="kw">c</span>(<span class="st">&quot;Id&quot;</span>,<span class="st">&quot;HelpfulnessNumerator&quot;</span>,<span class="st">&quot;HelpfulnessDenominator&quot;</span>,<span class="st">&quot;Score&quot;</span>,<span class="st">&quot;Summary&quot;</span>,<span class="st">&quot;Text&quot;</span>)
fine_food_data_selected &lt;-fine_food_data[,select_col]

<span class="kw">dim</span>(fine_food_data_selected)</code></pre></div>
<pre><code>## [1] 3518    6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Summary text</span>
##original
fine_food_data_selected[<span class="dv">2</span>,<span class="dv">6</span>]</code></pre></div>
<pre><code>## [1] &quot;McCann&#39;s Instant Oatmeal is great if you must have your oatmeal but can only scrape together two or three minutes to prepare it. There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation.  Still, the McCann&#39;s is as good as it gets for instant oatmeal. It&#39;s even better than the organic, all-natural brands I have tried.  All the varieties in the McCann&#39;s variety pack taste good.  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue.&lt;br /&gt;&lt;br /&gt;McCann&#39;s use of actual cane sugar instead of high fructose corn syrup helped me decide to buy this product.  Real sugar tastes better and is not as harmful as the other stuff. One thing I do not like, though, is McCann&#39;s use of thickeners.  Oats plus water plus heat should make a creamy, tasty oatmeal without the need for guar gum. But this is a convenience product.  Maybe the guar gum is why, after sitting in the bowl a while, the instant McCann&#39;s becomes too thick and gluey.&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(LSAfun)
<span class="kw">genericSummary</span>(fine_food_data_selected[<span class="dv">2</span>,<span class="dv">6</span>],<span class="dt">k=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] &quot; There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">genericSummary</span>(fine_food_data_selected[<span class="dv">2</span>,<span class="dv">6</span>],<span class="dt">k=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] &quot; There is no escaping the fact, however, that even the best instant oatmeal is nowhere near as good as even a store brand of oatmeal requiring stovetop preparation&quot;
## [2] &quot;  It can be prepared in the microwave or by adding boiling water so it is convenient in the extreme when time is an issue&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># TF and IDF</span>
<span class="kw">library</span>(tm)

fine_food_data_corpus &lt;-<span class="kw">VCorpus</span>(<span class="kw">VectorSource</span>(fine_food_data_selected<span class="op">$</span>Text))

## Standardize the text - Pre-Processing
fine_food_data_text_dtm &lt;-<span class="kw">DocumentTermMatrix</span>(fine_food_data_corpus, 
                          <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">tolower =</span><span class="ot">TRUE</span>,
                                       <span class="dt">removeNumbers =</span><span class="ot">TRUE</span>,
                                       <span class="dt">stopwords =</span><span class="ot">TRUE</span>,
                                       <span class="dt">removePunctuation =</span><span class="ot">TRUE</span>,
                                       <span class="dt">stemming =</span><span class="ot">TRUE</span>
                                       ))

<span class="co">#save frequently-appearing terms( more than 500 times) to a character vector</span>
fine_food_data_text_freq &lt;-<span class="kw">findFreqTerms</span>(fine_food_data_text_dtm, <span class="dv">500</span>)

<span class="co"># create DTMs with only the frequent terms</span>
fine_food_data_text_dtm &lt;-fine_food_data_text_dtm[ , fine_food_data_text_freq]
tm<span class="op">::</span><span class="kw">inspect</span>(fine_food_data_text_dtm[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt;
## Non-/sparse entries: 8/42
## Sparsity           : 84%
## Maximal term length: 6
## Weighting          : term frequency (tf)
## Sample             :
##     Terms
## Docs also bag buy can coffe dog eat find flavor food
##    1    1   0   0   0     0   0   0    0      0    0
##    2    0   0   1   2     0   0   0    0      0    0
##    3    0   0   0   0     2   0   0    0      0    0
##    4    0   0   0   0     0   0   1    1      0    0
##    5    0   0   0   0     0   0   0    1      2    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Create a tf-idf matrix</span>
fine_food_data_tfidf &lt;-<span class="kw">weightTfIdf</span>(fine_food_data_text_dtm, <span class="dt">normalize=</span><span class="ot">FALSE</span>)
tm<span class="op">::</span><span class="kw">inspect</span>(fine_food_data_tfidf[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>])</code></pre></div>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt;
## Non-/sparse entries: 8/42
## Sparsity           : 84%
## Maximal term length: 6
## Weighting          : term frequency - inverse document frequency (tf-idf)
## Sample             :
##     Terms
## Docs    also bag      buy      can   coffe dog      eat     find   flavor
##    1 3.04583   0 0.000000 0.000000 0.00000   0 0.000000 0.000000 0.000000
##    2 0.00000   0 2.635882 4.525741 0.00000   0 0.000000 0.000000 0.000000
##    3 0.00000   0 0.000000 0.000000 5.82035   0 0.000000 0.000000 0.000000
##    4 0.00000   0 0.000000 0.000000 0.00000   0 2.960361 2.992637 0.000000
##    5 0.00000   0 0.000000 0.000000 0.00000   0 0.000000 2.992637 4.024711
##     Terms
## Docs food
##    1    0
##    2    0
##    3    0
##    4    0
##    5    0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Part of Speech tagging</span>
fine_food_data_corpus&lt;-<span class="kw">Corpus</span>(<span class="kw">VectorSource</span>(fine_food_data_selected<span class="op">$</span>Text[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]))
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_corpus, PlainTextDocument)

##tolwer
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_cleaned, tolower)
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_cleaned, removeWords, <span class="kw">stopwords</span>(<span class="st">&quot;english&quot;</span>))
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_cleaned,removePunctuation)
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_cleaned, removeNumbers)
fine_food_data_cleaned &lt;-<span class="kw">tm_map</span>(fine_food_data_cleaned, stripWhitespace)

<span class="kw">library</span>(openNLP)
<span class="kw">library</span>(NLP)

fine_food_data_string &lt;-NLP<span class="op">::</span><span class="kw">as.String</span>(fine_food_data_cleaned[[<span class="dv">1</span>]])
sent_token_annotator &lt;-<span class="kw">Maxent_Sent_Token_Annotator</span>()
word_token_annotator &lt;-<span class="kw">Maxent_Word_Token_Annotator</span>()
fine_food_data_string_an &lt;-<span class="kw">annotate</span>(fine_food_data_string, <span class="kw">list</span>(sent_token_annotator, word_token_annotator))

pos_tag_annotator &lt;-<span class="kw">Maxent_POS_Tag_Annotator</span>()
fine_food_data_string_an2 &lt;-<span class="kw">annotate</span>(fine_food_data_string, pos_tag_annotator, fine_food_data_string_an)

<span class="kw">head</span>(<span class="kw">annotate</span>(fine_food_data_string, <span class="kw">Maxent_POS_Tag_Annotator</span>(<span class="dt">probs =</span><span class="ot">TRUE</span>), fine_food_data_string_an2))</code></pre></div>
<pre><code>##  id type     start end features
##   1 sentence     1 524 constituents=&lt;&lt;integer,77&gt;&gt;
##   2 word         1   9 POS=NNS, POS=NNS, POS_prob=0.7822268
##   3 word        11  20 POS=VBP, POS=VBP, POS_prob=0.3488425
##   4 word        22  30 POS=NN, POS=NN, POS_prob=0.8055908
##   5 word        32  39 POS=JJ, POS=JJ, POS_prob=0.6114238
##   6 word        41  45 POS=NN, POS=NN, POS_prob=0.9833723</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fine_food_data_string_an2w &lt;-<span class="kw">subset</span>(fine_food_data_string_an2, type <span class="op">==</span><span class="st"> &quot;word&quot;</span>)
tags &lt;-<span class="kw">sapply</span>(fine_food_data_string_an2w<span class="op">$</span>features, <span class="st">`</span><span class="dt">[[</span><span class="st">`</span>, <span class="st">&quot;POS&quot;</span>)
<span class="kw">table</span>(tags)</code></pre></div>
<pre><code>## tags
##   ,  CC  CD  IN  JJ JJS  NN NNS  RB  VB VBD VBG VBN VBP VBZ 
##   1   2   1   1  10   2  28   9   5   1   6   2   4   2   3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">table</span>(tags), <span class="dt">type =</span><span class="st">&quot;h&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Part-Of_Speech&quot;</span>, <span class="dt">ylab =</span><span class="st">&quot;Frequency&quot;</span>)</code></pre></div>
<p><img src="09-TextMining_files/figure-html/textmining-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">sprintf</span>(<span class="st">&quot;%s/%s&quot;</span>, fine_food_data_string[fine_food_data_string_an2w], tags),<span class="dv">15</span>)</code></pre></div>
<pre><code>##  [1] &quot;twizzlers/NNS&quot;    &quot;strawberry/VBP&quot;   &quot;childhood/NN&quot;    
##  [4] &quot;favorite/JJ&quot;      &quot;candy/NN&quot;         &quot;made/VBD&quot;        
##  [7] &quot;lancaster/NN&quot;     &quot;pennsylvania/NN&quot;  &quot;y/RB&quot;            
## [10] &quot;s/VBZ&quot;            &quot;candies/NNS&quot;      &quot;inc/CC&quot;          
## [13] &quot;one/CD&quot;           &quot;oldest/JJS&quot;       &quot;confectionery/NN&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># wordcloud</span>
<span class="kw">library</span>(SnowballC)
<span class="kw">library</span>(wordcloud)
<span class="kw">library</span>(slam)

fine_food_data_corpus &lt;-<span class="kw">VCorpus</span>(<span class="kw">VectorSource</span>(fine_food_data_selected<span class="op">$</span>Text))
fine_food_data_text_tdm &lt;-<span class="kw">TermDocumentMatrix</span>(fine_food_data_corpus,
                          <span class="dt">control =</span><span class="kw">list</span>(<span class="dt">tolower =</span><span class="ot">TRUE</span>,
                                        <span class="dt">removeNumbers =</span><span class="ot">TRUE</span>,
                                        <span class="dt">stopwords =</span><span class="ot">TRUE</span>,
                                        <span class="dt">removePunctuation =</span><span class="ot">TRUE</span>,
                                        <span class="dt">stemming =</span><span class="ot">TRUE</span>
                                        ))

wc_tdm &lt;-<span class="st"> </span><span class="kw">rollup</span>(fine_food_data_text_tdm,<span class="dv">2</span>,<span class="dt">na.rm=</span><span class="ot">TRUE</span>,<span class="dt">FUN=</span>sum)
matrix_c &lt;-<span class="kw">as.matrix</span>(wc_tdm)
wc_freq &lt;-<span class="kw">sort</span>(<span class="kw">rowSums</span>(matrix_c))
wc_tmdata &lt;-<span class="kw">data.frame</span>(<span class="dt">words=</span><span class="kw">names</span>(wc_freq), wc_freq)
wc_tmdata &lt;-<span class="kw">na.omit</span>(wc_tmdata)

<span class="kw">wordcloud</span>(<span class="kw">tail</span>(wc_tmdata<span class="op">$</span>words,<span class="dv">100</span>), <span class="kw">tail</span>(wc_tmdata<span class="op">$</span>wc_freq,<span class="dv">100</span>), <span class="dt">random.order=</span><span class="ot">FALSE</span>, <span class="dt">colors=</span><span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Dark2&quot;</span>))</code></pre></div>
<p><img src="09-TextMining_files/figure-html/textmining-2.png" width="672" /></p>
</div>
<div id="text-summarization-gong-liu-method-2001-via-latent-semantic-analysis" class="section level2">
<h2><span class="header-section-number">8.2</span> Text summarization : Gong &amp; Liu method (2001) via latent semantic analysis</h2>
<ul>
<li><ol style="list-style-type: decimal">
<li>Decompose the document D into individual sentences and use these sentences to form the candidate sentence set S and set k = 1.</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Construct the terms by sentences matrix A for the document D.</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>Perform the SVD on A to obtain the singular value matrix, and the right singular vector matrix V^t. In the singular vector space, each sentence i is represented by the column vector.</li>
</ol></li>
<li><ol start="4" style="list-style-type: decimal">
<li>Select the k’th right singular vector from matrix V^t.</li>
</ol></li>
<li><ol start="5" style="list-style-type: decimal">
<li>Select the sentence that has the largest index value with the k’th right singular vector and include it in the summary.</li>
</ol></li>
<li><ol start="6" style="list-style-type: decimal">
<li>If k reaches the predefined number, terminate the operation otherwise, increment k by 1 and go back to Step 4</li>
</ol></li>
</ul>
</div>
<div id="text-analysis" class="section level2">
<h2><span class="header-section-number">8.3</span> Text analysis</h2>
<p>we will introduce you to the powerful world of text analytics by using a third-party API ( (Application Programming Interface) called from within R. We will be using Microsoft Cognitive Services API to show some real-time analysis of text from the Twitter feed of a news agency. Microsoft Cognitive Services is a machine intelligence service. This service provide a cloud-based APIs for developers to do lot of high-end functions like face recognition, speech recognition, text mining, video feed analysis, and many others. We will be using their free developer service to show some text analytics features like : - <strong>Sentiment analysis</strong>: Sentiment analysis will tell us what kind of emotions the tweets are carrying. The Microsoft API returns a value between 0 and 1, where 1 means highly positive sentiment while 0 means highly negative sentiment. - <strong>Topic detection</strong>: What the topic of discussion is a document? - <strong>Language detection</strong>: Can you just provide something written and it shows you which language it is? - <strong>Summarization</strong>: Can we automatically summarize a big document to make it manageable to read</p>
<p>Exemple : Use twitter to analyse Attention besoin d’un compte Microsoft cognitive</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">##&quot; NEED microsoft account, don&#39;t realy work&quot;

<span class="co"># # library(&quot;twitteR&quot;)</span>
<span class="co"># # See Machine learning with R p 424 to use twitter for text analytics</span>
<span class="co"># </span>
<span class="co"># #install.packages(&quot;mscstexta4r&quot;)</span>
<span class="co"># library(mscstexta4r)</span>
<span class="co"># </span>
<span class="co"># Sys.setenv(MSCS_TEXTANALYTICS_URL =&quot;https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment&quot;)</span>
<span class="co"># Sys.setenv(MSCS_TEXTANALYTICS_KEY =&quot;2673988d37f941f89440d665ae6dad9b&quot;)</span>
<span class="co"># </span>
<span class="co"># #Initialize the service</span>
<span class="co"># textaInit()</span>
<span class="co"># </span>
<span class="co"># # Load Packages</span>
<span class="co"># require(tm)</span>
<span class="co"># require(NLP)</span>
<span class="co"># require(openNLP)</span>
<span class="co"># #Read the Forbes article into R environment</span>
<span class="co"># y &lt;-paste(scan(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/india_after_independence.txt&quot;, what=&quot;character&quot;,sep=&quot; &quot;),collapse=&quot; &quot;)</span>
<span class="co"># </span>
<span class="co"># convert_text_to_sentences &lt;-function(text, lang =&quot;en&quot;) {</span>
<span class="co"># # Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language &#39;en&#39;.</span>
<span class="co"># sentence_token_annotator &lt;-Maxent_Sent_Token_Annotator(language = lang)</span>
<span class="co"># # Convert text to class String from package NLP</span>
<span class="co"># text &lt;-as.String(text)</span>
<span class="co"># # Sentence boundaries in text</span>
<span class="co"># sentence.boundaries &lt;-annotate(text, sentence_token_annotator)</span>
<span class="co"># # Extract sentences</span>
<span class="co"># sentences &lt;-text[sentence.boundaries]</span>
<span class="co"># # return sentences</span>
<span class="co"># return(sentences)</span>
<span class="co"># }</span>
<span class="co"># </span>
<span class="co"># # Convert the text into sentences</span>
<span class="co"># article_text =convert_text_to_sentences(y, lang =&quot;en&quot;)</span>
<span class="co"># </span>
<span class="co"># </span>
<span class="co"># </span><span class="al">###</span><span class="co"> SEntiment analysis </span><span class="al">###</span><span class="co"> </span>
<span class="co"># #import tweet</span>
<span class="co"># </span>
<span class="co"># tweets = read.csv(file=&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Twitter Feed From TimesNow.csv&quot;)</span>
<span class="co"># </span>
<span class="co"># </span>
<span class="co"># document_lang &lt;-rep(&quot;en&quot;, length(tweets$text))</span>
<span class="co"># tweets$text= as.character(tweets$text)</span>
<span class="co"># </span>
<span class="co"># tryCatch({</span>
<span class="co">#   # Perform sentiment analysis</span>
<span class="co">#   output_1 &lt;-textaSentiment(</span>
<span class="co">#             documents = tweets$text, # Input sentences or documents</span>
<span class="co">#             languages = document_lang</span>
<span class="co"># # &quot;en&quot;(English, default)|&quot;es&quot;(Spanish)|&quot;fr&quot;(French)|&quot;pt&quot;(Portuguese)</span>
<span class="co">#                             )</span>
<span class="co">#         }, error = function(err) {</span>
<span class="co"># # Print error</span>
<span class="co">#             geterrmessage()</span>
<span class="co">#         })</span>
<span class="co"># merged &lt;-output_1$results</span>
<span class="co"># </span>
<span class="co"># library(httr)</span>
<span class="co"># library(jsonlite)</span>
<span class="co"># #Setup</span>
<span class="co"># cogapikey&lt;-&quot;2673988d37f941f89440d665ae6dad9b&quot;</span>
<span class="co"># cogapi&lt;-&quot;https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages&quot;</span>
<span class="co"># </span>
<span class="co"># text=c(&quot;is this english?&quot;</span>
<span class="co">#        ,&quot;tak er der mere kage&quot;</span>
<span class="co">#        ,&quot;merci beaucoup&quot;</span>
<span class="co">#        ,&quot;guten morgen&quot;</span>
<span class="co">#        ,&quot;bonjour&quot;</span>
<span class="co">#        ,&quot;merde&quot;</span>
<span class="co">#        ,&quot;That&#39;s terrible&quot;</span>
<span class="co">#        ,&quot;R is awesome&quot;)</span>
<span class="co"># </span>
<span class="co"># # Prep data</span>
<span class="co"># df&lt;-data_frame(id=1:8,text)</span>
<span class="co"># mydata&lt;-list(documents= df)</span>
<span class="co"># </span>
<span class="co"># </span>
<span class="co"># cogapi&lt;-&quot;https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment&quot;</span>
<span class="co"># # Construct a request</span>
<span class="co"># response&lt;-POST(cogapi, </span>
<span class="co">#                add_headers(`Ocp-Apim-Subscription-Key`=cogapikey),</span>
<span class="co">#                body=toJSON(mydata))</span>
<span class="co"># </span>
<span class="co"># # Process reponse</span>
<span class="co"># respcontent&lt;-content(response, as=&quot;text&quot;)</span>
<span class="co"># </span>
<span class="co"># fromJSON(respcontent)$documents %&gt;%</span>
<span class="co">#    mutate(id=as.numeric(id)) -&gt;</span>
<span class="co">#    responses</span></code></pre></div>
</div>
<div id="other-topic" class="section level2">
<h2><span class="header-section-number">8.4</span> Other topic</h2>
<ul>
<li><strong>Named entity recognition NER</strong></li>
<li><strong>OOptical character recognition OCR</strong></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="neural-network.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayes-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["bookdown-start.pdf", "bookdown-start.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
